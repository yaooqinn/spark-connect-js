//
// Licensed to the Apache Software Foundation (ASF) under one or more
// contributor license agreements.  See the NOTICE file distributed with
// this work for additional information regarding copyright ownership.
// The ASF licenses this file to You under the Apache License, Version 2.0
// (the "License"); you may not use this file except in compliance with
// the License.  You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.2.3 with parameter "target=ts,import_extension=none,js_import_style=module"
// @generated from file spark/connect/relations.proto (package spark.connect, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import type { Any } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_any } from "@bufbuild/protobuf/wkt";
import type { CommonInlineUserDefinedFunction, Expression, Expression_Alias, Expression_Literal, Expression_SortOrder } from "./expressions_pb";
import { file_spark_connect_expressions } from "./expressions_pb";
import type { DataType } from "./types_pb";
import { file_spark_connect_types } from "./types_pb";
import type { Catalog } from "./catalog_pb";
import { file_spark_connect_catalog } from "./catalog_pb";
import type { Origin } from "./common_pb";
import { file_spark_connect_common } from "./common_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file spark/connect/relations.proto.
 */
export const file_spark_connect_relations: GenFile = /*@__PURE__*/
  fileDesc("Ch1zcGFyay9jb25uZWN0L3JlbGF0aW9ucy5wcm90bxINc3BhcmsuY29ubmVjdCLoFgoIUmVsYXRpb24SLQoGY29tbW9uGAEgASgLMh0uc3BhcmsuY29ubmVjdC5SZWxhdGlvbkNvbW1vbhIjCgRyZWFkGAIgASgLMhMuc3BhcmsuY29ubmVjdC5SZWFkSAASKQoHcHJvamVjdBgDIAEoCzIWLnNwYXJrLmNvbm5lY3QuUHJvamVjdEgAEicKBmZpbHRlchgEIAEoCzIVLnNwYXJrLmNvbm5lY3QuRmlsdGVySAASIwoEam9pbhgFIAEoCzITLnNwYXJrLmNvbm5lY3QuSm9pbkgAEi0KBnNldF9vcBgGIAEoCzIbLnNwYXJrLmNvbm5lY3QuU2V0T3BlcmF0aW9uSAASIwoEc29ydBgHIAEoCzITLnNwYXJrLmNvbm5lY3QuU29ydEgAEiUKBWxpbWl0GAggASgLMhQuc3BhcmsuY29ubmVjdC5MaW1pdEgAEi0KCWFnZ3JlZ2F0ZRgJIAEoCzIYLnNwYXJrLmNvbm5lY3QuQWdncmVnYXRlSAASIQoDc3FsGAogASgLMhIuc3BhcmsuY29ubmVjdC5TUUxIABI2Cg5sb2NhbF9yZWxhdGlvbhgLIAEoCzIcLnNwYXJrLmNvbm5lY3QuTG9jYWxSZWxhdGlvbkgAEicKBnNhbXBsZRgMIAEoCzIVLnNwYXJrLmNvbm5lY3QuU2FtcGxlSAASJwoGb2Zmc2V0GA0gASgLMhUuc3BhcmsuY29ubmVjdC5PZmZzZXRIABIxCgtkZWR1cGxpY2F0ZRgOIAEoCzIaLnNwYXJrLmNvbm5lY3QuRGVkdXBsaWNhdGVIABIlCgVyYW5nZRgPIAEoCzIULnNwYXJrLmNvbm5lY3QuUmFuZ2VIABI2Cg5zdWJxdWVyeV9hbGlhcxgQIAEoCzIcLnNwYXJrLmNvbm5lY3QuU3VicXVlcnlBbGlhc0gAEjEKC3JlcGFydGl0aW9uGBEgASgLMhouc3BhcmsuY29ubmVjdC5SZXBhcnRpdGlvbkgAEiQKBXRvX2RmGBIgASgLMhMuc3BhcmsuY29ubmVjdC5Ub0RGSAASQQoUd2l0aF9jb2x1bW5zX3JlbmFtZWQYEyABKAsyIS5zcGFyay5jb25uZWN0LldpdGhDb2x1bW5zUmVuYW1lZEgAEjAKC3Nob3dfc3RyaW5nGBQgASgLMhkuc3BhcmsuY29ubmVjdC5TaG93U3RyaW5nSAASIwoEZHJvcBgVIAEoCzITLnNwYXJrLmNvbm5lY3QuRHJvcEgAEiMKBHRhaWwYFiABKAsyEy5zcGFyay5jb25uZWN0LlRhaWxIABIyCgx3aXRoX2NvbHVtbnMYFyABKAsyGi5zcGFyay5jb25uZWN0LldpdGhDb2x1bW5zSAASIwoEaGludBgYIAEoCzITLnNwYXJrLmNvbm5lY3QuSGludEgAEikKB3VucGl2b3QYGSABKAsyFi5zcGFyay5jb25uZWN0LlVucGl2b3RIABIsCgl0b19zY2hlbWEYGiABKAsyFy5zcGFyay5jb25uZWN0LlRvU2NoZW1hSAASSwoZcmVwYXJ0aXRpb25fYnlfZXhwcmVzc2lvbhgbIAEoCzImLnNwYXJrLmNvbm5lY3QuUmVwYXJ0aXRpb25CeUV4cHJlc3Npb25IABI2Cg5tYXBfcGFydGl0aW9ucxgcIAEoCzIcLnNwYXJrLmNvbm5lY3QuTWFwUGFydGl0aW9uc0gAEjgKD2NvbGxlY3RfbWV0cmljcxgdIAEoCzIdLnNwYXJrLmNvbm5lY3QuQ29sbGVjdE1ldHJpY3NIABIlCgVwYXJzZRgeIAEoCzIULnNwYXJrLmNvbm5lY3QuUGFyc2VIABIsCglncm91cF9tYXAYHyABKAsyFy5zcGFyay5jb25uZWN0Lkdyb3VwTWFwSAASMQoMY29fZ3JvdXBfbWFwGCAgASgLMhkuc3BhcmsuY29ubmVjdC5Db0dyb3VwTWFwSAASNgoOd2l0aF93YXRlcm1hcmsYISABKAsyHC5zcGFyay5jb25uZWN0LldpdGhXYXRlcm1hcmtIABJLChphcHBseV9pbl9wYW5kYXNfd2l0aF9zdGF0ZRgiIAEoCzIlLnNwYXJrLmNvbm5lY3QuQXBwbHlJblBhbmRhc1dpdGhTdGF0ZUgAEjAKC2h0bWxfc3RyaW5nGCMgASgLMhkuc3BhcmsuY29ubmVjdC5IdG1sU3RyaW5nSAASQwoVY2FjaGVkX2xvY2FsX3JlbGF0aW9uGCQgASgLMiIuc3BhcmsuY29ubmVjdC5DYWNoZWRMb2NhbFJlbGF0aW9uSAASRQoWY2FjaGVkX3JlbW90ZV9yZWxhdGlvbhglIAEoCzIjLnNwYXJrLmNvbm5lY3QuQ2FjaGVkUmVtb3RlUmVsYXRpb25IABJoCiljb21tb25faW5saW5lX3VzZXJfZGVmaW5lZF90YWJsZV9mdW5jdGlvbhgmIAEoCzIzLnNwYXJrLmNvbm5lY3QuQ29tbW9uSW5saW5lVXNlckRlZmluZWRUYWJsZUZ1bmN0aW9uSAASLQoKYXNfb2Zfam9pbhgnIAEoCzIXLnNwYXJrLmNvbm5lY3QuQXNPZkpvaW5IABJiCiZjb21tb25faW5saW5lX3VzZXJfZGVmaW5lZF9kYXRhX3NvdXJjZRgoIAEoCzIwLnNwYXJrLmNvbm5lY3QuQ29tbW9uSW5saW5lVXNlckRlZmluZWREYXRhU291cmNlSAASNgoOd2l0aF9yZWxhdGlvbnMYKSABKAsyHC5zcGFyay5jb25uZWN0LldpdGhSZWxhdGlvbnNIABItCgl0cmFuc3Bvc2UYKiABKAsyGC5zcGFyay5jb25uZWN0LlRyYW5zcG9zZUgAElgKIHVucmVzb2x2ZWRfdGFibGVfdmFsdWVkX2Z1bmN0aW9uGCsgASgLMiwuc3BhcmsuY29ubmVjdC5VbnJlc29sdmVkVGFibGVWYWx1ZWRGdW5jdGlvbkgAEigKB2ZpbGxfbmEYWiABKAsyFS5zcGFyay5jb25uZWN0Lk5BRmlsbEgAEigKB2Ryb3BfbmEYWyABKAsyFS5zcGFyay5jb25uZWN0Lk5BRHJvcEgAEisKB3JlcGxhY2UYXCABKAsyGC5zcGFyay5jb25uZWN0Lk5BUmVwbGFjZUgAEi0KB3N1bW1hcnkYZCABKAsyGi5zcGFyay5jb25uZWN0LlN0YXRTdW1tYXJ5SAASLwoIY3Jvc3N0YWIYZSABKAsyGy5zcGFyay5jb25uZWN0LlN0YXRDcm9zc3RhYkgAEi8KCGRlc2NyaWJlGGYgASgLMhsuc3BhcmsuY29ubmVjdC5TdGF0RGVzY3JpYmVIABIlCgNjb3YYZyABKAsyFi5zcGFyay5jb25uZWN0LlN0YXRDb3ZIABInCgRjb3JyGGggASgLMhcuc3BhcmsuY29ubmVjdC5TdGF0Q29yckgAEjwKD2FwcHJveF9xdWFudGlsZRhpIAEoCzIhLnNwYXJrLmNvbm5lY3QuU3RhdEFwcHJveFF1YW50aWxlSAASMgoKZnJlcV9pdGVtcxhqIAEoCzIcLnNwYXJrLmNvbm5lY3QuU3RhdEZyZXFJdGVtc0gAEjAKCXNhbXBsZV9ieRhrIAEoCzIbLnNwYXJrLmNvbm5lY3QuU3RhdFNhbXBsZUJ5SAASKgoHY2F0YWxvZxjIASABKAsyFi5zcGFyay5jb25uZWN0LkNhdGFsb2dIABIqCglleHRlbnNpb24Y5gcgASgLMhQuZ29vZ2xlLnByb3RvYnVmLkFueUgAEioKB3Vua25vd24Y5wcgASgLMhYuc3BhcmsuY29ubmVjdC5Vbmtub3duSABCCgoIcmVsX3R5cGUiCQoHVW5rbm93biJyCg5SZWxhdGlvbkNvbW1vbhIXCgtzb3VyY2VfaW5mbxgBIAEoCUICGAESFAoHcGxhbl9pZBgCIAEoA0gAiAEBEiUKBm9yaWdpbhgDIAEoCzIVLnNwYXJrLmNvbm5lY3QuT3JpZ2luQgoKCF9wbGFuX2lkIpIDCgNTUUwSDQoFcXVlcnkYASABKAkSLgoEYXJncxgCIAMoCzIcLnNwYXJrLmNvbm5lY3QuU1FMLkFyZ3NFbnRyeUICGAESNwoIcG9zX2FyZ3MYAyADKAsyIS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24uTGl0ZXJhbEICGAESPwoPbmFtZWRfYXJndW1lbnRzGAQgAygLMiYuc3BhcmsuY29ubmVjdC5TUUwuTmFtZWRBcmd1bWVudHNFbnRyeRIwCg1wb3NfYXJndW1lbnRzGAUgAygLMhkuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uGk4KCUFyZ3NFbnRyeRILCgNrZXkYASABKAkSMAoFdmFsdWUYAiABKAsyIS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24uTGl0ZXJhbDoCOAEaUAoTTmFtZWRBcmd1bWVudHNFbnRyeRILCgNrZXkYASABKAkSKAoFdmFsdWUYAiABKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb246AjgBImMKDVdpdGhSZWxhdGlvbnMSJQoEcm9vdBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SKwoKcmVmZXJlbmNlcxgCIAMoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24ikQQKBFJlYWQSNQoLbmFtZWRfdGFibGUYASABKAsyHi5zcGFyay5jb25uZWN0LlJlYWQuTmFtZWRUYWJsZUgAEjUKC2RhdGFfc291cmNlGAIgASgLMh4uc3BhcmsuY29ubmVjdC5SZWFkLkRhdGFTb3VyY2VIABIUCgxpc19zdHJlYW1pbmcYAyABKAgalwEKCk5hbWVkVGFibGUSGwoTdW5wYXJzZWRfaWRlbnRpZmllchgBIAEoCRI8CgdvcHRpb25zGAIgAygLMisuc3BhcmsuY29ubmVjdC5SZWFkLk5hbWVkVGFibGUuT3B0aW9uc0VudHJ5Gi4KDE9wdGlvbnNFbnRyeRILCgNrZXkYASABKAkSDQoFdmFsdWUYAiABKAk6AjgBGt0BCgpEYXRhU291cmNlEhMKBmZvcm1hdBgBIAEoCUgAiAEBEhMKBnNjaGVtYRgCIAEoCUgBiAEBEjwKB29wdGlvbnMYAyADKAsyKy5zcGFyay5jb25uZWN0LlJlYWQuRGF0YVNvdXJjZS5PcHRpb25zRW50cnkSDQoFcGF0aHMYBCADKAkSEgoKcHJlZGljYXRlcxgFIAMoCRouCgxPcHRpb25zRW50cnkSCwoDa2V5GAEgASgJEg0KBXZhbHVlGAIgASgJOgI4AUIJCgdfZm9ybWF0QgkKB19zY2hlbWFCCwoJcmVhZF90eXBlImEKB1Byb2plY3QSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEi4KC2V4cHJlc3Npb25zGAMgAygLMhkuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uIl4KBkZpbHRlchImCgVpbnB1dBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SLAoJY29uZGl0aW9uGAIgASgLMhkuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uIrYECgRKb2luEiUKBGxlZnQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEiYKBXJpZ2h0GAIgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIxCg5qb2luX2NvbmRpdGlvbhgDIAEoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbhIvCglqb2luX3R5cGUYBCABKA4yHC5zcGFyay5jb25uZWN0LkpvaW4uSm9pblR5cGUSFQoNdXNpbmdfY29sdW1ucxgFIAMoCRI9Cg5qb2luX2RhdGFfdHlwZRgGIAEoCzIgLnNwYXJrLmNvbm5lY3QuSm9pbi5Kb2luRGF0YVR5cGVIAIgBARo/CgxKb2luRGF0YVR5cGUSFgoOaXNfbGVmdF9zdHJ1Y3QYASABKAgSFwoPaXNfcmlnaHRfc3RydWN0GAIgASgIItABCghKb2luVHlwZRIZChVKT0lOX1RZUEVfVU5TUEVDSUZJRUQQABITCg9KT0lOX1RZUEVfSU5ORVIQARIYChRKT0lOX1RZUEVfRlVMTF9PVVRFUhACEhgKFEpPSU5fVFlQRV9MRUZUX09VVEVSEAMSGQoVSk9JTl9UWVBFX1JJR0hUX09VVEVSEAQSFwoTSk9JTl9UWVBFX0xFRlRfQU5USRAFEhcKE0pPSU5fVFlQRV9MRUZUX1NFTUkQBhITCg9KT0lOX1RZUEVfQ1JPU1MQB0IRCg9fam9pbl9kYXRhX3R5cGUimQMKDFNldE9wZXJhdGlvbhIrCgpsZWZ0X2lucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIsCgtyaWdodF9pbnB1dBgCIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SOgoLc2V0X29wX3R5cGUYAyABKA4yJS5zcGFyay5jb25uZWN0LlNldE9wZXJhdGlvbi5TZXRPcFR5cGUSEwoGaXNfYWxsGAQgASgISACIAQESFAoHYnlfbmFtZRgFIAEoCEgBiAEBEiIKFWFsbG93X21pc3NpbmdfY29sdW1ucxgGIAEoCEgCiAEBInIKCVNldE9wVHlwZRIbChdTRVRfT1BfVFlQRV9VTlNQRUNJRklFRBAAEhkKFVNFVF9PUF9UWVBFX0lOVEVSU0VDVBABEhUKEVNFVF9PUF9UWVBFX1VOSU9OEAISFgoSU0VUX09QX1RZUEVfRVhDRVBUEANCCQoHX2lzX2FsbEIKCghfYnlfbmFtZUIYChZfYWxsb3dfbWlzc2luZ19jb2x1bW5zIj4KBUxpbWl0EiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhINCgVsaW1pdBgCIAEoBSJACgZPZmZzZXQSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEg4KBm9mZnNldBgCIAEoBSI9CgRUYWlsEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhINCgVsaW1pdBgCIAEoBSKSBQoJQWdncmVnYXRlEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhI2Cgpncm91cF90eXBlGAIgASgOMiIuc3BhcmsuY29ubmVjdC5BZ2dyZWdhdGUuR3JvdXBUeXBlEjcKFGdyb3VwaW5nX2V4cHJlc3Npb25zGAMgAygLMhkuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uEjgKFWFnZ3JlZ2F0ZV9leHByZXNzaW9ucxgEIAMoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbhItCgVwaXZvdBgFIAEoCzIeLnNwYXJrLmNvbm5lY3QuQWdncmVnYXRlLlBpdm90EjwKDWdyb3VwaW5nX3NldHMYBiADKAsyJS5zcGFyay5jb25uZWN0LkFnZ3JlZ2F0ZS5Hcm91cGluZ1NldHMaYgoFUGl2b3QSJgoDY29sGAEgASgLMhkuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uEjEKBnZhbHVlcxgCIAMoCzIhLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbi5MaXRlcmFsGj8KDEdyb3VwaW5nU2V0cxIvCgxncm91cGluZ19zZXQYASADKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24inwEKCUdyb3VwVHlwZRIaChZHUk9VUF9UWVBFX1VOU1BFQ0lGSUVEEAASFgoSR1JPVVBfVFlQRV9HUk9VUEJZEAESFQoRR1JPVVBfVFlQRV9ST0xMVVAQAhITCg9HUk9VUF9UWVBFX0NVQkUQAxIUChBHUk9VUF9UWVBFX1BJVk9UEAQSHAoYR1JPVVBfVFlQRV9HUk9VUElOR19TRVRTEAUiiAEKBFNvcnQSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEjIKBW9yZGVyGAIgAygLMiMuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uLlNvcnRPcmRlchIWCglpc19nbG9iYWwYAyABKAhIAIgBAUIMCgpfaXNfZ2xvYmFsInAKBERyb3ASJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEioKB2NvbHVtbnMYAiADKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24SFAoMY29sdW1uX25hbWVzGAMgAygJIrkBCgtEZWR1cGxpY2F0ZRImCgVpbnB1dBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SFAoMY29sdW1uX25hbWVzGAIgAygJEiAKE2FsbF9jb2x1bW5zX2FzX2tleXMYAyABKAhIAIgBARIdChB3aXRoaW5fd2F0ZXJtYXJrGAQgASgISAGIAQFCFgoUX2FsbF9jb2x1bW5zX2FzX2tleXNCEwoRX3dpdGhpbl93YXRlcm1hcmsiSwoNTG9jYWxSZWxhdGlvbhIRCgRkYXRhGAEgASgMSACIAQESEwoGc2NoZW1hGAIgASgJSAGIAQFCBwoFX2RhdGFCCQoHX3NjaGVtYSJCChNDYWNoZWRMb2NhbFJlbGF0aW9uEgwKBGhhc2gYAyABKAlKBAgBEAJKBAgCEANSBnVzZXJJZFIJc2Vzc2lvbklkIisKFENhY2hlZFJlbW90ZVJlbGF0aW9uEhMKC3JlbGF0aW9uX2lkGAEgASgJIscBCgZTYW1wbGUSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEhMKC2xvd2VyX2JvdW5kGAIgASgBEhMKC3VwcGVyX2JvdW5kGAMgASgBEh0KEHdpdGhfcmVwbGFjZW1lbnQYBCABKAhIAIgBARIRCgRzZWVkGAUgASgDSAGIAQESGwoTZGV0ZXJtaW5pc3RpY19vcmRlchgGIAEoCEITChFfd2l0aF9yZXBsYWNlbWVudEIHCgVfc2VlZCJwCgVSYW5nZRISCgVzdGFydBgBIAEoA0gAiAEBEgsKA2VuZBgCIAEoAxIMCgRzdGVwGAMgASgDEhsKDm51bV9wYXJ0aXRpb25zGAQgASgFSAGIAQFCCAoGX3N0YXJ0QhEKD19udW1fcGFydGl0aW9ucyJZCg1TdWJxdWVyeUFsaWFzEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhINCgVhbGlhcxgCIAEoCRIRCglxdWFsaWZpZXIYAyADKAkibwoLUmVwYXJ0aXRpb24SJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEhYKDm51bV9wYXJ0aXRpb25zGAIgASgFEhQKB3NodWZmbGUYAyABKAhIAIgBAUIKCghfc2h1ZmZsZSJqCgpTaG93U3RyaW5nEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIQCghudW1fcm93cxgCIAEoBRIQCgh0cnVuY2F0ZRgDIAEoBRIQCgh2ZXJ0aWNhbBgEIAEoCCJYCgpIdG1sU3RyaW5nEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIQCghudW1fcm93cxgCIAEoBRIQCgh0cnVuY2F0ZRgDIAEoBSJJCgtTdGF0U3VtbWFyeRImCgVpbnB1dBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SEgoKc3RhdGlzdGljcxgCIAMoCSJECgxTdGF0RGVzY3JpYmUSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEgwKBGNvbHMYAiADKAkiUgoMU3RhdENyb3NzdGFiEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIMCgRjb2wxGAIgASgJEgwKBGNvbDIYAyABKAkiTQoHU3RhdENvdhImCgVpbnB1dBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SDAoEY29sMRgCIAEoCRIMCgRjb2wyGAMgASgJIm4KCFN0YXRDb3JyEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIMCgRjb2wxGAIgASgJEgwKBGNvbDIYAyABKAkSEwoGbWV0aG9kGAQgASgJSACIAQFCCQoHX21ldGhvZCJ5ChJTdGF0QXBwcm94UXVhbnRpbGUSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEgwKBGNvbHMYAiADKAkSFQoNcHJvYmFiaWxpdGllcxgDIAMoARIWCg5yZWxhdGl2ZV9lcnJvchgEIAEoASJnCg1TdGF0RnJlcUl0ZW1zEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIMCgRjb2xzGAIgAygJEhQKB3N1cHBvcnQYAyABKAFIAIgBAUIKCghfc3VwcG9ydCKFAgoMU3RhdFNhbXBsZUJ5EiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhImCgNjb2wYAiABKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24SNwoJZnJhY3Rpb25zGAMgAygLMiQuc3BhcmsuY29ubmVjdC5TdGF0U2FtcGxlQnkuRnJhY3Rpb24SEQoEc2VlZBgFIAEoA0gAiAEBGlAKCEZyYWN0aW9uEjIKB3N0cmF0dW0YASABKAsyIS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24uTGl0ZXJhbBIQCghmcmFjdGlvbhgCIAEoAUIHCgVfc2VlZCJxCgZOQUZpbGwSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEgwKBGNvbHMYAiADKAkSMQoGdmFsdWVzGAMgAygLMiEuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uLkxpdGVyYWwibAoGTkFEcm9wEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIMCgRjb2xzGAIgAygJEhoKDW1pbl9ub25fbnVsbHMYAyABKAVIAIgBAUIQCg5fbWluX25vbl9udWxscyL4AQoJTkFSZXBsYWNlEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIMCgRjb2xzGAIgAygJEjoKDHJlcGxhY2VtZW50cxgDIAMoCzIkLnNwYXJrLmNvbm5lY3QuTkFSZXBsYWNlLlJlcGxhY2VtZW50GnkKC1JlcGxhY2VtZW50EjQKCW9sZF92YWx1ZRgBIAEoCzIhLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbi5MaXRlcmFsEjQKCW5ld192YWx1ZRgCIAEoCzIhLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbi5MaXRlcmFsIkQKBFRvREYSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEhQKDGNvbHVtbl9uYW1lcxgCIAMoCSK7AgoSV2l0aENvbHVtbnNSZW5hbWVkEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhJXChJyZW5hbWVfY29sdW1uc19tYXAYAiADKAsyNy5zcGFyay5jb25uZWN0LldpdGhDb2x1bW5zUmVuYW1lZC5SZW5hbWVDb2x1bW5zTWFwRW50cnlCAhgBEjkKB3JlbmFtZXMYAyADKAsyKC5zcGFyay5jb25uZWN0LldpdGhDb2x1bW5zUmVuYW1lZC5SZW5hbWUaNwoVUmVuYW1lQ29sdW1uc01hcEVudHJ5EgsKA2tleRgBIAEoCRINCgV2YWx1ZRgCIAEoCToCOAEaMAoGUmVuYW1lEhAKCGNvbF9uYW1lGAEgASgJEhQKDG5ld19jb2xfbmFtZRgCIAEoCSJnCgtXaXRoQ29sdW1ucxImCgVpbnB1dBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SMAoHYWxpYXNlcxgCIAMoCzIfLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbi5BbGlhcyJkCg1XaXRoV2F0ZXJtYXJrEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhISCgpldmVudF90aW1lGAIgASgJEhcKD2RlbGF5X3RocmVzaG9sZBgDIAEoCSJrCgRIaW50EiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIMCgRuYW1lGAIgASgJEi0KCnBhcmFtZXRlcnMYAyADKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24ihgIKB1VucGl2b3QSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEiYKA2lkcxgCIAMoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbhIyCgZ2YWx1ZXMYAyABKAsyHS5zcGFyay5jb25uZWN0LlVucGl2b3QuVmFsdWVzSACIAQESHAoUdmFyaWFibGVfY29sdW1uX25hbWUYBCABKAkSGQoRdmFsdWVfY29sdW1uX25hbWUYBSABKAkaMwoGVmFsdWVzEikKBnZhbHVlcxgBIAMoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbkIJCgdfdmFsdWVzImUKCVRyYW5zcG9zZRImCgVpbnB1dBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SMAoNaW5kZXhfY29sdW1ucxgCIAMoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbiJkCh1VbnJlc29sdmVkVGFibGVWYWx1ZWRGdW5jdGlvbhIVCg1mdW5jdGlvbl9uYW1lGAEgASgJEiwKCWFyZ3VtZW50cxgCIAMoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbiJbCghUb1NjaGVtYRImCgVpbnB1dBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SJwoGc2NoZW1hGAIgASgLMhcuc3BhcmsuY29ubmVjdC5EYXRhVHlwZSKlAQoXUmVwYXJ0aXRpb25CeUV4cHJlc3Npb24SJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEjIKD3BhcnRpdGlvbl9leHBycxgCIAMoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbhIbCg5udW1fcGFydGl0aW9ucxgDIAEoBUgAiAEBQhEKD19udW1fcGFydGl0aW9ucyLFAQoNTWFwUGFydGl0aW9ucxImCgVpbnB1dBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SPAoEZnVuYxgCIAEoCzIuLnNwYXJrLmNvbm5lY3QuQ29tbW9uSW5saW5lVXNlckRlZmluZWRGdW5jdGlvbhIXCgppc19iYXJyaWVyGAMgASgISACIAQESFwoKcHJvZmlsZV9pZBgEIAEoBUgBiAEBQg0KC19pc19iYXJyaWVyQg0KC19wcm9maWxlX2lkIuwDCghHcm91cE1hcBImCgVpbnB1dBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SNwoUZ3JvdXBpbmdfZXhwcmVzc2lvbnMYAiADKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24SPAoEZnVuYxgDIAEoCzIuLnNwYXJrLmNvbm5lY3QuQ29tbW9uSW5saW5lVXNlckRlZmluZWRGdW5jdGlvbhI2ChNzb3J0aW5nX2V4cHJlc3Npb25zGAQgAygLMhkuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uEi4KDWluaXRpYWxfaW5wdXQYBSABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEj8KHGluaXRpYWxfZ3JvdXBpbmdfZXhwcmVzc2lvbnMYBiADKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24SJQoYaXNfbWFwX2dyb3Vwc193aXRoX3N0YXRlGAcgASgISACIAQESGAoLb3V0cHV0X21vZGUYCCABKAlIAYgBARIZCgx0aW1lb3V0X2NvbmYYCSABKAlIAogBAUIbChlfaXNfbWFwX2dyb3Vwc193aXRoX3N0YXRlQg4KDF9vdXRwdXRfbW9kZUIPCg1fdGltZW91dF9jb25mIpQDCgpDb0dyb3VwTWFwEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhI9ChppbnB1dF9ncm91cGluZ19leHByZXNzaW9ucxgCIAMoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbhImCgVvdGhlchgDIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SPQoab3RoZXJfZ3JvdXBpbmdfZXhwcmVzc2lvbnMYBCADKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24SPAoEZnVuYxgFIAEoCzIuLnNwYXJrLmNvbm5lY3QuQ29tbW9uSW5saW5lVXNlckRlZmluZWRGdW5jdGlvbhI8ChlpbnB1dF9zb3J0aW5nX2V4cHJlc3Npb25zGAYgAygLMhkuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uEjwKGW90aGVyX3NvcnRpbmdfZXhwcmVzc2lvbnMYByADKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24ijwIKFkFwcGx5SW5QYW5kYXNXaXRoU3RhdGUSJgoFaW5wdXQYASABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEjcKFGdyb3VwaW5nX2V4cHJlc3Npb25zGAIgAygLMhkuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uEjwKBGZ1bmMYAyABKAsyLi5zcGFyay5jb25uZWN0LkNvbW1vbklubGluZVVzZXJEZWZpbmVkRnVuY3Rpb24SFQoNb3V0cHV0X3NjaGVtYRgEIAEoCRIUCgxzdGF0ZV9zY2hlbWEYBSABKAkSEwoLb3V0cHV0X21vZGUYBiABKAkSFAoMdGltZW91dF9jb25mGAcgASgJIsABCiRDb21tb25JbmxpbmVVc2VyRGVmaW5lZFRhYmxlRnVuY3Rpb24SFQoNZnVuY3Rpb25fbmFtZRgBIAEoCRIVCg1kZXRlcm1pbmlzdGljGAIgASgIEiwKCWFyZ3VtZW50cxgDIAMoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbhIwCgtweXRob25fdWR0ZhgEIAEoCzIZLnNwYXJrLmNvbm5lY3QuUHl0aG9uVURURkgAQgoKCGZ1bmN0aW9uIocBCgpQeXRob25VRFRGEjEKC3JldHVybl90eXBlGAEgASgLMhcuc3BhcmsuY29ubmVjdC5EYXRhVHlwZUgAiAEBEhEKCWV2YWxfdHlwZRgCIAEoBRIPCgdjb21tYW5kGAMgASgMEhIKCnB5dGhvbl92ZXIYBCABKAlCDgoMX3JldHVybl90eXBlIn8KIUNvbW1vbklubGluZVVzZXJEZWZpbmVkRGF0YVNvdXJjZRIMCgRuYW1lGAEgASgJEj0KEnB5dGhvbl9kYXRhX3NvdXJjZRgCIAEoCzIfLnNwYXJrLmNvbm5lY3QuUHl0aG9uRGF0YVNvdXJjZUgAQg0KC2RhdGFfc291cmNlIjcKEFB5dGhvbkRhdGFTb3VyY2USDwoHY29tbWFuZBgBIAEoDBISCgpweXRob25fdmVyGAIgASgJInIKDkNvbGxlY3RNZXRyaWNzEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIMCgRuYW1lGAIgASgJEioKB21ldHJpY3MYAyADKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24i2AIKBVBhcnNlEiYKBWlucHV0GAEgASgLMhcuc3BhcmsuY29ubmVjdC5SZWxhdGlvbhIwCgZmb3JtYXQYAiABKA4yIC5zcGFyay5jb25uZWN0LlBhcnNlLlBhcnNlRm9ybWF0EiwKBnNjaGVtYRgDIAEoCzIXLnNwYXJrLmNvbm5lY3QuRGF0YVR5cGVIAIgBARIyCgdvcHRpb25zGAQgAygLMiEuc3BhcmsuY29ubmVjdC5QYXJzZS5PcHRpb25zRW50cnkaLgoMT3B0aW9uc0VudHJ5EgsKA2tleRgBIAEoCRINCgV2YWx1ZRgCIAEoCToCOAEiWAoLUGFyc2VGb3JtYXQSHAoYUEFSU0VfRk9STUFUX1VOU1BFQ0lGSUVEEAASFAoQUEFSU0VfRk9STUFUX0NTVhABEhUKEVBBUlNFX0ZPUk1BVF9KU09OEAJCCQoHX3NjaGVtYSLuAgoIQXNPZkpvaW4SJQoEbGVmdBgBIAEoCzIXLnNwYXJrLmNvbm5lY3QuUmVsYXRpb24SJgoFcmlnaHQYAiABKAsyFy5zcGFyay5jb25uZWN0LlJlbGF0aW9uEi0KCmxlZnRfYXNfb2YYAyABKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24SLgoLcmlnaHRfYXNfb2YYBCABKAsyGS5zcGFyay5jb25uZWN0LkV4cHJlc3Npb24SLAoJam9pbl9leHByGAUgASgLMhkuc3BhcmsuY29ubmVjdC5FeHByZXNzaW9uEhUKDXVzaW5nX2NvbHVtbnMYBiADKAkSEQoJam9pbl90eXBlGAcgASgJEiwKCXRvbGVyYW5jZRgIIAEoCzIZLnNwYXJrLmNvbm5lY3QuRXhwcmVzc2lvbhIbChNhbGxvd19leGFjdF9tYXRjaGVzGAkgASgIEhEKCWRpcmVjdGlvbhgKIAEoCUI2Ch5vcmcuYXBhY2hlLnNwYXJrLmNvbm5lY3QucHJvdG9QAVoSaW50ZXJuYWwvZ2VuZXJhdGVkYgZwcm90bzM", [file_google_protobuf_any, file_spark_connect_expressions, file_spark_connect_types, file_spark_connect_catalog, file_spark_connect_common]);

/**
 * The main [[Relation]] type. Fundamentally, a relation is a typed container
 * that has exactly one explicit relation type set.
 *
 * When adding new relation types, they have to be registered here.
 *
 * @generated from message spark.connect.Relation
 */
export type Relation = Message<"spark.connect.Relation"> & {
  /**
   * @generated from field: spark.connect.RelationCommon common = 1;
   */
  common?: RelationCommon;

  /**
   * @generated from oneof spark.connect.Relation.rel_type
   */
  relType: {
    /**
     * @generated from field: spark.connect.Read read = 2;
     */
    value: Read;
    case: "read";
  } | {
    /**
     * @generated from field: spark.connect.Project project = 3;
     */
    value: Project;
    case: "project";
  } | {
    /**
     * @generated from field: spark.connect.Filter filter = 4;
     */
    value: Filter;
    case: "filter";
  } | {
    /**
     * @generated from field: spark.connect.Join join = 5;
     */
    value: Join;
    case: "join";
  } | {
    /**
     * @generated from field: spark.connect.SetOperation set_op = 6;
     */
    value: SetOperation;
    case: "setOp";
  } | {
    /**
     * @generated from field: spark.connect.Sort sort = 7;
     */
    value: Sort;
    case: "sort";
  } | {
    /**
     * @generated from field: spark.connect.Limit limit = 8;
     */
    value: Limit;
    case: "limit";
  } | {
    /**
     * @generated from field: spark.connect.Aggregate aggregate = 9;
     */
    value: Aggregate;
    case: "aggregate";
  } | {
    /**
     * @generated from field: spark.connect.SQL sql = 10;
     */
    value: SQL;
    case: "sql";
  } | {
    /**
     * @generated from field: spark.connect.LocalRelation local_relation = 11;
     */
    value: LocalRelation;
    case: "localRelation";
  } | {
    /**
     * @generated from field: spark.connect.Sample sample = 12;
     */
    value: Sample;
    case: "sample";
  } | {
    /**
     * @generated from field: spark.connect.Offset offset = 13;
     */
    value: Offset;
    case: "offset";
  } | {
    /**
     * @generated from field: spark.connect.Deduplicate deduplicate = 14;
     */
    value: Deduplicate;
    case: "deduplicate";
  } | {
    /**
     * @generated from field: spark.connect.Range range = 15;
     */
    value: Range;
    case: "range";
  } | {
    /**
     * @generated from field: spark.connect.SubqueryAlias subquery_alias = 16;
     */
    value: SubqueryAlias;
    case: "subqueryAlias";
  } | {
    /**
     * @generated from field: spark.connect.Repartition repartition = 17;
     */
    value: Repartition;
    case: "repartition";
  } | {
    /**
     * @generated from field: spark.connect.ToDF to_df = 18;
     */
    value: ToDF;
    case: "toDf";
  } | {
    /**
     * @generated from field: spark.connect.WithColumnsRenamed with_columns_renamed = 19;
     */
    value: WithColumnsRenamed;
    case: "withColumnsRenamed";
  } | {
    /**
     * @generated from field: spark.connect.ShowString show_string = 20;
     */
    value: ShowString;
    case: "showString";
  } | {
    /**
     * @generated from field: spark.connect.Drop drop = 21;
     */
    value: Drop;
    case: "drop";
  } | {
    /**
     * @generated from field: spark.connect.Tail tail = 22;
     */
    value: Tail;
    case: "tail";
  } | {
    /**
     * @generated from field: spark.connect.WithColumns with_columns = 23;
     */
    value: WithColumns;
    case: "withColumns";
  } | {
    /**
     * @generated from field: spark.connect.Hint hint = 24;
     */
    value: Hint;
    case: "hint";
  } | {
    /**
     * @generated from field: spark.connect.Unpivot unpivot = 25;
     */
    value: Unpivot;
    case: "unpivot";
  } | {
    /**
     * @generated from field: spark.connect.ToSchema to_schema = 26;
     */
    value: ToSchema;
    case: "toSchema";
  } | {
    /**
     * @generated from field: spark.connect.RepartitionByExpression repartition_by_expression = 27;
     */
    value: RepartitionByExpression;
    case: "repartitionByExpression";
  } | {
    /**
     * @generated from field: spark.connect.MapPartitions map_partitions = 28;
     */
    value: MapPartitions;
    case: "mapPartitions";
  } | {
    /**
     * @generated from field: spark.connect.CollectMetrics collect_metrics = 29;
     */
    value: CollectMetrics;
    case: "collectMetrics";
  } | {
    /**
     * @generated from field: spark.connect.Parse parse = 30;
     */
    value: Parse;
    case: "parse";
  } | {
    /**
     * @generated from field: spark.connect.GroupMap group_map = 31;
     */
    value: GroupMap;
    case: "groupMap";
  } | {
    /**
     * @generated from field: spark.connect.CoGroupMap co_group_map = 32;
     */
    value: CoGroupMap;
    case: "coGroupMap";
  } | {
    /**
     * @generated from field: spark.connect.WithWatermark with_watermark = 33;
     */
    value: WithWatermark;
    case: "withWatermark";
  } | {
    /**
     * @generated from field: spark.connect.ApplyInPandasWithState apply_in_pandas_with_state = 34;
     */
    value: ApplyInPandasWithState;
    case: "applyInPandasWithState";
  } | {
    /**
     * @generated from field: spark.connect.HtmlString html_string = 35;
     */
    value: HtmlString;
    case: "htmlString";
  } | {
    /**
     * @generated from field: spark.connect.CachedLocalRelation cached_local_relation = 36;
     */
    value: CachedLocalRelation;
    case: "cachedLocalRelation";
  } | {
    /**
     * @generated from field: spark.connect.CachedRemoteRelation cached_remote_relation = 37;
     */
    value: CachedRemoteRelation;
    case: "cachedRemoteRelation";
  } | {
    /**
     * @generated from field: spark.connect.CommonInlineUserDefinedTableFunction common_inline_user_defined_table_function = 38;
     */
    value: CommonInlineUserDefinedTableFunction;
    case: "commonInlineUserDefinedTableFunction";
  } | {
    /**
     * @generated from field: spark.connect.AsOfJoin as_of_join = 39;
     */
    value: AsOfJoin;
    case: "asOfJoin";
  } | {
    /**
     * @generated from field: spark.connect.CommonInlineUserDefinedDataSource common_inline_user_defined_data_source = 40;
     */
    value: CommonInlineUserDefinedDataSource;
    case: "commonInlineUserDefinedDataSource";
  } | {
    /**
     * @generated from field: spark.connect.WithRelations with_relations = 41;
     */
    value: WithRelations;
    case: "withRelations";
  } | {
    /**
     * @generated from field: spark.connect.Transpose transpose = 42;
     */
    value: Transpose;
    case: "transpose";
  } | {
    /**
     * @generated from field: spark.connect.UnresolvedTableValuedFunction unresolved_table_valued_function = 43;
     */
    value: UnresolvedTableValuedFunction;
    case: "unresolvedTableValuedFunction";
  } | {
    /**
     * NA functions
     *
     * @generated from field: spark.connect.NAFill fill_na = 90;
     */
    value: NAFill;
    case: "fillNa";
  } | {
    /**
     * @generated from field: spark.connect.NADrop drop_na = 91;
     */
    value: NADrop;
    case: "dropNa";
  } | {
    /**
     * @generated from field: spark.connect.NAReplace replace = 92;
     */
    value: NAReplace;
    case: "replace";
  } | {
    /**
     * stat functions
     *
     * @generated from field: spark.connect.StatSummary summary = 100;
     */
    value: StatSummary;
    case: "summary";
  } | {
    /**
     * @generated from field: spark.connect.StatCrosstab crosstab = 101;
     */
    value: StatCrosstab;
    case: "crosstab";
  } | {
    /**
     * @generated from field: spark.connect.StatDescribe describe = 102;
     */
    value: StatDescribe;
    case: "describe";
  } | {
    /**
     * @generated from field: spark.connect.StatCov cov = 103;
     */
    value: StatCov;
    case: "cov";
  } | {
    /**
     * @generated from field: spark.connect.StatCorr corr = 104;
     */
    value: StatCorr;
    case: "corr";
  } | {
    /**
     * @generated from field: spark.connect.StatApproxQuantile approx_quantile = 105;
     */
    value: StatApproxQuantile;
    case: "approxQuantile";
  } | {
    /**
     * @generated from field: spark.connect.StatFreqItems freq_items = 106;
     */
    value: StatFreqItems;
    case: "freqItems";
  } | {
    /**
     * @generated from field: spark.connect.StatSampleBy sample_by = 107;
     */
    value: StatSampleBy;
    case: "sampleBy";
  } | {
    /**
     * Catalog API (experimental / unstable)
     *
     * @generated from field: spark.connect.Catalog catalog = 200;
     */
    value: Catalog;
    case: "catalog";
  } | {
    /**
     * This field is used to mark extensions to the protocol. When plugins generate arbitrary
     * relations they can add them here. During the planning the correct resolution is done.
     *
     * @generated from field: google.protobuf.Any extension = 998;
     */
    value: Any;
    case: "extension";
  } | {
    /**
     * @generated from field: spark.connect.Unknown unknown = 999;
     */
    value: Unknown;
    case: "unknown";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.Relation.
 * Use `create(RelationSchema)` to create a new message.
 */
export const RelationSchema: GenMessage<Relation> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 0);

/**
 * Used for testing purposes only.
 *
 * @generated from message spark.connect.Unknown
 */
export type Unknown = Message<"spark.connect.Unknown"> & {
};

/**
 * Describes the message spark.connect.Unknown.
 * Use `create(UnknownSchema)` to create a new message.
 */
export const UnknownSchema: GenMessage<Unknown> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 1);

/**
 * Common metadata of all relations.
 *
 * @generated from message spark.connect.RelationCommon
 */
export type RelationCommon = Message<"spark.connect.RelationCommon"> & {
  /**
   * (Required) Shared relation metadata.
   *
   * @generated from field: string source_info = 1 [deprecated = true];
   * @deprecated
   */
  sourceInfo: string;

  /**
   * (Optional) A per-client globally unique id for a given connect plan.
   *
   * @generated from field: optional int64 plan_id = 2;
   */
  planId?: bigint;

  /**
   * (Optional) Keep the information of the origin for this expression such as stacktrace.
   *
   * @generated from field: spark.connect.Origin origin = 3;
   */
  origin?: Origin;
};

/**
 * Describes the message spark.connect.RelationCommon.
 * Use `create(RelationCommonSchema)` to create a new message.
 */
export const RelationCommonSchema: GenMessage<RelationCommon> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 2);

/**
 * Relation that uses a SQL query to generate the output.
 *
 * @generated from message spark.connect.SQL
 */
export type SQL = Message<"spark.connect.SQL"> & {
  /**
   * (Required) The SQL query.
   *
   * @generated from field: string query = 1;
   */
  query: string;

  /**
   * (Optional) A map of parameter names to literal expressions.
   *
   * @generated from field: map<string, spark.connect.Expression.Literal> args = 2 [deprecated = true];
   * @deprecated
   */
  args: { [key: string]: Expression_Literal };

  /**
   * (Optional) A sequence of literal expressions for positional parameters in the SQL query text.
   *
   * @generated from field: repeated spark.connect.Expression.Literal pos_args = 3 [deprecated = true];
   * @deprecated
   */
  posArgs: Expression_Literal[];

  /**
   * (Optional) A map of parameter names to expressions.
   * It cannot coexist with `pos_arguments`.
   *
   * @generated from field: map<string, spark.connect.Expression> named_arguments = 4;
   */
  namedArguments: { [key: string]: Expression };

  /**
   * (Optional) A sequence of expressions for positional parameters in the SQL query text.
   * It cannot coexist with `named_arguments`.
   *
   * @generated from field: repeated spark.connect.Expression pos_arguments = 5;
   */
  posArguments: Expression[];
};

/**
 * Describes the message spark.connect.SQL.
 * Use `create(SQLSchema)` to create a new message.
 */
export const SQLSchema: GenMessage<SQL> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 3);

/**
 * Relation of type [[WithRelations]].
 *
 * This relation contains a root plan, and one or more references that are used by the root plan.
 * There are two ways of referencing a relation, by name (through a subquery alias), or by plan_id
 * (using RelationCommon.plan_id).
 *
 * This relation can be used to implement CTEs, describe DAGs, or to reduce tree depth.
 *
 * @generated from message spark.connect.WithRelations
 */
export type WithRelations = Message<"spark.connect.WithRelations"> & {
  /**
   * (Required) Plan at the root of the query tree. This plan is expected to contain one or more
   * references. Those references get expanded later on by the engine.
   *
   * @generated from field: spark.connect.Relation root = 1;
   */
  root?: Relation;

  /**
   * (Required) Plans referenced by the root plan. Relations in this list are also allowed to
   * contain references to other relations in this list, as long they do not form cycles.
   *
   * @generated from field: repeated spark.connect.Relation references = 2;
   */
  references: Relation[];
};

/**
 * Describes the message spark.connect.WithRelations.
 * Use `create(WithRelationsSchema)` to create a new message.
 */
export const WithRelationsSchema: GenMessage<WithRelations> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 4);

/**
 * Relation that reads from a file / table or other data source. Does not have additional
 * inputs.
 *
 * @generated from message spark.connect.Read
 */
export type Read = Message<"spark.connect.Read"> & {
  /**
   * @generated from oneof spark.connect.Read.read_type
   */
  readType: {
    /**
     * @generated from field: spark.connect.Read.NamedTable named_table = 1;
     */
    value: Read_NamedTable;
    case: "namedTable";
  } | {
    /**
     * @generated from field: spark.connect.Read.DataSource data_source = 2;
     */
    value: Read_DataSource;
    case: "dataSource";
  } | { case: undefined; value?: undefined };

  /**
   * (Optional) Indicates if this is a streaming read.
   *
   * @generated from field: bool is_streaming = 3;
   */
  isStreaming: boolean;
};

/**
 * Describes the message spark.connect.Read.
 * Use `create(ReadSchema)` to create a new message.
 */
export const ReadSchema: GenMessage<Read> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 5);

/**
 * @generated from message spark.connect.Read.NamedTable
 */
export type Read_NamedTable = Message<"spark.connect.Read.NamedTable"> & {
  /**
   * (Required) Unparsed identifier for the table.
   *
   * @generated from field: string unparsed_identifier = 1;
   */
  unparsedIdentifier: string;

  /**
   * Options for the named table. The map key is case insensitive.
   *
   * @generated from field: map<string, string> options = 2;
   */
  options: { [key: string]: string };
};

/**
 * Describes the message spark.connect.Read.NamedTable.
 * Use `create(Read_NamedTableSchema)` to create a new message.
 */
export const Read_NamedTableSchema: GenMessage<Read_NamedTable> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 5, 0);

/**
 * @generated from message spark.connect.Read.DataSource
 */
export type Read_DataSource = Message<"spark.connect.Read.DataSource"> & {
  /**
   * (Optional) Supported formats include: parquet, orc, text, json, parquet, csv, avro.
   *
   * If not set, the value from SQL conf 'spark.sql.sources.default' will be used.
   *
   * @generated from field: optional string format = 1;
   */
  format?: string;

  /**
   * (Optional) If not set, Spark will infer the schema.
   *
   * This schema string should be either DDL-formatted or JSON-formatted.
   *
   * @generated from field: optional string schema = 2;
   */
  schema?: string;

  /**
   * Options for the data source. The context of this map varies based on the
   * data source format. This options could be empty for valid data source format.
   * The map key is case insensitive.
   *
   * @generated from field: map<string, string> options = 3;
   */
  options: { [key: string]: string };

  /**
   * (Optional) A list of path for file-system backed data sources.
   *
   * @generated from field: repeated string paths = 4;
   */
  paths: string[];

  /**
   * (Optional) Condition in the where clause for each partition.
   *
   * This is only supported by the JDBC data source.
   *
   * @generated from field: repeated string predicates = 5;
   */
  predicates: string[];
};

/**
 * Describes the message spark.connect.Read.DataSource.
 * Use `create(Read_DataSourceSchema)` to create a new message.
 */
export const Read_DataSourceSchema: GenMessage<Read_DataSource> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 5, 1);

/**
 * Projection of a bag of expressions for a given input relation.
 *
 * The input relation must be specified.
 * The projected expression can be an arbitrary expression.
 *
 * @generated from message spark.connect.Project
 */
export type Project = Message<"spark.connect.Project"> & {
  /**
   * (Optional) Input relation is optional for Project.
   *
   * For example, `SELECT ABS(-1)` is valid plan without an input plan.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) A Project requires at least one expression.
   *
   * @generated from field: repeated spark.connect.Expression expressions = 3;
   */
  expressions: Expression[];
};

/**
 * Describes the message spark.connect.Project.
 * Use `create(ProjectSchema)` to create a new message.
 */
export const ProjectSchema: GenMessage<Project> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 6);

/**
 * Relation that applies a boolean expression `condition` on each row of `input` to produce
 * the output result.
 *
 * @generated from message spark.connect.Filter
 */
export type Filter = Message<"spark.connect.Filter"> & {
  /**
   * (Required) Input relation for a Filter.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) A Filter must have a condition expression.
   *
   * @generated from field: spark.connect.Expression condition = 2;
   */
  condition?: Expression;
};

/**
 * Describes the message spark.connect.Filter.
 * Use `create(FilterSchema)` to create a new message.
 */
export const FilterSchema: GenMessage<Filter> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 7);

/**
 * Relation of type [[Join]].
 *
 * `left` and `right` must be present.
 *
 * @generated from message spark.connect.Join
 */
export type Join = Message<"spark.connect.Join"> & {
  /**
   * (Required) Left input relation for a Join.
   *
   * @generated from field: spark.connect.Relation left = 1;
   */
  left?: Relation;

  /**
   * (Required) Right input relation for a Join.
   *
   * @generated from field: spark.connect.Relation right = 2;
   */
  right?: Relation;

  /**
   * (Optional) The join condition. Could be unset when `using_columns` is utilized.
   *
   * This field does not co-exist with using_columns.
   *
   * @generated from field: spark.connect.Expression join_condition = 3;
   */
  joinCondition?: Expression;

  /**
   * (Required) The join type.
   *
   * @generated from field: spark.connect.Join.JoinType join_type = 4;
   */
  joinType: Join_JoinType;

  /**
   * Optional. using_columns provides a list of columns that should present on both sides of
   * the join inputs that this Join will join on. For example A JOIN B USING col_name is
   * equivalent to A JOIN B on A.col_name = B.col_name.
   *
   * This field does not co-exist with join_condition.
   *
   * @generated from field: repeated string using_columns = 5;
   */
  usingColumns: string[];

  /**
   * (Optional) Only used by joinWith. Set the left and right join data types.
   *
   * @generated from field: optional spark.connect.Join.JoinDataType join_data_type = 6;
   */
  joinDataType?: Join_JoinDataType;
};

/**
 * Describes the message spark.connect.Join.
 * Use `create(JoinSchema)` to create a new message.
 */
export const JoinSchema: GenMessage<Join> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 8);

/**
 * @generated from message spark.connect.Join.JoinDataType
 */
export type Join_JoinDataType = Message<"spark.connect.Join.JoinDataType"> & {
  /**
   * If the left data type is a struct.
   *
   * @generated from field: bool is_left_struct = 1;
   */
  isLeftStruct: boolean;

  /**
   * If the right data type is a struct.
   *
   * @generated from field: bool is_right_struct = 2;
   */
  isRightStruct: boolean;
};

/**
 * Describes the message spark.connect.Join.JoinDataType.
 * Use `create(Join_JoinDataTypeSchema)` to create a new message.
 */
export const Join_JoinDataTypeSchema: GenMessage<Join_JoinDataType> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 8, 0);

/**
 * @generated from enum spark.connect.Join.JoinType
 */
export enum Join_JoinType {
  /**
   * @generated from enum value: JOIN_TYPE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * @generated from enum value: JOIN_TYPE_INNER = 1;
   */
  INNER = 1,

  /**
   * @generated from enum value: JOIN_TYPE_FULL_OUTER = 2;
   */
  FULL_OUTER = 2,

  /**
   * @generated from enum value: JOIN_TYPE_LEFT_OUTER = 3;
   */
  LEFT_OUTER = 3,

  /**
   * @generated from enum value: JOIN_TYPE_RIGHT_OUTER = 4;
   */
  RIGHT_OUTER = 4,

  /**
   * @generated from enum value: JOIN_TYPE_LEFT_ANTI = 5;
   */
  LEFT_ANTI = 5,

  /**
   * @generated from enum value: JOIN_TYPE_LEFT_SEMI = 6;
   */
  LEFT_SEMI = 6,

  /**
   * @generated from enum value: JOIN_TYPE_CROSS = 7;
   */
  CROSS = 7,
}

/**
 * Describes the enum spark.connect.Join.JoinType.
 */
export const Join_JoinTypeSchema: GenEnum<Join_JoinType> = /*@__PURE__*/
  enumDesc(file_spark_connect_relations, 8, 0);

/**
 * Relation of type [[SetOperation]]
 *
 * @generated from message spark.connect.SetOperation
 */
export type SetOperation = Message<"spark.connect.SetOperation"> & {
  /**
   * (Required) Left input relation for a Set operation.
   *
   * @generated from field: spark.connect.Relation left_input = 1;
   */
  leftInput?: Relation;

  /**
   * (Required) Right input relation for a Set operation.
   *
   * @generated from field: spark.connect.Relation right_input = 2;
   */
  rightInput?: Relation;

  /**
   * (Required) The Set operation type.
   *
   * @generated from field: spark.connect.SetOperation.SetOpType set_op_type = 3;
   */
  setOpType: SetOperation_SetOpType;

  /**
   * (Optional) If to remove duplicate rows.
   *
   * True to preserve all results.
   * False to remove duplicate rows.
   *
   * @generated from field: optional bool is_all = 4;
   */
  isAll?: boolean;

  /**
   * (Optional) If to perform the Set operation based on name resolution.
   *
   * Only UNION supports this option.
   *
   * @generated from field: optional bool by_name = 5;
   */
  byName?: boolean;

  /**
   * (Optional) If to perform the Set operation and allow missing columns.
   *
   * Only UNION supports this option.
   *
   * @generated from field: optional bool allow_missing_columns = 6;
   */
  allowMissingColumns?: boolean;
};

/**
 * Describes the message spark.connect.SetOperation.
 * Use `create(SetOperationSchema)` to create a new message.
 */
export const SetOperationSchema: GenMessage<SetOperation> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 9);

/**
 * @generated from enum spark.connect.SetOperation.SetOpType
 */
export enum SetOperation_SetOpType {
  /**
   * @generated from enum value: SET_OP_TYPE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * @generated from enum value: SET_OP_TYPE_INTERSECT = 1;
   */
  INTERSECT = 1,

  /**
   * @generated from enum value: SET_OP_TYPE_UNION = 2;
   */
  UNION = 2,

  /**
   * @generated from enum value: SET_OP_TYPE_EXCEPT = 3;
   */
  EXCEPT = 3,
}

/**
 * Describes the enum spark.connect.SetOperation.SetOpType.
 */
export const SetOperation_SetOpTypeSchema: GenEnum<SetOperation_SetOpType> = /*@__PURE__*/
  enumDesc(file_spark_connect_relations, 9, 0);

/**
 * Relation of type [[Limit]] that is used to `limit` rows from the input relation.
 *
 * @generated from message spark.connect.Limit
 */
export type Limit = Message<"spark.connect.Limit"> & {
  /**
   * (Required) Input relation for a Limit.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) the limit.
   *
   * @generated from field: int32 limit = 2;
   */
  limit: number;
};

/**
 * Describes the message spark.connect.Limit.
 * Use `create(LimitSchema)` to create a new message.
 */
export const LimitSchema: GenMessage<Limit> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 10);

/**
 * Relation of type [[Offset]] that is used to read rows staring from the `offset` on
 * the input relation.
 *
 * @generated from message spark.connect.Offset
 */
export type Offset = Message<"spark.connect.Offset"> & {
  /**
   * (Required) Input relation for an Offset.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) the limit.
   *
   * @generated from field: int32 offset = 2;
   */
  offset: number;
};

/**
 * Describes the message spark.connect.Offset.
 * Use `create(OffsetSchema)` to create a new message.
 */
export const OffsetSchema: GenMessage<Offset> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 11);

/**
 * Relation of type [[Tail]] that is used to fetch `limit` rows from the last of the input relation.
 *
 * @generated from message spark.connect.Tail
 */
export type Tail = Message<"spark.connect.Tail"> & {
  /**
   * (Required) Input relation for an Tail.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) the limit.
   *
   * @generated from field: int32 limit = 2;
   */
  limit: number;
};

/**
 * Describes the message spark.connect.Tail.
 * Use `create(TailSchema)` to create a new message.
 */
export const TailSchema: GenMessage<Tail> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 12);

/**
 * Relation of type [[Aggregate]].
 *
 * @generated from message spark.connect.Aggregate
 */
export type Aggregate = Message<"spark.connect.Aggregate"> & {
  /**
   * (Required) Input relation for a RelationalGroupedDataset.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) How the RelationalGroupedDataset was built.
   *
   * @generated from field: spark.connect.Aggregate.GroupType group_type = 2;
   */
  groupType: Aggregate_GroupType;

  /**
   * (Required) Expressions for grouping keys
   *
   * @generated from field: repeated spark.connect.Expression grouping_expressions = 3;
   */
  groupingExpressions: Expression[];

  /**
   * (Required) List of values that will be translated to columns in the output DataFrame.
   *
   * @generated from field: repeated spark.connect.Expression aggregate_expressions = 4;
   */
  aggregateExpressions: Expression[];

  /**
   * (Optional) Pivots a column of the current `DataFrame` and performs the specified aggregation.
   *
   * @generated from field: spark.connect.Aggregate.Pivot pivot = 5;
   */
  pivot?: Aggregate_Pivot;

  /**
   * (Optional) List of values that will be translated to columns in the output DataFrame.
   *
   * @generated from field: repeated spark.connect.Aggregate.GroupingSets grouping_sets = 6;
   */
  groupingSets: Aggregate_GroupingSets[];
};

/**
 * Describes the message spark.connect.Aggregate.
 * Use `create(AggregateSchema)` to create a new message.
 */
export const AggregateSchema: GenMessage<Aggregate> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 13);

/**
 * @generated from message spark.connect.Aggregate.Pivot
 */
export type Aggregate_Pivot = Message<"spark.connect.Aggregate.Pivot"> & {
  /**
   * (Required) The column to pivot
   *
   * @generated from field: spark.connect.Expression col = 1;
   */
  col?: Expression;

  /**
   * (Optional) List of values that will be translated to columns in the output DataFrame.
   *
   * Note that if it is empty, the server side will immediately trigger a job to collect
   * the distinct values of the column.
   *
   * @generated from field: repeated spark.connect.Expression.Literal values = 2;
   */
  values: Expression_Literal[];
};

/**
 * Describes the message spark.connect.Aggregate.Pivot.
 * Use `create(Aggregate_PivotSchema)` to create a new message.
 */
export const Aggregate_PivotSchema: GenMessage<Aggregate_Pivot> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 13, 0);

/**
 * @generated from message spark.connect.Aggregate.GroupingSets
 */
export type Aggregate_GroupingSets = Message<"spark.connect.Aggregate.GroupingSets"> & {
  /**
   * (Required) Individual grouping set
   *
   * @generated from field: repeated spark.connect.Expression grouping_set = 1;
   */
  groupingSet: Expression[];
};

/**
 * Describes the message spark.connect.Aggregate.GroupingSets.
 * Use `create(Aggregate_GroupingSetsSchema)` to create a new message.
 */
export const Aggregate_GroupingSetsSchema: GenMessage<Aggregate_GroupingSets> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 13, 1);

/**
 * @generated from enum spark.connect.Aggregate.GroupType
 */
export enum Aggregate_GroupType {
  /**
   * @generated from enum value: GROUP_TYPE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * @generated from enum value: GROUP_TYPE_GROUPBY = 1;
   */
  GROUPBY = 1,

  /**
   * @generated from enum value: GROUP_TYPE_ROLLUP = 2;
   */
  ROLLUP = 2,

  /**
   * @generated from enum value: GROUP_TYPE_CUBE = 3;
   */
  CUBE = 3,

  /**
   * @generated from enum value: GROUP_TYPE_PIVOT = 4;
   */
  PIVOT = 4,

  /**
   * @generated from enum value: GROUP_TYPE_GROUPING_SETS = 5;
   */
  GROUPING_SETS = 5,
}

/**
 * Describes the enum spark.connect.Aggregate.GroupType.
 */
export const Aggregate_GroupTypeSchema: GenEnum<Aggregate_GroupType> = /*@__PURE__*/
  enumDesc(file_spark_connect_relations, 13, 0);

/**
 * Relation of type [[Sort]].
 *
 * @generated from message spark.connect.Sort
 */
export type Sort = Message<"spark.connect.Sort"> & {
  /**
   * (Required) Input relation for a Sort.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The ordering expressions
   *
   * @generated from field: repeated spark.connect.Expression.SortOrder order = 2;
   */
  order: Expression_SortOrder[];

  /**
   * (Optional) if this is a global sort.
   *
   * @generated from field: optional bool is_global = 3;
   */
  isGlobal?: boolean;
};

/**
 * Describes the message spark.connect.Sort.
 * Use `create(SortSchema)` to create a new message.
 */
export const SortSchema: GenMessage<Sort> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 14);

/**
 * Drop specified columns.
 *
 * @generated from message spark.connect.Drop
 */
export type Drop = Message<"spark.connect.Drop"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional) columns to drop.
   *
   * @generated from field: repeated spark.connect.Expression columns = 2;
   */
  columns: Expression[];

  /**
   * (Optional) names of columns to drop.
   *
   * @generated from field: repeated string column_names = 3;
   */
  columnNames: string[];
};

/**
 * Describes the message spark.connect.Drop.
 * Use `create(DropSchema)` to create a new message.
 */
export const DropSchema: GenMessage<Drop> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 15);

/**
 * Relation of type [[Deduplicate]] which have duplicate rows removed, could consider either only
 * the subset of columns or all the columns.
 *
 * @generated from message spark.connect.Deduplicate
 */
export type Deduplicate = Message<"spark.connect.Deduplicate"> & {
  /**
   * (Required) Input relation for a Deduplicate.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional) Deduplicate based on a list of column names.
   *
   * This field does not co-use with `all_columns_as_keys`.
   *
   * @generated from field: repeated string column_names = 2;
   */
  columnNames: string[];

  /**
   * (Optional) Deduplicate based on all the columns of the input relation.
   *
   * This field does not co-use with `column_names`.
   *
   * @generated from field: optional bool all_columns_as_keys = 3;
   */
  allColumnsAsKeys?: boolean;

  /**
   * (Optional) Deduplicate within the time range of watermark.
   *
   * @generated from field: optional bool within_watermark = 4;
   */
  withinWatermark?: boolean;
};

/**
 * Describes the message spark.connect.Deduplicate.
 * Use `create(DeduplicateSchema)` to create a new message.
 */
export const DeduplicateSchema: GenMessage<Deduplicate> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 16);

/**
 * A relation that does not need to be qualified by name.
 *
 * @generated from message spark.connect.LocalRelation
 */
export type LocalRelation = Message<"spark.connect.LocalRelation"> & {
  /**
   * (Optional) Local collection data serialized into Arrow IPC streaming format which contains
   * the schema of the data.
   *
   * @generated from field: optional bytes data = 1;
   */
  data?: Uint8Array;

  /**
   * (Optional) The schema of local data.
   * It should be either a DDL-formatted type string or a JSON string.
   *
   * The server side will update the column names and data types according to this schema.
   * If the 'data' is not provided, then this schema will be required.
   *
   * @generated from field: optional string schema = 2;
   */
  schema?: string;
};

/**
 * Describes the message spark.connect.LocalRelation.
 * Use `create(LocalRelationSchema)` to create a new message.
 */
export const LocalRelationSchema: GenMessage<LocalRelation> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 17);

/**
 * A local relation that has been cached already.
 *
 * @generated from message spark.connect.CachedLocalRelation
 */
export type CachedLocalRelation = Message<"spark.connect.CachedLocalRelation"> & {
  /**
   * (Required) A sha-256 hash of the serialized local relation in proto, see LocalRelation.
   *
   * @generated from field: string hash = 3;
   */
  hash: string;
};

/**
 * Describes the message spark.connect.CachedLocalRelation.
 * Use `create(CachedLocalRelationSchema)` to create a new message.
 */
export const CachedLocalRelationSchema: GenMessage<CachedLocalRelation> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 18);

/**
 * Represents a remote relation that has been cached on server.
 *
 * @generated from message spark.connect.CachedRemoteRelation
 */
export type CachedRemoteRelation = Message<"spark.connect.CachedRemoteRelation"> & {
  /**
   * (Required) ID of the remote related (assigned by the service).
   *
   * @generated from field: string relation_id = 1;
   */
  relationId: string;
};

/**
 * Describes the message spark.connect.CachedRemoteRelation.
 * Use `create(CachedRemoteRelationSchema)` to create a new message.
 */
export const CachedRemoteRelationSchema: GenMessage<CachedRemoteRelation> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 19);

/**
 * Relation of type [[Sample]] that samples a fraction of the dataset.
 *
 * @generated from message spark.connect.Sample
 */
export type Sample = Message<"spark.connect.Sample"> & {
  /**
   * (Required) Input relation for a Sample.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) lower bound.
   *
   * @generated from field: double lower_bound = 2;
   */
  lowerBound: number;

  /**
   * (Required) upper bound.
   *
   * @generated from field: double upper_bound = 3;
   */
  upperBound: number;

  /**
   * (Optional) Whether to sample with replacement.
   *
   * @generated from field: optional bool with_replacement = 4;
   */
  withReplacement?: boolean;

  /**
   * (Required) The random seed.
   * This field is required to avoid generating mutable dataframes (see SPARK-48184 for details),
   * however, still keep it 'optional' here for backward compatibility.
   *
   * @generated from field: optional int64 seed = 5;
   */
  seed?: bigint;

  /**
   * (Required) Explicitly sort the underlying plan to make the ordering deterministic or cache it.
   * This flag is true when invoking `dataframe.randomSplit` to randomly splits DataFrame with the
   * provided weights. Otherwise, it is false.
   *
   * @generated from field: bool deterministic_order = 6;
   */
  deterministicOrder: boolean;
};

/**
 * Describes the message spark.connect.Sample.
 * Use `create(SampleSchema)` to create a new message.
 */
export const SampleSchema: GenMessage<Sample> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 20);

/**
 * Relation of type [[Range]] that generates a sequence of integers.
 *
 * @generated from message spark.connect.Range
 */
export type Range = Message<"spark.connect.Range"> & {
  /**
   * (Optional) Default value = 0
   *
   * @generated from field: optional int64 start = 1;
   */
  start?: bigint;

  /**
   * (Required)
   *
   * @generated from field: int64 end = 2;
   */
  end: bigint;

  /**
   * (Required)
   *
   * @generated from field: int64 step = 3;
   */
  step: bigint;

  /**
   * Optional. Default value is assigned by 1) SQL conf "spark.sql.leafNodeDefaultParallelism" if
   * it is set, or 2) spark default parallelism.
   *
   * @generated from field: optional int32 num_partitions = 4;
   */
  numPartitions?: number;
};

/**
 * Describes the message spark.connect.Range.
 * Use `create(RangeSchema)` to create a new message.
 */
export const RangeSchema: GenMessage<Range> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 21);

/**
 * Relation alias.
 *
 * @generated from message spark.connect.SubqueryAlias
 */
export type SubqueryAlias = Message<"spark.connect.SubqueryAlias"> & {
  /**
   * (Required) The input relation of SubqueryAlias.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The alias.
   *
   * @generated from field: string alias = 2;
   */
  alias: string;

  /**
   * (Optional) Qualifier of the alias.
   *
   * @generated from field: repeated string qualifier = 3;
   */
  qualifier: string[];
};

/**
 * Describes the message spark.connect.SubqueryAlias.
 * Use `create(SubqueryAliasSchema)` to create a new message.
 */
export const SubqueryAliasSchema: GenMessage<SubqueryAlias> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 22);

/**
 * Relation repartition.
 *
 * @generated from message spark.connect.Repartition
 */
export type Repartition = Message<"spark.connect.Repartition"> & {
  /**
   * (Required) The input relation of Repartition.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Must be positive.
   *
   * @generated from field: int32 num_partitions = 2;
   */
  numPartitions: number;

  /**
   * (Optional) Default value is false.
   *
   * @generated from field: optional bool shuffle = 3;
   */
  shuffle?: boolean;
};

/**
 * Describes the message spark.connect.Repartition.
 * Use `create(RepartitionSchema)` to create a new message.
 */
export const RepartitionSchema: GenMessage<Repartition> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 23);

/**
 * Compose the string representing rows for output.
 * It will invoke 'Dataset.showString' to compute the results.
 *
 * @generated from message spark.connect.ShowString
 */
export type ShowString = Message<"spark.connect.ShowString"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Number of rows to show.
   *
   * @generated from field: int32 num_rows = 2;
   */
  numRows: number;

  /**
   * (Required) If set to more than 0, truncates strings to
   * `truncate` characters and all cells will be aligned right.
   *
   * @generated from field: int32 truncate = 3;
   */
  truncate: number;

  /**
   * (Required) If set to true, prints output rows vertically (one line per column value).
   *
   * @generated from field: bool vertical = 4;
   */
  vertical: boolean;
};

/**
 * Describes the message spark.connect.ShowString.
 * Use `create(ShowStringSchema)` to create a new message.
 */
export const ShowStringSchema: GenMessage<ShowString> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 24);

/**
 * Compose the string representing rows for output.
 * It will invoke 'Dataset.htmlString' to compute the results.
 *
 * @generated from message spark.connect.HtmlString
 */
export type HtmlString = Message<"spark.connect.HtmlString"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Number of rows to show.
   *
   * @generated from field: int32 num_rows = 2;
   */
  numRows: number;

  /**
   * (Required) If set to more than 0, truncates strings to
   * `truncate` characters and all cells will be aligned right.
   *
   * @generated from field: int32 truncate = 3;
   */
  truncate: number;
};

/**
 * Describes the message spark.connect.HtmlString.
 * Use `create(HtmlStringSchema)` to create a new message.
 */
export const HtmlStringSchema: GenMessage<HtmlString> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 25);

/**
 * Computes specified statistics for numeric and string columns.
 * It will invoke 'Dataset.summary' (same as 'StatFunctions.summary')
 * to compute the results.
 *
 * @generated from message spark.connect.StatSummary
 */
export type StatSummary = Message<"spark.connect.StatSummary"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional) Statistics from to be computed.
   *
   * Available statistics are:
   *  count
   *  mean
   *  stddev
   *  min
   *  max
   *  arbitrary approximate percentiles specified as a percentage (e.g. 75%)
   *  count_distinct
   *  approx_count_distinct
   *
   * If no statistics are given, this function computes 'count', 'mean', 'stddev', 'min',
   * 'approximate quartiles' (percentiles at 25%, 50%, and 75%), and 'max'.
   *
   * @generated from field: repeated string statistics = 2;
   */
  statistics: string[];
};

/**
 * Describes the message spark.connect.StatSummary.
 * Use `create(StatSummarySchema)` to create a new message.
 */
export const StatSummarySchema: GenMessage<StatSummary> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 26);

/**
 * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
 * and max. If no columns are given, this function computes statistics for all numerical or
 * string columns.
 *
 * @generated from message spark.connect.StatDescribe
 */
export type StatDescribe = Message<"spark.connect.StatDescribe"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional) Columns to compute statistics on.
   *
   * @generated from field: repeated string cols = 2;
   */
  cols: string[];
};

/**
 * Describes the message spark.connect.StatDescribe.
 * Use `create(StatDescribeSchema)` to create a new message.
 */
export const StatDescribeSchema: GenMessage<StatDescribe> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 27);

/**
 * Computes a pair-wise frequency table of the given columns. Also known as a contingency table.
 * It will invoke 'Dataset.stat.crosstab' (same as 'StatFunctions.crossTabulate')
 * to compute the results.
 *
 * @generated from message spark.connect.StatCrosstab
 */
export type StatCrosstab = Message<"spark.connect.StatCrosstab"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The name of the first column.
   *
   * Distinct items will make the first item of each row.
   *
   * @generated from field: string col1 = 2;
   */
  col1: string;

  /**
   * (Required) The name of the second column.
   *
   * Distinct items will make the column names of the DataFrame.
   *
   * @generated from field: string col2 = 3;
   */
  col2: string;
};

/**
 * Describes the message spark.connect.StatCrosstab.
 * Use `create(StatCrosstabSchema)` to create a new message.
 */
export const StatCrosstabSchema: GenMessage<StatCrosstab> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 28);

/**
 * Calculate the sample covariance of two numerical columns of a DataFrame.
 * It will invoke 'Dataset.stat.cov' (same as 'StatFunctions.calculateCov') to compute the results.
 *
 * @generated from message spark.connect.StatCov
 */
export type StatCov = Message<"spark.connect.StatCov"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The name of the first column.
   *
   * @generated from field: string col1 = 2;
   */
  col1: string;

  /**
   * (Required) The name of the second column.
   *
   * @generated from field: string col2 = 3;
   */
  col2: string;
};

/**
 * Describes the message spark.connect.StatCov.
 * Use `create(StatCovSchema)` to create a new message.
 */
export const StatCovSchema: GenMessage<StatCov> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 29);

/**
 * Calculates the correlation of two columns of a DataFrame. Currently only supports the Pearson
 * Correlation Coefficient. It will invoke 'Dataset.stat.corr' (same as
 * 'StatFunctions.pearsonCorrelation') to compute the results.
 *
 * @generated from message spark.connect.StatCorr
 */
export type StatCorr = Message<"spark.connect.StatCorr"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The name of the first column.
   *
   * @generated from field: string col1 = 2;
   */
  col1: string;

  /**
   * (Required) The name of the second column.
   *
   * @generated from field: string col2 = 3;
   */
  col2: string;

  /**
   * (Optional) Default value is 'pearson'.
   *
   * Currently only supports the Pearson Correlation Coefficient.
   *
   * @generated from field: optional string method = 4;
   */
  method?: string;
};

/**
 * Describes the message spark.connect.StatCorr.
 * Use `create(StatCorrSchema)` to create a new message.
 */
export const StatCorrSchema: GenMessage<StatCorr> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 30);

/**
 * Calculates the approximate quantiles of numerical columns of a DataFrame.
 * It will invoke 'Dataset.stat.approxQuantile' (same as 'StatFunctions.approxQuantile')
 * to compute the results.
 *
 * @generated from message spark.connect.StatApproxQuantile
 */
export type StatApproxQuantile = Message<"spark.connect.StatApproxQuantile"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The names of the numerical columns.
   *
   * @generated from field: repeated string cols = 2;
   */
  cols: string[];

  /**
   * (Required) A list of quantile probabilities.
   *
   * Each number must belong to [0, 1].
   * For example 0 is the minimum, 0.5 is the median, 1 is the maximum.
   *
   * @generated from field: repeated double probabilities = 3;
   */
  probabilities: number[];

  /**
   * (Required) The relative target precision to achieve (greater than or equal to 0).
   *
   * If set to zero, the exact quantiles are computed, which could be very expensive.
   * Note that values greater than 1 are accepted but give the same result as 1.
   *
   * @generated from field: double relative_error = 4;
   */
  relativeError: number;
};

/**
 * Describes the message spark.connect.StatApproxQuantile.
 * Use `create(StatApproxQuantileSchema)` to create a new message.
 */
export const StatApproxQuantileSchema: GenMessage<StatApproxQuantile> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 31);

/**
 * Finding frequent items for columns, possibly with false positives.
 * It will invoke 'Dataset.stat.freqItems' (same as 'StatFunctions.freqItems')
 * to compute the results.
 *
 * @generated from message spark.connect.StatFreqItems
 */
export type StatFreqItems = Message<"spark.connect.StatFreqItems"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The names of the columns to search frequent items in.
   *
   * @generated from field: repeated string cols = 2;
   */
  cols: string[];

  /**
   * (Optional) The minimum frequency for an item to be considered `frequent`.
   * Should be greater than 1e-4.
   *
   * @generated from field: optional double support = 3;
   */
  support?: number;
};

/**
 * Describes the message spark.connect.StatFreqItems.
 * Use `create(StatFreqItemsSchema)` to create a new message.
 */
export const StatFreqItemsSchema: GenMessage<StatFreqItems> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 32);

/**
 * Returns a stratified sample without replacement based on the fraction
 * given on each stratum.
 * It will invoke 'Dataset.stat.freqItems' (same as 'StatFunctions.freqItems')
 * to compute the results.
 *
 * @generated from message spark.connect.StatSampleBy
 */
export type StatSampleBy = Message<"spark.connect.StatSampleBy"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The column that defines strata.
   *
   * @generated from field: spark.connect.Expression col = 2;
   */
  col?: Expression;

  /**
   * (Required) Sampling fraction for each stratum.
   *
   * If a stratum is not specified, we treat its fraction as zero.
   *
   * @generated from field: repeated spark.connect.StatSampleBy.Fraction fractions = 3;
   */
  fractions: StatSampleBy_Fraction[];

  /**
   * (Required) The random seed.
   * This field is required to avoid generating mutable dataframes (see SPARK-48184 for details),
   * however, still keep it 'optional' here for backward compatibility.
   *
   * @generated from field: optional int64 seed = 5;
   */
  seed?: bigint;
};

/**
 * Describes the message spark.connect.StatSampleBy.
 * Use `create(StatSampleBySchema)` to create a new message.
 */
export const StatSampleBySchema: GenMessage<StatSampleBy> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 33);

/**
 * @generated from message spark.connect.StatSampleBy.Fraction
 */
export type StatSampleBy_Fraction = Message<"spark.connect.StatSampleBy.Fraction"> & {
  /**
   * (Required) The stratum.
   *
   * @generated from field: spark.connect.Expression.Literal stratum = 1;
   */
  stratum?: Expression_Literal;

  /**
   * (Required) The fraction value. Must be in [0, 1].
   *
   * @generated from field: double fraction = 2;
   */
  fraction: number;
};

/**
 * Describes the message spark.connect.StatSampleBy.Fraction.
 * Use `create(StatSampleBy_FractionSchema)` to create a new message.
 */
export const StatSampleBy_FractionSchema: GenMessage<StatSampleBy_Fraction> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 33, 0);

/**
 * Replaces null values.
 * It will invoke 'Dataset.na.fill' (same as 'DataFrameNaFunctions.fill') to compute the results.
 * Following 3 parameter combinations are supported:
 *  1, 'values' only contains 1 item, 'cols' is empty:
 *    replaces null values in all type-compatible columns.
 *  2, 'values' only contains 1 item, 'cols' is not empty:
 *    replaces null values in specified columns.
 *  3, 'values' contains more than 1 items, then 'cols' is required to have the same length:
 *    replaces each specified column with corresponding value.
 *
 * @generated from message spark.connect.NAFill
 */
export type NAFill = Message<"spark.connect.NAFill"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional) Optional list of column names to consider.
   *
   * @generated from field: repeated string cols = 2;
   */
  cols: string[];

  /**
   * (Required) Values to replace null values with.
   *
   * Should contain at least 1 item.
   * Only 4 data types are supported now: bool, long, double, string
   *
   * @generated from field: repeated spark.connect.Expression.Literal values = 3;
   */
  values: Expression_Literal[];
};

/**
 * Describes the message spark.connect.NAFill.
 * Use `create(NAFillSchema)` to create a new message.
 */
export const NAFillSchema: GenMessage<NAFill> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 34);

/**
 * Drop rows containing null values.
 * It will invoke 'Dataset.na.drop' (same as 'DataFrameNaFunctions.drop') to compute the results.
 *
 * @generated from message spark.connect.NADrop
 */
export type NADrop = Message<"spark.connect.NADrop"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional) Optional list of column names to consider.
   *
   * When it is empty, all the columns in the input relation will be considered.
   *
   * @generated from field: repeated string cols = 2;
   */
  cols: string[];

  /**
   * (Optional) The minimum number of non-null and non-NaN values required to keep.
   *
   * When not set, it is equivalent to the number of considered columns, which means
   * a row will be kept only if all columns are non-null.
   *
   * 'how' options ('all', 'any') can be easily converted to this field:
   *   - 'all' -> set 'min_non_nulls' 1;
   *   - 'any' -> keep 'min_non_nulls' unset;
   *
   * @generated from field: optional int32 min_non_nulls = 3;
   */
  minNonNulls?: number;
};

/**
 * Describes the message spark.connect.NADrop.
 * Use `create(NADropSchema)` to create a new message.
 */
export const NADropSchema: GenMessage<NADrop> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 35);

/**
 * Replaces old values with the corresponding values.
 * It will invoke 'Dataset.na.replace' (same as 'DataFrameNaFunctions.replace')
 * to compute the results.
 *
 * @generated from message spark.connect.NAReplace
 */
export type NAReplace = Message<"spark.connect.NAReplace"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional) List of column names to consider.
   *
   * When it is empty, all the type-compatible columns in the input relation will be considered.
   *
   * @generated from field: repeated string cols = 2;
   */
  cols: string[];

  /**
   * (Optional) The value replacement mapping.
   *
   * @generated from field: repeated spark.connect.NAReplace.Replacement replacements = 3;
   */
  replacements: NAReplace_Replacement[];
};

/**
 * Describes the message spark.connect.NAReplace.
 * Use `create(NAReplaceSchema)` to create a new message.
 */
export const NAReplaceSchema: GenMessage<NAReplace> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 36);

/**
 * @generated from message spark.connect.NAReplace.Replacement
 */
export type NAReplace_Replacement = Message<"spark.connect.NAReplace.Replacement"> & {
  /**
   * (Required) The old value.
   *
   * Only 4 data types are supported now: null, bool, double, string.
   *
   * @generated from field: spark.connect.Expression.Literal old_value = 1;
   */
  oldValue?: Expression_Literal;

  /**
   * (Required) The new value.
   *
   * Should be of the same data type with the old value.
   *
   * @generated from field: spark.connect.Expression.Literal new_value = 2;
   */
  newValue?: Expression_Literal;
};

/**
 * Describes the message spark.connect.NAReplace.Replacement.
 * Use `create(NAReplace_ReplacementSchema)` to create a new message.
 */
export const NAReplace_ReplacementSchema: GenMessage<NAReplace_Replacement> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 36, 0);

/**
 * Rename columns on the input relation by the same length of names.
 *
 * @generated from message spark.connect.ToDF
 */
export type ToDF = Message<"spark.connect.ToDF"> & {
  /**
   * (Required) The input relation of RenameColumnsBySameLengthNames.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required)
   *
   * The number of columns of the input relation must be equal to the length
   * of this field. If this is not true, an exception will be returned.
   *
   * @generated from field: repeated string column_names = 2;
   */
  columnNames: string[];
};

/**
 * Describes the message spark.connect.ToDF.
 * Use `create(ToDFSchema)` to create a new message.
 */
export const ToDFSchema: GenMessage<ToDF> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 37);

/**
 * Rename columns on the input relation by a map with name to name mapping.
 *
 * @generated from message spark.connect.WithColumnsRenamed
 */
export type WithColumnsRenamed = Message<"spark.connect.WithColumnsRenamed"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional)
   *
   * Renaming column names of input relation from A to B where A is the map key
   * and B is the map value. This is a no-op if schema doesn't contain any A. It
   * does not require that all input relation column names to present as keys.
   * duplicated B are not allowed.
   *
   * @generated from field: map<string, string> rename_columns_map = 2 [deprecated = true];
   * @deprecated
   */
  renameColumnsMap: { [key: string]: string };

  /**
   * @generated from field: repeated spark.connect.WithColumnsRenamed.Rename renames = 3;
   */
  renames: WithColumnsRenamed_Rename[];
};

/**
 * Describes the message spark.connect.WithColumnsRenamed.
 * Use `create(WithColumnsRenamedSchema)` to create a new message.
 */
export const WithColumnsRenamedSchema: GenMessage<WithColumnsRenamed> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 38);

/**
 * @generated from message spark.connect.WithColumnsRenamed.Rename
 */
export type WithColumnsRenamed_Rename = Message<"spark.connect.WithColumnsRenamed.Rename"> & {
  /**
   * (Required) The existing column name.
   *
   * @generated from field: string col_name = 1;
   */
  colName: string;

  /**
   * (Required) The new column name.
   *
   * @generated from field: string new_col_name = 2;
   */
  newColName: string;
};

/**
 * Describes the message spark.connect.WithColumnsRenamed.Rename.
 * Use `create(WithColumnsRenamed_RenameSchema)` to create a new message.
 */
export const WithColumnsRenamed_RenameSchema: GenMessage<WithColumnsRenamed_Rename> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 38, 0);

/**
 * Adding columns or replacing the existing columns that have the same names.
 *
 * @generated from message spark.connect.WithColumns
 */
export type WithColumns = Message<"spark.connect.WithColumns"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required)
   *
   * Given a column name, apply the corresponding expression on the column. If column
   * name exists in the input relation, then replace the column. If the column name
   * does not exist in the input relation, then adds it as a new column.
   *
   * Only one name part is expected from each Expression.Alias.
   *
   * An exception is thrown when duplicated names are present in the mapping.
   *
   * @generated from field: repeated spark.connect.Expression.Alias aliases = 2;
   */
  aliases: Expression_Alias[];
};

/**
 * Describes the message spark.connect.WithColumns.
 * Use `create(WithColumnsSchema)` to create a new message.
 */
export const WithColumnsSchema: GenMessage<WithColumns> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 39);

/**
 * @generated from message spark.connect.WithWatermark
 */
export type WithWatermark = Message<"spark.connect.WithWatermark"> & {
  /**
   * (Required) The input relation
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Name of the column containing event time.
   *
   * @generated from field: string event_time = 2;
   */
  eventTime: string;

  /**
   * (Required)
   *
   * @generated from field: string delay_threshold = 3;
   */
  delayThreshold: string;
};

/**
 * Describes the message spark.connect.WithWatermark.
 * Use `create(WithWatermarkSchema)` to create a new message.
 */
export const WithWatermarkSchema: GenMessage<WithWatermark> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 40);

/**
 * Specify a hint over a relation. Hint should have a name and optional parameters.
 *
 * @generated from message spark.connect.Hint
 */
export type Hint = Message<"spark.connect.Hint"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Hint name.
   *
   * Supported Join hints include BROADCAST, MERGE, SHUFFLE_HASH, SHUFFLE_REPLICATE_NL.
   *
   * Supported partitioning hints include COALESCE, REPARTITION, REPARTITION_BY_RANGE.
   *
   * @generated from field: string name = 2;
   */
  name: string;

  /**
   * (Optional) Hint parameters.
   *
   * @generated from field: repeated spark.connect.Expression parameters = 3;
   */
  parameters: Expression[];
};

/**
 * Describes the message spark.connect.Hint.
 * Use `create(HintSchema)` to create a new message.
 */
export const HintSchema: GenMessage<Hint> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 41);

/**
 * Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns set.
 *
 * @generated from message spark.connect.Unpivot
 */
export type Unpivot = Message<"spark.connect.Unpivot"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Id columns.
   *
   * @generated from field: repeated spark.connect.Expression ids = 2;
   */
  ids: Expression[];

  /**
   * (Optional) Value columns to unpivot.
   *
   * @generated from field: optional spark.connect.Unpivot.Values values = 3;
   */
  values?: Unpivot_Values;

  /**
   * (Required) Name of the variable column.
   *
   * @generated from field: string variable_column_name = 4;
   */
  variableColumnName: string;

  /**
   * (Required) Name of the value column.
   *
   * @generated from field: string value_column_name = 5;
   */
  valueColumnName: string;
};

/**
 * Describes the message spark.connect.Unpivot.
 * Use `create(UnpivotSchema)` to create a new message.
 */
export const UnpivotSchema: GenMessage<Unpivot> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 42);

/**
 * @generated from message spark.connect.Unpivot.Values
 */
export type Unpivot_Values = Message<"spark.connect.Unpivot.Values"> & {
  /**
   * @generated from field: repeated spark.connect.Expression values = 1;
   */
  values: Expression[];
};

/**
 * Describes the message spark.connect.Unpivot.Values.
 * Use `create(Unpivot_ValuesSchema)` to create a new message.
 */
export const Unpivot_ValuesSchema: GenMessage<Unpivot_Values> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 42, 0);

/**
 * Transpose a DataFrame, switching rows to columns.
 * Transforms the DataFrame such that the values in the specified index column
 * become the new columns of the DataFrame.
 *
 * @generated from message spark.connect.Transpose
 */
export type Transpose = Message<"spark.connect.Transpose"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional) A list of columns that will be treated as the indices.
   * Only single column is supported now.
   *
   * @generated from field: repeated spark.connect.Expression index_columns = 2;
   */
  indexColumns: Expression[];
};

/**
 * Describes the message spark.connect.Transpose.
 * Use `create(TransposeSchema)` to create a new message.
 */
export const TransposeSchema: GenMessage<Transpose> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 43);

/**
 * @generated from message spark.connect.UnresolvedTableValuedFunction
 */
export type UnresolvedTableValuedFunction = Message<"spark.connect.UnresolvedTableValuedFunction"> & {
  /**
   * (Required) name (or unparsed name for user defined function) for the unresolved function.
   *
   * @generated from field: string function_name = 1;
   */
  functionName: string;

  /**
   * (Optional) Function arguments. Empty arguments are allowed.
   *
   * @generated from field: repeated spark.connect.Expression arguments = 2;
   */
  arguments: Expression[];
};

/**
 * Describes the message spark.connect.UnresolvedTableValuedFunction.
 * Use `create(UnresolvedTableValuedFunctionSchema)` to create a new message.
 */
export const UnresolvedTableValuedFunctionSchema: GenMessage<UnresolvedTableValuedFunction> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 44);

/**
 * @generated from message spark.connect.ToSchema
 */
export type ToSchema = Message<"spark.connect.ToSchema"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The user provided schema.
   *
   * The Sever side will update the dataframe with this schema.
   *
   * @generated from field: spark.connect.DataType schema = 2;
   */
  schema?: DataType;
};

/**
 * Describes the message spark.connect.ToSchema.
 * Use `create(ToSchemaSchema)` to create a new message.
 */
export const ToSchemaSchema: GenMessage<ToSchema> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 45);

/**
 * @generated from message spark.connect.RepartitionByExpression
 */
export type RepartitionByExpression = Message<"spark.connect.RepartitionByExpression"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The partitioning expressions.
   *
   * @generated from field: repeated spark.connect.Expression partition_exprs = 2;
   */
  partitionExprs: Expression[];

  /**
   * (Optional) number of partitions, must be positive.
   *
   * @generated from field: optional int32 num_partitions = 3;
   */
  numPartitions?: number;
};

/**
 * Describes the message spark.connect.RepartitionByExpression.
 * Use `create(RepartitionByExpressionSchema)` to create a new message.
 */
export const RepartitionByExpressionSchema: GenMessage<RepartitionByExpression> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 46);

/**
 * @generated from message spark.connect.MapPartitions
 */
export type MapPartitions = Message<"spark.connect.MapPartitions"> & {
  /**
   * (Required) Input relation for a mapPartitions-equivalent API: mapInPandas, mapInArrow.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Input user-defined function.
   *
   * @generated from field: spark.connect.CommonInlineUserDefinedFunction func = 2;
   */
  func?: CommonInlineUserDefinedFunction;

  /**
   * (Optional) Whether to use barrier mode execution or not.
   *
   * @generated from field: optional bool is_barrier = 3;
   */
  isBarrier?: boolean;

  /**
   * (Optional) ResourceProfile id used for the stage level scheduling.
   *
   * @generated from field: optional int32 profile_id = 4;
   */
  profileId?: number;
};

/**
 * Describes the message spark.connect.MapPartitions.
 * Use `create(MapPartitionsSchema)` to create a new message.
 */
export const MapPartitionsSchema: GenMessage<MapPartitions> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 47);

/**
 * @generated from message spark.connect.GroupMap
 */
export type GroupMap = Message<"spark.connect.GroupMap"> & {
  /**
   * (Required) Input relation for Group Map API: apply, applyInPandas.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Expressions for grouping keys.
   *
   * @generated from field: repeated spark.connect.Expression grouping_expressions = 2;
   */
  groupingExpressions: Expression[];

  /**
   * (Required) Input user-defined function.
   *
   * @generated from field: spark.connect.CommonInlineUserDefinedFunction func = 3;
   */
  func?: CommonInlineUserDefinedFunction;

  /**
   * (Optional) Expressions for sorting. Only used by Scala Sorted Group Map API.
   *
   * @generated from field: repeated spark.connect.Expression sorting_expressions = 4;
   */
  sortingExpressions: Expression[];

  /**
   * Below fields are only used by (Flat)MapGroupsWithState
   * (Optional) Input relation for initial State.
   *
   * @generated from field: spark.connect.Relation initial_input = 5;
   */
  initialInput?: Relation;

  /**
   * (Optional) Expressions for grouping keys of the initial state input relation.
   *
   * @generated from field: repeated spark.connect.Expression initial_grouping_expressions = 6;
   */
  initialGroupingExpressions: Expression[];

  /**
   * (Optional) True if MapGroupsWithState, false if FlatMapGroupsWithState.
   *
   * @generated from field: optional bool is_map_groups_with_state = 7;
   */
  isMapGroupsWithState?: boolean;

  /**
   * (Optional) The output mode of the function.
   *
   * @generated from field: optional string output_mode = 8;
   */
  outputMode?: string;

  /**
   * (Optional) Timeout configuration for groups that do not receive data for a while.
   *
   * @generated from field: optional string timeout_conf = 9;
   */
  timeoutConf?: string;
};

/**
 * Describes the message spark.connect.GroupMap.
 * Use `create(GroupMapSchema)` to create a new message.
 */
export const GroupMapSchema: GenMessage<GroupMap> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 48);

/**
 * @generated from message spark.connect.CoGroupMap
 */
export type CoGroupMap = Message<"spark.connect.CoGroupMap"> & {
  /**
   * (Required) One input relation for CoGroup Map API - applyInPandas.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * Expressions for grouping keys of the first input relation.
   *
   * @generated from field: repeated spark.connect.Expression input_grouping_expressions = 2;
   */
  inputGroupingExpressions: Expression[];

  /**
   * (Required) The other input relation.
   *
   * @generated from field: spark.connect.Relation other = 3;
   */
  other?: Relation;

  /**
   * Expressions for grouping keys of the other input relation.
   *
   * @generated from field: repeated spark.connect.Expression other_grouping_expressions = 4;
   */
  otherGroupingExpressions: Expression[];

  /**
   * (Required) Input user-defined function.
   *
   * @generated from field: spark.connect.CommonInlineUserDefinedFunction func = 5;
   */
  func?: CommonInlineUserDefinedFunction;

  /**
   * (Optional) Expressions for sorting. Only used by Scala Sorted CoGroup Map API.
   *
   * @generated from field: repeated spark.connect.Expression input_sorting_expressions = 6;
   */
  inputSortingExpressions: Expression[];

  /**
   * (Optional) Expressions for sorting. Only used by Scala Sorted CoGroup Map API.
   *
   * @generated from field: repeated spark.connect.Expression other_sorting_expressions = 7;
   */
  otherSortingExpressions: Expression[];
};

/**
 * Describes the message spark.connect.CoGroupMap.
 * Use `create(CoGroupMapSchema)` to create a new message.
 */
export const CoGroupMapSchema: GenMessage<CoGroupMap> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 49);

/**
 * @generated from message spark.connect.ApplyInPandasWithState
 */
export type ApplyInPandasWithState = Message<"spark.connect.ApplyInPandasWithState"> & {
  /**
   * (Required) Input relation for applyInPandasWithState.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Expressions for grouping keys.
   *
   * @generated from field: repeated spark.connect.Expression grouping_expressions = 2;
   */
  groupingExpressions: Expression[];

  /**
   * (Required) Input user-defined function.
   *
   * @generated from field: spark.connect.CommonInlineUserDefinedFunction func = 3;
   */
  func?: CommonInlineUserDefinedFunction;

  /**
   * (Required) Schema for the output DataFrame.
   *
   * @generated from field: string output_schema = 4;
   */
  outputSchema: string;

  /**
   * (Required) Schema for the state.
   *
   * @generated from field: string state_schema = 5;
   */
  stateSchema: string;

  /**
   * (Required) The output mode of the function.
   *
   * @generated from field: string output_mode = 6;
   */
  outputMode: string;

  /**
   * (Required) Timeout configuration for groups that do not receive data for a while.
   *
   * @generated from field: string timeout_conf = 7;
   */
  timeoutConf: string;
};

/**
 * Describes the message spark.connect.ApplyInPandasWithState.
 * Use `create(ApplyInPandasWithStateSchema)` to create a new message.
 */
export const ApplyInPandasWithStateSchema: GenMessage<ApplyInPandasWithState> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 50);

/**
 * @generated from message spark.connect.CommonInlineUserDefinedTableFunction
 */
export type CommonInlineUserDefinedTableFunction = Message<"spark.connect.CommonInlineUserDefinedTableFunction"> & {
  /**
   * (Required) Name of the user-defined table function.
   *
   * @generated from field: string function_name = 1;
   */
  functionName: string;

  /**
   * (Optional) Whether the user-defined table function is deterministic.
   *
   * @generated from field: bool deterministic = 2;
   */
  deterministic: boolean;

  /**
   * (Optional) Function input arguments. Empty arguments are allowed.
   *
   * @generated from field: repeated spark.connect.Expression arguments = 3;
   */
  arguments: Expression[];

  /**
   * (Required) Type of the user-defined table function.
   *
   * @generated from oneof spark.connect.CommonInlineUserDefinedTableFunction.function
   */
  function: {
    /**
     * @generated from field: spark.connect.PythonUDTF python_udtf = 4;
     */
    value: PythonUDTF;
    case: "pythonUdtf";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.CommonInlineUserDefinedTableFunction.
 * Use `create(CommonInlineUserDefinedTableFunctionSchema)` to create a new message.
 */
export const CommonInlineUserDefinedTableFunctionSchema: GenMessage<CommonInlineUserDefinedTableFunction> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 51);

/**
 * @generated from message spark.connect.PythonUDTF
 */
export type PythonUDTF = Message<"spark.connect.PythonUDTF"> & {
  /**
   * (Optional) Return type of the Python UDTF.
   *
   * @generated from field: optional spark.connect.DataType return_type = 1;
   */
  returnType?: DataType;

  /**
   * (Required) EvalType of the Python UDTF.
   *
   * @generated from field: int32 eval_type = 2;
   */
  evalType: number;

  /**
   * (Required) The encoded commands of the Python UDTF.
   *
   * @generated from field: bytes command = 3;
   */
  command: Uint8Array;

  /**
   * (Required) Python version being used in the client.
   *
   * @generated from field: string python_ver = 4;
   */
  pythonVer: string;
};

/**
 * Describes the message spark.connect.PythonUDTF.
 * Use `create(PythonUDTFSchema)` to create a new message.
 */
export const PythonUDTFSchema: GenMessage<PythonUDTF> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 52);

/**
 * @generated from message spark.connect.CommonInlineUserDefinedDataSource
 */
export type CommonInlineUserDefinedDataSource = Message<"spark.connect.CommonInlineUserDefinedDataSource"> & {
  /**
   * (Required) Name of the data source.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * (Required) The data source type.
   *
   * @generated from oneof spark.connect.CommonInlineUserDefinedDataSource.data_source
   */
  dataSource: {
    /**
     * @generated from field: spark.connect.PythonDataSource python_data_source = 2;
     */
    value: PythonDataSource;
    case: "pythonDataSource";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.CommonInlineUserDefinedDataSource.
 * Use `create(CommonInlineUserDefinedDataSourceSchema)` to create a new message.
 */
export const CommonInlineUserDefinedDataSourceSchema: GenMessage<CommonInlineUserDefinedDataSource> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 53);

/**
 * @generated from message spark.connect.PythonDataSource
 */
export type PythonDataSource = Message<"spark.connect.PythonDataSource"> & {
  /**
   * (Required) The encoded commands of the Python data source.
   *
   * @generated from field: bytes command = 1;
   */
  command: Uint8Array;

  /**
   * (Required) Python version being used in the client.
   *
   * @generated from field: string python_ver = 2;
   */
  pythonVer: string;
};

/**
 * Describes the message spark.connect.PythonDataSource.
 * Use `create(PythonDataSourceSchema)` to create a new message.
 */
export const PythonDataSourceSchema: GenMessage<PythonDataSource> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 54);

/**
 * Collect arbitrary (named) metrics from a dataset.
 *
 * @generated from message spark.connect.CollectMetrics
 */
export type CollectMetrics = Message<"spark.connect.CollectMetrics"> & {
  /**
   * (Required) The input relation.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) Name of the metrics.
   *
   * @generated from field: string name = 2;
   */
  name: string;

  /**
   * (Required) The metric sequence.
   *
   * @generated from field: repeated spark.connect.Expression metrics = 3;
   */
  metrics: Expression[];
};

/**
 * Describes the message spark.connect.CollectMetrics.
 * Use `create(CollectMetricsSchema)` to create a new message.
 */
export const CollectMetricsSchema: GenMessage<CollectMetrics> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 55);

/**
 * @generated from message spark.connect.Parse
 */
export type Parse = Message<"spark.connect.Parse"> & {
  /**
   * (Required) Input relation to Parse. The input is expected to have single text column.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The expected format of the text.
   *
   * @generated from field: spark.connect.Parse.ParseFormat format = 2;
   */
  format: Parse_ParseFormat;

  /**
   * (Optional) DataType representing the schema. If not set, Spark will infer the schema.
   *
   * @generated from field: optional spark.connect.DataType schema = 3;
   */
  schema?: DataType;

  /**
   * Options for the csv/json parser. The map key is case insensitive.
   *
   * @generated from field: map<string, string> options = 4;
   */
  options: { [key: string]: string };
};

/**
 * Describes the message spark.connect.Parse.
 * Use `create(ParseSchema)` to create a new message.
 */
export const ParseSchema: GenMessage<Parse> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 56);

/**
 * @generated from enum spark.connect.Parse.ParseFormat
 */
export enum Parse_ParseFormat {
  /**
   * @generated from enum value: PARSE_FORMAT_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * @generated from enum value: PARSE_FORMAT_CSV = 1;
   */
  CSV = 1,

  /**
   * @generated from enum value: PARSE_FORMAT_JSON = 2;
   */
  JSON = 2,
}

/**
 * Describes the enum spark.connect.Parse.ParseFormat.
 */
export const Parse_ParseFormatSchema: GenEnum<Parse_ParseFormat> = /*@__PURE__*/
  enumDesc(file_spark_connect_relations, 56, 0);

/**
 * Relation of type [[AsOfJoin]].
 *
 * `left` and `right` must be present.
 *
 * @generated from message spark.connect.AsOfJoin
 */
export type AsOfJoin = Message<"spark.connect.AsOfJoin"> & {
  /**
   * (Required) Left input relation for a Join.
   *
   * @generated from field: spark.connect.Relation left = 1;
   */
  left?: Relation;

  /**
   * (Required) Right input relation for a Join.
   *
   * @generated from field: spark.connect.Relation right = 2;
   */
  right?: Relation;

  /**
   * (Required) Field to join on in left DataFrame
   *
   * @generated from field: spark.connect.Expression left_as_of = 3;
   */
  leftAsOf?: Expression;

  /**
   * (Required) Field to join on in right DataFrame
   *
   * @generated from field: spark.connect.Expression right_as_of = 4;
   */
  rightAsOf?: Expression;

  /**
   * (Optional) The join condition. Could be unset when `using_columns` is utilized.
   *
   * This field does not co-exist with using_columns.
   *
   * @generated from field: spark.connect.Expression join_expr = 5;
   */
  joinExpr?: Expression;

  /**
   * Optional. using_columns provides a list of columns that should present on both sides of
   * the join inputs that this Join will join on. For example A JOIN B USING col_name is
   * equivalent to A JOIN B on A.col_name = B.col_name.
   *
   * This field does not co-exist with join_condition.
   *
   * @generated from field: repeated string using_columns = 6;
   */
  usingColumns: string[];

  /**
   * (Required) The join type.
   *
   * @generated from field: string join_type = 7;
   */
  joinType: string;

  /**
   * (Optional) The asof tolerance within this range.
   *
   * @generated from field: spark.connect.Expression tolerance = 8;
   */
  tolerance?: Expression;

  /**
   * (Required) Whether allow matching with the same value or not.
   *
   * @generated from field: bool allow_exact_matches = 9;
   */
  allowExactMatches: boolean;

  /**
   * (Required) Whether to search for prior, subsequent, or closest matches.
   *
   * @generated from field: string direction = 10;
   */
  direction: string;
};

/**
 * Describes the message spark.connect.AsOfJoin.
 * Use `create(AsOfJoinSchema)` to create a new message.
 */
export const AsOfJoinSchema: GenMessage<AsOfJoin> = /*@__PURE__*/
  messageDesc(file_spark_connect_relations, 57);

