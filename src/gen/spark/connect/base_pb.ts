//
// Licensed to the Apache Software Foundation (ASF) under one or more
// contributor license agreements.  See the NOTICE file distributed with
// this work for additional information regarding copyright ownership.
// The ASF licenses this file to You under the Apache License, Version 2.0
// (the "License"); you may not use this file except in compliance with
// the License.  You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.2.3 with parameter "target=ts,import_extension=none,js_import_style=module"
// @generated from file spark/connect/base.proto (package spark.connect, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import type { Any } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_any } from "@bufbuild/protobuf/wkt";
import type { Command, CreateResourceProfileCommandResult, GetResourcesCommandResult, StreamingQueryCommandResult, StreamingQueryListenerEventsResult, StreamingQueryManagerCommandResult, WriteStreamOperationStartResult } from "./commands_pb";
import { file_spark_connect_commands } from "./commands_pb";
import type { StorageLevel } from "./common_pb";
import { file_spark_connect_common } from "./common_pb";
import type { Expression_Literal } from "./expressions_pb";
import { file_spark_connect_expressions } from "./expressions_pb";
import type { CachedRemoteRelation, Relation } from "./relations_pb";
import { file_spark_connect_relations } from "./relations_pb";
import type { DataType } from "./types_pb";
import { file_spark_connect_types } from "./types_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file spark/connect/base.proto.
 */
export const file_spark_connect_base: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_protobuf_any, file_spark_connect_commands, file_spark_connect_common, file_spark_connect_expressions, file_spark_connect_relations, file_spark_connect_types]);

/**
 * A [[Plan]] is the structure that carries the runtime information for the execution from the
 * client to the server. A [[Plan]] can either be of the type [[Relation]] which is a reference
 * to the underlying logical plan or it can be of the [[Command]] type that is used to execute
 * commands on the server.
 *
 * @generated from message spark.connect.Plan
 */
export type Plan = Message<"spark.connect.Plan"> & {
  /**
   * @generated from oneof spark.connect.Plan.op_type
   */
  opType: {
    /**
     * @generated from field: spark.connect.Relation root = 1;
     */
    value: Relation;
    case: "root";
  } | {
    /**
     * @generated from field: spark.connect.Command command = 2;
     */
    value: Command;
    case: "command";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.Plan.
 * Use `create(PlanSchema)` to create a new message.
 */
export const PlanSchema: GenMessage<Plan> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 0);

/**
 * User Context is used to refer to one particular user session that is executing
 * queries in the backend.
 *
 * @generated from message spark.connect.UserContext
 */
export type UserContext = Message<"spark.connect.UserContext"> & {
  /**
   * @generated from field: string user_id = 1;
   */
  userId: string;

  /**
   * @generated from field: string user_name = 2;
   */
  userName: string;

  /**
   * To extend the existing user context message that is used to identify incoming requests,
   * Spark Connect leverages the Any protobuf type that can be used to inject arbitrary other
   * messages into this message. Extensions are stored as a `repeated` type to be able to
   * handle multiple active extensions.
   *
   * @generated from field: repeated google.protobuf.Any extensions = 999;
   */
  extensions: Any[];
};

/**
 * Describes the message spark.connect.UserContext.
 * Use `create(UserContextSchema)` to create a new message.
 */
export const UserContextSchema: GenMessage<UserContext> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 1);

/**
 * Request to perform plan analyze, optionally to explain the plan.
 *
 * @generated from message spark.connect.AnalyzePlanRequest
 */
export type AnalyzePlanRequest = Message<"spark.connect.AnalyzePlanRequest"> & {
  /**
   * (Required)
   *
   * The session_id specifies a spark session for a user id (which is specified
   * by user_context.user_id). The session_id is set by the client to be able to
   * collate streaming responses from different queries within the dedicated session.
   * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * (Optional)
   *
   * Server-side generated idempotency key from the previous responses (if any). Server
   * can use this to validate that the server side session has not changed.
   *
   * @generated from field: optional string client_observed_server_side_session_id = 17;
   */
  clientObservedServerSideSessionId?: string;

  /**
   * (Required) User context
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 3;
   */
  clientType?: string;

  /**
   * @generated from oneof spark.connect.AnalyzePlanRequest.analyze
   */
  analyze: {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.Schema schema = 4;
     */
    value: AnalyzePlanRequest_Schema;
    case: "schema";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.Explain explain = 5;
     */
    value: AnalyzePlanRequest_Explain;
    case: "explain";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;
     */
    value: AnalyzePlanRequest_TreeString;
    case: "treeString";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;
     */
    value: AnalyzePlanRequest_IsLocal;
    case: "isLocal";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;
     */
    value: AnalyzePlanRequest_IsStreaming;
    case: "isStreaming";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;
     */
    value: AnalyzePlanRequest_InputFiles;
    case: "inputFiles";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;
     */
    value: AnalyzePlanRequest_SparkVersion;
    case: "sparkVersion";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;
     */
    value: AnalyzePlanRequest_DDLParse;
    case: "ddlParse";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;
     */
    value: AnalyzePlanRequest_SameSemantics;
    case: "sameSemantics";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;
     */
    value: AnalyzePlanRequest_SemanticHash;
    case: "semanticHash";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.Persist persist = 14;
     */
    value: AnalyzePlanRequest_Persist;
    case: "persist";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;
     */
    value: AnalyzePlanRequest_Unpersist;
    case: "unpersist";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;
     */
    value: AnalyzePlanRequest_GetStorageLevel;
    case: "getStorageLevel";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.
 * Use `create(AnalyzePlanRequestSchema)` to create a new message.
 */
export const AnalyzePlanRequestSchema: GenMessage<AnalyzePlanRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.Schema
 */
export type AnalyzePlanRequest_Schema = Message<"spark.connect.AnalyzePlanRequest.Schema"> & {
  /**
   * (Required) The logical plan to be analyzed.
   *
   * @generated from field: spark.connect.Plan plan = 1;
   */
  plan?: Plan;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.Schema.
 * Use `create(AnalyzePlanRequest_SchemaSchema)` to create a new message.
 */
export const AnalyzePlanRequest_SchemaSchema: GenMessage<AnalyzePlanRequest_Schema> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 0);

/**
 * Explains the input plan based on a configurable mode.
 *
 * @generated from message spark.connect.AnalyzePlanRequest.Explain
 */
export type AnalyzePlanRequest_Explain = Message<"spark.connect.AnalyzePlanRequest.Explain"> & {
  /**
   * (Required) The logical plan to be analyzed.
   *
   * @generated from field: spark.connect.Plan plan = 1;
   */
  plan?: Plan;

  /**
   * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
   *
   * @generated from field: spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;
   */
  explainMode: AnalyzePlanRequest_Explain_ExplainMode;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.Explain.
 * Use `create(AnalyzePlanRequest_ExplainSchema)` to create a new message.
 */
export const AnalyzePlanRequest_ExplainSchema: GenMessage<AnalyzePlanRequest_Explain> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 1);

/**
 * Plan explanation mode.
 *
 * @generated from enum spark.connect.AnalyzePlanRequest.Explain.ExplainMode
 */
export enum AnalyzePlanRequest_Explain_ExplainMode {
  /**
   * @generated from enum value: EXPLAIN_MODE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * Generates only physical plan.
   *
   * @generated from enum value: EXPLAIN_MODE_SIMPLE = 1;
   */
  SIMPLE = 1,

  /**
   * Generates parsed logical plan, analyzed logical plan, optimized logical plan and physical plan.
   * Parsed Logical plan is a unresolved plan that extracted from the query. Analyzed logical plans
   * transforms which translates unresolvedAttribute and unresolvedRelation into fully typed objects.
   * The optimized logical plan transforms through a set of optimization rules, resulting in the
   * physical plan.
   *
   * @generated from enum value: EXPLAIN_MODE_EXTENDED = 2;
   */
  EXTENDED = 2,

  /**
   * Generates code for the statement, if any and a physical plan.
   *
   * @generated from enum value: EXPLAIN_MODE_CODEGEN = 3;
   */
  CODEGEN = 3,

  /**
   * If plan node statistics are available, generates a logical plan and also the statistics.
   *
   * @generated from enum value: EXPLAIN_MODE_COST = 4;
   */
  COST = 4,

  /**
   * Generates a physical plan outline and also node details.
   *
   * @generated from enum value: EXPLAIN_MODE_FORMATTED = 5;
   */
  FORMATTED = 5,
}

/**
 * Describes the enum spark.connect.AnalyzePlanRequest.Explain.ExplainMode.
 */
export const AnalyzePlanRequest_Explain_ExplainModeSchema: GenEnum<AnalyzePlanRequest_Explain_ExplainMode> = /*@__PURE__*/
  enumDesc(file_spark_connect_base, 2, 1, 0);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.TreeString
 */
export type AnalyzePlanRequest_TreeString = Message<"spark.connect.AnalyzePlanRequest.TreeString"> & {
  /**
   * (Required) The logical plan to be analyzed.
   *
   * @generated from field: spark.connect.Plan plan = 1;
   */
  plan?: Plan;

  /**
   * (Optional) Max level of the schema.
   *
   * @generated from field: optional int32 level = 2;
   */
  level?: number;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.TreeString.
 * Use `create(AnalyzePlanRequest_TreeStringSchema)` to create a new message.
 */
export const AnalyzePlanRequest_TreeStringSchema: GenMessage<AnalyzePlanRequest_TreeString> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 2);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.IsLocal
 */
export type AnalyzePlanRequest_IsLocal = Message<"spark.connect.AnalyzePlanRequest.IsLocal"> & {
  /**
   * (Required) The logical plan to be analyzed.
   *
   * @generated from field: spark.connect.Plan plan = 1;
   */
  plan?: Plan;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.IsLocal.
 * Use `create(AnalyzePlanRequest_IsLocalSchema)` to create a new message.
 */
export const AnalyzePlanRequest_IsLocalSchema: GenMessage<AnalyzePlanRequest_IsLocal> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 3);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.IsStreaming
 */
export type AnalyzePlanRequest_IsStreaming = Message<"spark.connect.AnalyzePlanRequest.IsStreaming"> & {
  /**
   * (Required) The logical plan to be analyzed.
   *
   * @generated from field: spark.connect.Plan plan = 1;
   */
  plan?: Plan;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.IsStreaming.
 * Use `create(AnalyzePlanRequest_IsStreamingSchema)` to create a new message.
 */
export const AnalyzePlanRequest_IsStreamingSchema: GenMessage<AnalyzePlanRequest_IsStreaming> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 4);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.InputFiles
 */
export type AnalyzePlanRequest_InputFiles = Message<"spark.connect.AnalyzePlanRequest.InputFiles"> & {
  /**
   * (Required) The logical plan to be analyzed.
   *
   * @generated from field: spark.connect.Plan plan = 1;
   */
  plan?: Plan;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.InputFiles.
 * Use `create(AnalyzePlanRequest_InputFilesSchema)` to create a new message.
 */
export const AnalyzePlanRequest_InputFilesSchema: GenMessage<AnalyzePlanRequest_InputFiles> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 5);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.SparkVersion
 */
export type AnalyzePlanRequest_SparkVersion = Message<"spark.connect.AnalyzePlanRequest.SparkVersion"> & {
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.SparkVersion.
 * Use `create(AnalyzePlanRequest_SparkVersionSchema)` to create a new message.
 */
export const AnalyzePlanRequest_SparkVersionSchema: GenMessage<AnalyzePlanRequest_SparkVersion> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 6);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.DDLParse
 */
export type AnalyzePlanRequest_DDLParse = Message<"spark.connect.AnalyzePlanRequest.DDLParse"> & {
  /**
   * (Required) The DDL formatted string to be parsed.
   *
   * @generated from field: string ddl_string = 1;
   */
  ddlString: string;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.DDLParse.
 * Use `create(AnalyzePlanRequest_DDLParseSchema)` to create a new message.
 */
export const AnalyzePlanRequest_DDLParseSchema: GenMessage<AnalyzePlanRequest_DDLParse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 7);

/**
 * Returns `true` when the logical query plans  are equal and therefore return same results.
 *
 * @generated from message spark.connect.AnalyzePlanRequest.SameSemantics
 */
export type AnalyzePlanRequest_SameSemantics = Message<"spark.connect.AnalyzePlanRequest.SameSemantics"> & {
  /**
   * (Required) The plan to be compared.
   *
   * @generated from field: spark.connect.Plan target_plan = 1;
   */
  targetPlan?: Plan;

  /**
   * (Required) The other plan to be compared.
   *
   * @generated from field: spark.connect.Plan other_plan = 2;
   */
  otherPlan?: Plan;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.SameSemantics.
 * Use `create(AnalyzePlanRequest_SameSemanticsSchema)` to create a new message.
 */
export const AnalyzePlanRequest_SameSemanticsSchema: GenMessage<AnalyzePlanRequest_SameSemantics> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 8);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.SemanticHash
 */
export type AnalyzePlanRequest_SemanticHash = Message<"spark.connect.AnalyzePlanRequest.SemanticHash"> & {
  /**
   * (Required) The logical plan to get a hashCode.
   *
   * @generated from field: spark.connect.Plan plan = 1;
   */
  plan?: Plan;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.SemanticHash.
 * Use `create(AnalyzePlanRequest_SemanticHashSchema)` to create a new message.
 */
export const AnalyzePlanRequest_SemanticHashSchema: GenMessage<AnalyzePlanRequest_SemanticHash> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 9);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.Persist
 */
export type AnalyzePlanRequest_Persist = Message<"spark.connect.AnalyzePlanRequest.Persist"> & {
  /**
   * (Required) The logical plan to persist.
   *
   * @generated from field: spark.connect.Relation relation = 1;
   */
  relation?: Relation;

  /**
   * (Optional) The storage level.
   *
   * @generated from field: optional spark.connect.StorageLevel storage_level = 2;
   */
  storageLevel?: StorageLevel;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.Persist.
 * Use `create(AnalyzePlanRequest_PersistSchema)` to create a new message.
 */
export const AnalyzePlanRequest_PersistSchema: GenMessage<AnalyzePlanRequest_Persist> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 10);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.Unpersist
 */
export type AnalyzePlanRequest_Unpersist = Message<"spark.connect.AnalyzePlanRequest.Unpersist"> & {
  /**
   * (Required) The logical plan to unpersist.
   *
   * @generated from field: spark.connect.Relation relation = 1;
   */
  relation?: Relation;

  /**
   * (Optional) Whether to block until all blocks are deleted.
   *
   * @generated from field: optional bool blocking = 2;
   */
  blocking?: boolean;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.Unpersist.
 * Use `create(AnalyzePlanRequest_UnpersistSchema)` to create a new message.
 */
export const AnalyzePlanRequest_UnpersistSchema: GenMessage<AnalyzePlanRequest_Unpersist> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 11);

/**
 * @generated from message spark.connect.AnalyzePlanRequest.GetStorageLevel
 */
export type AnalyzePlanRequest_GetStorageLevel = Message<"spark.connect.AnalyzePlanRequest.GetStorageLevel"> & {
  /**
   * (Required) The logical plan to get the storage level.
   *
   * @generated from field: spark.connect.Relation relation = 1;
   */
  relation?: Relation;
};

/**
 * Describes the message spark.connect.AnalyzePlanRequest.GetStorageLevel.
 * Use `create(AnalyzePlanRequest_GetStorageLevelSchema)` to create a new message.
 */
export const AnalyzePlanRequest_GetStorageLevelSchema: GenMessage<AnalyzePlanRequest_GetStorageLevel> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 2, 12);

/**
 * Response to performing analysis of the query. Contains relevant metadata to be able to
 * reason about the performance.
 * Next ID: 16
 *
 * @generated from message spark.connect.AnalyzePlanResponse
 */
export type AnalyzePlanResponse = Message<"spark.connect.AnalyzePlanResponse"> & {
  /**
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * Server-side generated idempotency key that the client can use to assert that the server side
   * session has not changed.
   *
   * @generated from field: string server_side_session_id = 15;
   */
  serverSideSessionId: string;

  /**
   * @generated from oneof spark.connect.AnalyzePlanResponse.result
   */
  result: {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.Schema schema = 2;
     */
    value: AnalyzePlanResponse_Schema;
    case: "schema";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.Explain explain = 3;
     */
    value: AnalyzePlanResponse_Explain;
    case: "explain";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.TreeString tree_string = 4;
     */
    value: AnalyzePlanResponse_TreeString;
    case: "treeString";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.IsLocal is_local = 5;
     */
    value: AnalyzePlanResponse_IsLocal;
    case: "isLocal";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.IsStreaming is_streaming = 6;
     */
    value: AnalyzePlanResponse_IsStreaming;
    case: "isStreaming";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.InputFiles input_files = 7;
     */
    value: AnalyzePlanResponse_InputFiles;
    case: "inputFiles";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.SparkVersion spark_version = 8;
     */
    value: AnalyzePlanResponse_SparkVersion;
    case: "sparkVersion";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.DDLParse ddl_parse = 9;
     */
    value: AnalyzePlanResponse_DDLParse;
    case: "ddlParse";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.SameSemantics same_semantics = 10;
     */
    value: AnalyzePlanResponse_SameSemantics;
    case: "sameSemantics";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.SemanticHash semantic_hash = 11;
     */
    value: AnalyzePlanResponse_SemanticHash;
    case: "semanticHash";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.Persist persist = 12;
     */
    value: AnalyzePlanResponse_Persist;
    case: "persist";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.Unpersist unpersist = 13;
     */
    value: AnalyzePlanResponse_Unpersist;
    case: "unpersist";
  } | {
    /**
     * @generated from field: spark.connect.AnalyzePlanResponse.GetStorageLevel get_storage_level = 14;
     */
    value: AnalyzePlanResponse_GetStorageLevel;
    case: "getStorageLevel";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.
 * Use `create(AnalyzePlanResponseSchema)` to create a new message.
 */
export const AnalyzePlanResponseSchema: GenMessage<AnalyzePlanResponse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.Schema
 */
export type AnalyzePlanResponse_Schema = Message<"spark.connect.AnalyzePlanResponse.Schema"> & {
  /**
   * @generated from field: spark.connect.DataType schema = 1;
   */
  schema?: DataType;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.Schema.
 * Use `create(AnalyzePlanResponse_SchemaSchema)` to create a new message.
 */
export const AnalyzePlanResponse_SchemaSchema: GenMessage<AnalyzePlanResponse_Schema> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 0);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.Explain
 */
export type AnalyzePlanResponse_Explain = Message<"spark.connect.AnalyzePlanResponse.Explain"> & {
  /**
   * @generated from field: string explain_string = 1;
   */
  explainString: string;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.Explain.
 * Use `create(AnalyzePlanResponse_ExplainSchema)` to create a new message.
 */
export const AnalyzePlanResponse_ExplainSchema: GenMessage<AnalyzePlanResponse_Explain> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 1);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.TreeString
 */
export type AnalyzePlanResponse_TreeString = Message<"spark.connect.AnalyzePlanResponse.TreeString"> & {
  /**
   * @generated from field: string tree_string = 1;
   */
  treeString: string;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.TreeString.
 * Use `create(AnalyzePlanResponse_TreeStringSchema)` to create a new message.
 */
export const AnalyzePlanResponse_TreeStringSchema: GenMessage<AnalyzePlanResponse_TreeString> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 2);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.IsLocal
 */
export type AnalyzePlanResponse_IsLocal = Message<"spark.connect.AnalyzePlanResponse.IsLocal"> & {
  /**
   * @generated from field: bool is_local = 1;
   */
  isLocal: boolean;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.IsLocal.
 * Use `create(AnalyzePlanResponse_IsLocalSchema)` to create a new message.
 */
export const AnalyzePlanResponse_IsLocalSchema: GenMessage<AnalyzePlanResponse_IsLocal> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 3);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.IsStreaming
 */
export type AnalyzePlanResponse_IsStreaming = Message<"spark.connect.AnalyzePlanResponse.IsStreaming"> & {
  /**
   * @generated from field: bool is_streaming = 1;
   */
  isStreaming: boolean;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.IsStreaming.
 * Use `create(AnalyzePlanResponse_IsStreamingSchema)` to create a new message.
 */
export const AnalyzePlanResponse_IsStreamingSchema: GenMessage<AnalyzePlanResponse_IsStreaming> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 4);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.InputFiles
 */
export type AnalyzePlanResponse_InputFiles = Message<"spark.connect.AnalyzePlanResponse.InputFiles"> & {
  /**
   * A best-effort snapshot of the files that compose this Dataset
   *
   * @generated from field: repeated string files = 1;
   */
  files: string[];
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.InputFiles.
 * Use `create(AnalyzePlanResponse_InputFilesSchema)` to create a new message.
 */
export const AnalyzePlanResponse_InputFilesSchema: GenMessage<AnalyzePlanResponse_InputFiles> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 5);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.SparkVersion
 */
export type AnalyzePlanResponse_SparkVersion = Message<"spark.connect.AnalyzePlanResponse.SparkVersion"> & {
  /**
   * @generated from field: string version = 1;
   */
  version: string;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.SparkVersion.
 * Use `create(AnalyzePlanResponse_SparkVersionSchema)` to create a new message.
 */
export const AnalyzePlanResponse_SparkVersionSchema: GenMessage<AnalyzePlanResponse_SparkVersion> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 6);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.DDLParse
 */
export type AnalyzePlanResponse_DDLParse = Message<"spark.connect.AnalyzePlanResponse.DDLParse"> & {
  /**
   * @generated from field: spark.connect.DataType parsed = 1;
   */
  parsed?: DataType;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.DDLParse.
 * Use `create(AnalyzePlanResponse_DDLParseSchema)` to create a new message.
 */
export const AnalyzePlanResponse_DDLParseSchema: GenMessage<AnalyzePlanResponse_DDLParse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 7);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.SameSemantics
 */
export type AnalyzePlanResponse_SameSemantics = Message<"spark.connect.AnalyzePlanResponse.SameSemantics"> & {
  /**
   * @generated from field: bool result = 1;
   */
  result: boolean;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.SameSemantics.
 * Use `create(AnalyzePlanResponse_SameSemanticsSchema)` to create a new message.
 */
export const AnalyzePlanResponse_SameSemanticsSchema: GenMessage<AnalyzePlanResponse_SameSemantics> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 8);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.SemanticHash
 */
export type AnalyzePlanResponse_SemanticHash = Message<"spark.connect.AnalyzePlanResponse.SemanticHash"> & {
  /**
   * @generated from field: int32 result = 1;
   */
  result: number;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.SemanticHash.
 * Use `create(AnalyzePlanResponse_SemanticHashSchema)` to create a new message.
 */
export const AnalyzePlanResponse_SemanticHashSchema: GenMessage<AnalyzePlanResponse_SemanticHash> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 9);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.Persist
 */
export type AnalyzePlanResponse_Persist = Message<"spark.connect.AnalyzePlanResponse.Persist"> & {
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.Persist.
 * Use `create(AnalyzePlanResponse_PersistSchema)` to create a new message.
 */
export const AnalyzePlanResponse_PersistSchema: GenMessage<AnalyzePlanResponse_Persist> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 10);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.Unpersist
 */
export type AnalyzePlanResponse_Unpersist = Message<"spark.connect.AnalyzePlanResponse.Unpersist"> & {
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.Unpersist.
 * Use `create(AnalyzePlanResponse_UnpersistSchema)` to create a new message.
 */
export const AnalyzePlanResponse_UnpersistSchema: GenMessage<AnalyzePlanResponse_Unpersist> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 11);

/**
 * @generated from message spark.connect.AnalyzePlanResponse.GetStorageLevel
 */
export type AnalyzePlanResponse_GetStorageLevel = Message<"spark.connect.AnalyzePlanResponse.GetStorageLevel"> & {
  /**
   * (Required) The StorageLevel as a result of get_storage_level request.
   *
   * @generated from field: spark.connect.StorageLevel storage_level = 1;
   */
  storageLevel?: StorageLevel;
};

/**
 * Describes the message spark.connect.AnalyzePlanResponse.GetStorageLevel.
 * Use `create(AnalyzePlanResponse_GetStorageLevelSchema)` to create a new message.
 */
export const AnalyzePlanResponse_GetStorageLevelSchema: GenMessage<AnalyzePlanResponse_GetStorageLevel> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 3, 12);

/**
 * A request to be executed by the service.
 *
 * @generated from message spark.connect.ExecutePlanRequest
 */
export type ExecutePlanRequest = Message<"spark.connect.ExecutePlanRequest"> & {
  /**
   * (Required)
   *
   * The session_id specifies a spark session for a user id (which is specified
   * by user_context.user_id). The session_id is set by the client to be able to
   * collate streaming responses from different queries within the dedicated session.
   * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * (Optional)
   *
   * Server-side generated idempotency key from the previous responses (if any). Server
   * can use this to validate that the server side session has not changed.
   *
   * @generated from field: optional string client_observed_server_side_session_id = 8;
   */
  clientObservedServerSideSessionId?: string;

  /**
   * (Required) User context
   *
   * user_context.user_id and session+id both identify a unique remote spark session on the
   * server side.
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * (Optional)
   * Provide an id for this request. If not provided, it will be generated by the server.
   * It is returned in every ExecutePlanResponse.operation_id of the ExecutePlan response stream.
   * The id must be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   *
   * @generated from field: optional string operation_id = 6;
   */
  operationId?: string;

  /**
   * (Required) The logical plan to be executed / analyzed.
   *
   * @generated from field: spark.connect.Plan plan = 3;
   */
  plan?: Plan;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 4;
   */
  clientType?: string;

  /**
   * Repeated element for options that can be passed to the request. This element is currently
   * unused but allows to pass in an extension value used for arbitrary options.
   *
   * @generated from field: repeated spark.connect.ExecutePlanRequest.RequestOption request_options = 5;
   */
  requestOptions: ExecutePlanRequest_RequestOption[];

  /**
   * Tags to tag the given execution with.
   * Tags cannot contain ',' character and cannot be empty strings.
   * Used by Interrupt with interrupt.tag.
   *
   * @generated from field: repeated string tags = 7;
   */
  tags: string[];
};

/**
 * Describes the message spark.connect.ExecutePlanRequest.
 * Use `create(ExecutePlanRequestSchema)` to create a new message.
 */
export const ExecutePlanRequestSchema: GenMessage<ExecutePlanRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 4);

/**
 * @generated from message spark.connect.ExecutePlanRequest.RequestOption
 */
export type ExecutePlanRequest_RequestOption = Message<"spark.connect.ExecutePlanRequest.RequestOption"> & {
  /**
   * @generated from oneof spark.connect.ExecutePlanRequest.RequestOption.request_option
   */
  requestOption: {
    /**
     * @generated from field: spark.connect.ReattachOptions reattach_options = 1;
     */
    value: ReattachOptions;
    case: "reattachOptions";
  } | {
    /**
     * Extension type for request options
     *
     * @generated from field: google.protobuf.Any extension = 999;
     */
    value: Any;
    case: "extension";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.ExecutePlanRequest.RequestOption.
 * Use `create(ExecutePlanRequest_RequestOptionSchema)` to create a new message.
 */
export const ExecutePlanRequest_RequestOptionSchema: GenMessage<ExecutePlanRequest_RequestOption> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 4, 0);

/**
 * The response of a query, can be one or more for each request. Responses belonging to the
 * same input query, carry the same `session_id`.
 * Next ID: 17
 *
 * @generated from message spark.connect.ExecutePlanResponse
 */
export type ExecutePlanResponse = Message<"spark.connect.ExecutePlanResponse"> & {
  /**
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * Server-side generated idempotency key that the client can use to assert that the server side
   * session has not changed.
   *
   * @generated from field: string server_side_session_id = 15;
   */
  serverSideSessionId: string;

  /**
   * Identifies the ExecutePlan execution.
   * If set by the client in ExecutePlanRequest.operationId, that value is returned.
   * Otherwise generated by the server.
   * It is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   *
   * @generated from field: string operation_id = 12;
   */
  operationId: string;

  /**
   * Identified the response in the stream.
   * The id is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   *
   * @generated from field: string response_id = 13;
   */
  responseId: string;

  /**
   * Union type for the different response messages.
   *
   * @generated from oneof spark.connect.ExecutePlanResponse.response_type
   */
  responseType: {
    /**
     * @generated from field: spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;
     */
    value: ExecutePlanResponse_ArrowBatch;
    case: "arrowBatch";
  } | {
    /**
     * Special case for executing SQL commands.
     *
     * @generated from field: spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;
     */
    value: ExecutePlanResponse_SqlCommandResult;
    case: "sqlCommandResult";
  } | {
    /**
     * Response for a streaming query.
     *
     * @generated from field: spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;
     */
    value: WriteStreamOperationStartResult;
    case: "writeStreamOperationStartResult";
  } | {
    /**
     * Response for commands on a streaming query.
     *
     * @generated from field: spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;
     */
    value: StreamingQueryCommandResult;
    case: "streamingQueryCommandResult";
  } | {
    /**
     * Response for 'SparkContext.resources'.
     *
     * @generated from field: spark.connect.GetResourcesCommandResult get_resources_command_result = 10;
     */
    value: GetResourcesCommandResult;
    case: "getResourcesCommandResult";
  } | {
    /**
     * Response for commands on the streaming query manager.
     *
     * @generated from field: spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;
     */
    value: StreamingQueryManagerCommandResult;
    case: "streamingQueryManagerCommandResult";
  } | {
    /**
     * Response for commands on the client side streaming query listener.
     *
     * @generated from field: spark.connect.StreamingQueryListenerEventsResult streaming_query_listener_events_result = 16;
     */
    value: StreamingQueryListenerEventsResult;
    case: "streamingQueryListenerEventsResult";
  } | {
    /**
     * Response type informing if the stream is complete in reattachable execution.
     *
     * @generated from field: spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;
     */
    value: ExecutePlanResponse_ResultComplete;
    case: "resultComplete";
  } | {
    /**
     * Response for command that creates ResourceProfile.
     *
     * @generated from field: spark.connect.CreateResourceProfileCommandResult create_resource_profile_command_result = 17;
     */
    value: CreateResourceProfileCommandResult;
    case: "createResourceProfileCommandResult";
  } | {
    /**
     * (Optional) Intermediate query progress reports.
     *
     * @generated from field: spark.connect.ExecutePlanResponse.ExecutionProgress execution_progress = 18;
     */
    value: ExecutePlanResponse_ExecutionProgress;
    case: "executionProgress";
  } | {
    /**
     * Response for command that checkpoints a DataFrame.
     *
     * @generated from field: spark.connect.CheckpointCommandResult checkpoint_command_result = 19;
     */
    value: CheckpointCommandResult;
    case: "checkpointCommandResult";
  } | {
    /**
     * Support arbitrary result objects.
     *
     * @generated from field: google.protobuf.Any extension = 999;
     */
    value: Any;
    case: "extension";
  } | { case: undefined; value?: undefined };

  /**
   * Metrics for the query execution. Typically, this field is only present in the last
   * batch of results and then represent the overall state of the query execution.
   *
   * @generated from field: spark.connect.ExecutePlanResponse.Metrics metrics = 4;
   */
  metrics?: ExecutePlanResponse_Metrics;

  /**
   * The metrics observed during the execution of the query plan.
   *
   * @generated from field: repeated spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;
   */
  observedMetrics: ExecutePlanResponse_ObservedMetrics[];

  /**
   * (Optional) The Spark schema. This field is available when `collect` is called.
   *
   * @generated from field: spark.connect.DataType schema = 7;
   */
  schema?: DataType;
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.
 * Use `create(ExecutePlanResponseSchema)` to create a new message.
 */
export const ExecutePlanResponseSchema: GenMessage<ExecutePlanResponse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5);

/**
 * A SQL command returns an opaque Relation that can be directly used as input for the next
 * call.
 *
 * @generated from message spark.connect.ExecutePlanResponse.SqlCommandResult
 */
export type ExecutePlanResponse_SqlCommandResult = Message<"spark.connect.ExecutePlanResponse.SqlCommandResult"> & {
  /**
   * @generated from field: spark.connect.Relation relation = 1;
   */
  relation?: Relation;
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.SqlCommandResult.
 * Use `create(ExecutePlanResponse_SqlCommandResultSchema)` to create a new message.
 */
export const ExecutePlanResponse_SqlCommandResultSchema: GenMessage<ExecutePlanResponse_SqlCommandResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5, 0);

/**
 * Batch results of metrics.
 *
 * @generated from message spark.connect.ExecutePlanResponse.ArrowBatch
 */
export type ExecutePlanResponse_ArrowBatch = Message<"spark.connect.ExecutePlanResponse.ArrowBatch"> & {
  /**
   * Count rows in `data`. Must match the number of rows inside `data`.
   *
   * @generated from field: int64 row_count = 1;
   */
  rowCount: bigint;

  /**
   * Serialized Arrow data.
   *
   * @generated from field: bytes data = 2;
   */
  data: Uint8Array;

  /**
   * If set, row offset of the start of this ArrowBatch in execution results.
   *
   * @generated from field: optional int64 start_offset = 3;
   */
  startOffset?: bigint;
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.ArrowBatch.
 * Use `create(ExecutePlanResponse_ArrowBatchSchema)` to create a new message.
 */
export const ExecutePlanResponse_ArrowBatchSchema: GenMessage<ExecutePlanResponse_ArrowBatch> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5, 1);

/**
 * @generated from message spark.connect.ExecutePlanResponse.Metrics
 */
export type ExecutePlanResponse_Metrics = Message<"spark.connect.ExecutePlanResponse.Metrics"> & {
  /**
   * @generated from field: repeated spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;
   */
  metrics: ExecutePlanResponse_Metrics_MetricObject[];
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.Metrics.
 * Use `create(ExecutePlanResponse_MetricsSchema)` to create a new message.
 */
export const ExecutePlanResponse_MetricsSchema: GenMessage<ExecutePlanResponse_Metrics> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5, 2);

/**
 * @generated from message spark.connect.ExecutePlanResponse.Metrics.MetricObject
 */
export type ExecutePlanResponse_Metrics_MetricObject = Message<"spark.connect.ExecutePlanResponse.Metrics.MetricObject"> & {
  /**
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @generated from field: int64 plan_id = 2;
   */
  planId: bigint;

  /**
   * @generated from field: int64 parent = 3;
   */
  parent: bigint;

  /**
   * @generated from field: map<string, spark.connect.ExecutePlanResponse.Metrics.MetricValue> execution_metrics = 4;
   */
  executionMetrics: { [key: string]: ExecutePlanResponse_Metrics_MetricValue };
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.Metrics.MetricObject.
 * Use `create(ExecutePlanResponse_Metrics_MetricObjectSchema)` to create a new message.
 */
export const ExecutePlanResponse_Metrics_MetricObjectSchema: GenMessage<ExecutePlanResponse_Metrics_MetricObject> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5, 2, 0);

/**
 * @generated from message spark.connect.ExecutePlanResponse.Metrics.MetricValue
 */
export type ExecutePlanResponse_Metrics_MetricValue = Message<"spark.connect.ExecutePlanResponse.Metrics.MetricValue"> & {
  /**
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @generated from field: int64 value = 2;
   */
  value: bigint;

  /**
   * @generated from field: string metric_type = 3;
   */
  metricType: string;
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.Metrics.MetricValue.
 * Use `create(ExecutePlanResponse_Metrics_MetricValueSchema)` to create a new message.
 */
export const ExecutePlanResponse_Metrics_MetricValueSchema: GenMessage<ExecutePlanResponse_Metrics_MetricValue> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5, 2, 1);

/**
 * @generated from message spark.connect.ExecutePlanResponse.ObservedMetrics
 */
export type ExecutePlanResponse_ObservedMetrics = Message<"spark.connect.ExecutePlanResponse.ObservedMetrics"> & {
  /**
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @generated from field: repeated spark.connect.Expression.Literal values = 2;
   */
  values: Expression_Literal[];

  /**
   * @generated from field: repeated string keys = 3;
   */
  keys: string[];

  /**
   * @generated from field: int64 plan_id = 4;
   */
  planId: bigint;
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.ObservedMetrics.
 * Use `create(ExecutePlanResponse_ObservedMetricsSchema)` to create a new message.
 */
export const ExecutePlanResponse_ObservedMetricsSchema: GenMessage<ExecutePlanResponse_ObservedMetrics> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5, 3);

/**
 * If present, in a reattachable execution this means that after server sends onComplete,
 * the execution is complete. If the server sends onComplete without sending a ResultComplete,
 * it means that there is more, and the client should use ReattachExecute RPC to continue.
 *
 * @generated from message spark.connect.ExecutePlanResponse.ResultComplete
 */
export type ExecutePlanResponse_ResultComplete = Message<"spark.connect.ExecutePlanResponse.ResultComplete"> & {
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.ResultComplete.
 * Use `create(ExecutePlanResponse_ResultCompleteSchema)` to create a new message.
 */
export const ExecutePlanResponse_ResultCompleteSchema: GenMessage<ExecutePlanResponse_ResultComplete> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5, 4);

/**
 * This message is used to communicate progress about the query progress during the execution.
 *
 * @generated from message spark.connect.ExecutePlanResponse.ExecutionProgress
 */
export type ExecutePlanResponse_ExecutionProgress = Message<"spark.connect.ExecutePlanResponse.ExecutionProgress"> & {
  /**
   * Captures the progress of each individual stage.
   *
   * @generated from field: repeated spark.connect.ExecutePlanResponse.ExecutionProgress.StageInfo stages = 1;
   */
  stages: ExecutePlanResponse_ExecutionProgress_StageInfo[];

  /**
   * Captures the currently in progress tasks.
   *
   * @generated from field: int64 num_inflight_tasks = 2;
   */
  numInflightTasks: bigint;
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.ExecutionProgress.
 * Use `create(ExecutePlanResponse_ExecutionProgressSchema)` to create a new message.
 */
export const ExecutePlanResponse_ExecutionProgressSchema: GenMessage<ExecutePlanResponse_ExecutionProgress> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5, 5);

/**
 * @generated from message spark.connect.ExecutePlanResponse.ExecutionProgress.StageInfo
 */
export type ExecutePlanResponse_ExecutionProgress_StageInfo = Message<"spark.connect.ExecutePlanResponse.ExecutionProgress.StageInfo"> & {
  /**
   * @generated from field: int64 stage_id = 1;
   */
  stageId: bigint;

  /**
   * @generated from field: int64 num_tasks = 2;
   */
  numTasks: bigint;

  /**
   * @generated from field: int64 num_completed_tasks = 3;
   */
  numCompletedTasks: bigint;

  /**
   * @generated from field: int64 input_bytes_read = 4;
   */
  inputBytesRead: bigint;

  /**
   * @generated from field: bool done = 5;
   */
  done: boolean;
};

/**
 * Describes the message spark.connect.ExecutePlanResponse.ExecutionProgress.StageInfo.
 * Use `create(ExecutePlanResponse_ExecutionProgress_StageInfoSchema)` to create a new message.
 */
export const ExecutePlanResponse_ExecutionProgress_StageInfoSchema: GenMessage<ExecutePlanResponse_ExecutionProgress_StageInfo> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 5, 5, 0);

/**
 * The key-value pair for the config request and response.
 *
 * @generated from message spark.connect.KeyValue
 */
export type KeyValue = Message<"spark.connect.KeyValue"> & {
  /**
   * (Required) The key.
   *
   * @generated from field: string key = 1;
   */
  key: string;

  /**
   * (Optional) The value.
   *
   * @generated from field: optional string value = 2;
   */
  value?: string;
};

/**
 * Describes the message spark.connect.KeyValue.
 * Use `create(KeyValueSchema)` to create a new message.
 */
export const KeyValueSchema: GenMessage<KeyValue> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 6);

/**
 * Request to update or fetch the configurations.
 *
 * @generated from message spark.connect.ConfigRequest
 */
export type ConfigRequest = Message<"spark.connect.ConfigRequest"> & {
  /**
   * (Required)
   *
   * The session_id specifies a spark session for a user id (which is specified
   * by user_context.user_id). The session_id is set by the client to be able to
   * collate streaming responses from different queries within the dedicated session.
   * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * (Optional)
   *
   * Server-side generated idempotency key from the previous responses (if any). Server
   * can use this to validate that the server side session has not changed.
   *
   * @generated from field: optional string client_observed_server_side_session_id = 8;
   */
  clientObservedServerSideSessionId?: string;

  /**
   * (Required) User context
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * (Required) The operation for the config.
   *
   * @generated from field: spark.connect.ConfigRequest.Operation operation = 3;
   */
  operation?: ConfigRequest_Operation;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 4;
   */
  clientType?: string;
};

/**
 * Describes the message spark.connect.ConfigRequest.
 * Use `create(ConfigRequestSchema)` to create a new message.
 */
export const ConfigRequestSchema: GenMessage<ConfigRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 7);

/**
 * @generated from message spark.connect.ConfigRequest.Operation
 */
export type ConfigRequest_Operation = Message<"spark.connect.ConfigRequest.Operation"> & {
  /**
   * @generated from oneof spark.connect.ConfigRequest.Operation.op_type
   */
  opType: {
    /**
     * @generated from field: spark.connect.ConfigRequest.Set set = 1;
     */
    value: ConfigRequest_Set;
    case: "set";
  } | {
    /**
     * @generated from field: spark.connect.ConfigRequest.Get get = 2;
     */
    value: ConfigRequest_Get;
    case: "get";
  } | {
    /**
     * @generated from field: spark.connect.ConfigRequest.GetWithDefault get_with_default = 3;
     */
    value: ConfigRequest_GetWithDefault;
    case: "getWithDefault";
  } | {
    /**
     * @generated from field: spark.connect.ConfigRequest.GetOption get_option = 4;
     */
    value: ConfigRequest_GetOption;
    case: "getOption";
  } | {
    /**
     * @generated from field: spark.connect.ConfigRequest.GetAll get_all = 5;
     */
    value: ConfigRequest_GetAll;
    case: "getAll";
  } | {
    /**
     * @generated from field: spark.connect.ConfigRequest.Unset unset = 6;
     */
    value: ConfigRequest_Unset;
    case: "unset";
  } | {
    /**
     * @generated from field: spark.connect.ConfigRequest.IsModifiable is_modifiable = 7;
     */
    value: ConfigRequest_IsModifiable;
    case: "isModifiable";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.ConfigRequest.Operation.
 * Use `create(ConfigRequest_OperationSchema)` to create a new message.
 */
export const ConfigRequest_OperationSchema: GenMessage<ConfigRequest_Operation> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 7, 0);

/**
 * @generated from message spark.connect.ConfigRequest.Set
 */
export type ConfigRequest_Set = Message<"spark.connect.ConfigRequest.Set"> & {
  /**
   * (Required) The config key-value pairs to set.
   *
   * @generated from field: repeated spark.connect.KeyValue pairs = 1;
   */
  pairs: KeyValue[];
};

/**
 * Describes the message spark.connect.ConfigRequest.Set.
 * Use `create(ConfigRequest_SetSchema)` to create a new message.
 */
export const ConfigRequest_SetSchema: GenMessage<ConfigRequest_Set> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 7, 1);

/**
 * @generated from message spark.connect.ConfigRequest.Get
 */
export type ConfigRequest_Get = Message<"spark.connect.ConfigRequest.Get"> & {
  /**
   * (Required) The config keys to get.
   *
   * @generated from field: repeated string keys = 1;
   */
  keys: string[];
};

/**
 * Describes the message spark.connect.ConfigRequest.Get.
 * Use `create(ConfigRequest_GetSchema)` to create a new message.
 */
export const ConfigRequest_GetSchema: GenMessage<ConfigRequest_Get> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 7, 2);

/**
 * @generated from message spark.connect.ConfigRequest.GetWithDefault
 */
export type ConfigRequest_GetWithDefault = Message<"spark.connect.ConfigRequest.GetWithDefault"> & {
  /**
   * (Required) The config key-value pairs to get. The value will be used as the default value.
   *
   * @generated from field: repeated spark.connect.KeyValue pairs = 1;
   */
  pairs: KeyValue[];
};

/**
 * Describes the message spark.connect.ConfigRequest.GetWithDefault.
 * Use `create(ConfigRequest_GetWithDefaultSchema)` to create a new message.
 */
export const ConfigRequest_GetWithDefaultSchema: GenMessage<ConfigRequest_GetWithDefault> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 7, 3);

/**
 * @generated from message spark.connect.ConfigRequest.GetOption
 */
export type ConfigRequest_GetOption = Message<"spark.connect.ConfigRequest.GetOption"> & {
  /**
   * (Required) The config keys to get optionally.
   *
   * @generated from field: repeated string keys = 1;
   */
  keys: string[];
};

/**
 * Describes the message spark.connect.ConfigRequest.GetOption.
 * Use `create(ConfigRequest_GetOptionSchema)` to create a new message.
 */
export const ConfigRequest_GetOptionSchema: GenMessage<ConfigRequest_GetOption> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 7, 4);

/**
 * @generated from message spark.connect.ConfigRequest.GetAll
 */
export type ConfigRequest_GetAll = Message<"spark.connect.ConfigRequest.GetAll"> & {
  /**
   * (Optional) The prefix of the config key to get.
   *
   * @generated from field: optional string prefix = 1;
   */
  prefix?: string;
};

/**
 * Describes the message spark.connect.ConfigRequest.GetAll.
 * Use `create(ConfigRequest_GetAllSchema)` to create a new message.
 */
export const ConfigRequest_GetAllSchema: GenMessage<ConfigRequest_GetAll> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 7, 5);

/**
 * @generated from message spark.connect.ConfigRequest.Unset
 */
export type ConfigRequest_Unset = Message<"spark.connect.ConfigRequest.Unset"> & {
  /**
   * (Required) The config keys to unset.
   *
   * @generated from field: repeated string keys = 1;
   */
  keys: string[];
};

/**
 * Describes the message spark.connect.ConfigRequest.Unset.
 * Use `create(ConfigRequest_UnsetSchema)` to create a new message.
 */
export const ConfigRequest_UnsetSchema: GenMessage<ConfigRequest_Unset> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 7, 6);

/**
 * @generated from message spark.connect.ConfigRequest.IsModifiable
 */
export type ConfigRequest_IsModifiable = Message<"spark.connect.ConfigRequest.IsModifiable"> & {
  /**
   * (Required) The config keys to check the config is modifiable.
   *
   * @generated from field: repeated string keys = 1;
   */
  keys: string[];
};

/**
 * Describes the message spark.connect.ConfigRequest.IsModifiable.
 * Use `create(ConfigRequest_IsModifiableSchema)` to create a new message.
 */
export const ConfigRequest_IsModifiableSchema: GenMessage<ConfigRequest_IsModifiable> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 7, 7);

/**
 * Response to the config request.
 * Next ID: 5
 *
 * @generated from message spark.connect.ConfigResponse
 */
export type ConfigResponse = Message<"spark.connect.ConfigResponse"> & {
  /**
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * Server-side generated idempotency key that the client can use to assert that the server side
   * session has not changed.
   *
   * @generated from field: string server_side_session_id = 4;
   */
  serverSideSessionId: string;

  /**
   * (Optional) The result key-value pairs.
   *
   * Available when the operation is 'Get', 'GetWithDefault', 'GetOption', 'GetAll'.
   * Also available for the operation 'IsModifiable' with boolean string "true" and "false".
   *
   * @generated from field: repeated spark.connect.KeyValue pairs = 2;
   */
  pairs: KeyValue[];

  /**
   * (Optional)
   *
   * Warning messages for deprecated or unsupported configurations.
   *
   * @generated from field: repeated string warnings = 3;
   */
  warnings: string[];
};

/**
 * Describes the message spark.connect.ConfigResponse.
 * Use `create(ConfigResponseSchema)` to create a new message.
 */
export const ConfigResponseSchema: GenMessage<ConfigResponse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 8);

/**
 * Request to transfer client-local artifacts.
 *
 * @generated from message spark.connect.AddArtifactsRequest
 */
export type AddArtifactsRequest = Message<"spark.connect.AddArtifactsRequest"> & {
  /**
   * (Required)
   *
   * The session_id specifies a spark session for a user id (which is specified
   * by user_context.user_id). The session_id is set by the client to be able to
   * collate streaming responses from different queries within the dedicated session.
   * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * User context
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * (Optional)
   *
   * Server-side generated idempotency key from the previous responses (if any). Server
   * can use this to validate that the server side session has not changed.
   *
   * @generated from field: optional string client_observed_server_side_session_id = 7;
   */
  clientObservedServerSideSessionId?: string;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 6;
   */
  clientType?: string;

  /**
   * The payload is either a batch of artifacts or a partial chunk of a large artifact.
   *
   * @generated from oneof spark.connect.AddArtifactsRequest.payload
   */
  payload: {
    /**
     * @generated from field: spark.connect.AddArtifactsRequest.Batch batch = 3;
     */
    value: AddArtifactsRequest_Batch;
    case: "batch";
  } | {
    /**
     * The metadata and the initial chunk of a large artifact chunked into multiple requests.
     * The server side is notified about the total size of the large artifact as well as the
     * number of chunks to expect.
     *
     * @generated from field: spark.connect.AddArtifactsRequest.BeginChunkedArtifact begin_chunk = 4;
     */
    value: AddArtifactsRequest_BeginChunkedArtifact;
    case: "beginChunk";
  } | {
    /**
     * A chunk of an artifact excluding metadata. This can be any chunk of a large artifact
     * excluding the first chunk (which is included in `BeginChunkedArtifact`).
     *
     * @generated from field: spark.connect.AddArtifactsRequest.ArtifactChunk chunk = 5;
     */
    value: AddArtifactsRequest_ArtifactChunk;
    case: "chunk";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.AddArtifactsRequest.
 * Use `create(AddArtifactsRequestSchema)` to create a new message.
 */
export const AddArtifactsRequestSchema: GenMessage<AddArtifactsRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 9);

/**
 * A chunk of an Artifact.
 *
 * @generated from message spark.connect.AddArtifactsRequest.ArtifactChunk
 */
export type AddArtifactsRequest_ArtifactChunk = Message<"spark.connect.AddArtifactsRequest.ArtifactChunk"> & {
  /**
   * Data chunk.
   *
   * @generated from field: bytes data = 1;
   */
  data: Uint8Array;

  /**
   * CRC to allow server to verify integrity of the chunk.
   *
   * @generated from field: int64 crc = 2;
   */
  crc: bigint;
};

/**
 * Describes the message spark.connect.AddArtifactsRequest.ArtifactChunk.
 * Use `create(AddArtifactsRequest_ArtifactChunkSchema)` to create a new message.
 */
export const AddArtifactsRequest_ArtifactChunkSchema: GenMessage<AddArtifactsRequest_ArtifactChunk> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 9, 0);

/**
 * An artifact that is contained in a single `ArtifactChunk`.
 * Generally, this message represents tiny artifacts such as REPL-generated class files.
 *
 * @generated from message spark.connect.AddArtifactsRequest.SingleChunkArtifact
 */
export type AddArtifactsRequest_SingleChunkArtifact = Message<"spark.connect.AddArtifactsRequest.SingleChunkArtifact"> & {
  /**
   * The name of the artifact is expected in the form of a "Relative Path" that is made up of a
   * sequence of directories and the final file element.
   * Examples of "Relative Path"s: "jars/test.jar", "classes/xyz.class", "abc.xyz", "a/b/X.jar".
   * The server is expected to maintain the hierarchy of files as defined by their name. (i.e
   * The relative path of the file on the server's filesystem will be the same as the name of
   * the provided artifact)
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * A single data chunk.
   *
   * @generated from field: spark.connect.AddArtifactsRequest.ArtifactChunk data = 2;
   */
  data?: AddArtifactsRequest_ArtifactChunk;
};

/**
 * Describes the message spark.connect.AddArtifactsRequest.SingleChunkArtifact.
 * Use `create(AddArtifactsRequest_SingleChunkArtifactSchema)` to create a new message.
 */
export const AddArtifactsRequest_SingleChunkArtifactSchema: GenMessage<AddArtifactsRequest_SingleChunkArtifact> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 9, 1);

/**
 * A number of `SingleChunkArtifact` batched into a single RPC.
 *
 * @generated from message spark.connect.AddArtifactsRequest.Batch
 */
export type AddArtifactsRequest_Batch = Message<"spark.connect.AddArtifactsRequest.Batch"> & {
  /**
   * @generated from field: repeated spark.connect.AddArtifactsRequest.SingleChunkArtifact artifacts = 1;
   */
  artifacts: AddArtifactsRequest_SingleChunkArtifact[];
};

/**
 * Describes the message spark.connect.AddArtifactsRequest.Batch.
 * Use `create(AddArtifactsRequest_BatchSchema)` to create a new message.
 */
export const AddArtifactsRequest_BatchSchema: GenMessage<AddArtifactsRequest_Batch> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 9, 2);

/**
 * Signals the beginning/start of a chunked artifact.
 * A large artifact is transferred through a payload of `BeginChunkedArtifact` followed by a
 * sequence of `ArtifactChunk`s.
 *
 * @generated from message spark.connect.AddArtifactsRequest.BeginChunkedArtifact
 */
export type AddArtifactsRequest_BeginChunkedArtifact = Message<"spark.connect.AddArtifactsRequest.BeginChunkedArtifact"> & {
  /**
   * Name of the artifact undergoing chunking. Follows the same conventions as the `name` in
   * the `Artifact` message.
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Total size of the artifact in bytes.
   *
   * @generated from field: int64 total_bytes = 2;
   */
  totalBytes: bigint;

  /**
   * Number of chunks the artifact is split into.
   * This includes the `initial_chunk`.
   *
   * @generated from field: int64 num_chunks = 3;
   */
  numChunks: bigint;

  /**
   * The first/initial chunk.
   *
   * @generated from field: spark.connect.AddArtifactsRequest.ArtifactChunk initial_chunk = 4;
   */
  initialChunk?: AddArtifactsRequest_ArtifactChunk;
};

/**
 * Describes the message spark.connect.AddArtifactsRequest.BeginChunkedArtifact.
 * Use `create(AddArtifactsRequest_BeginChunkedArtifactSchema)` to create a new message.
 */
export const AddArtifactsRequest_BeginChunkedArtifactSchema: GenMessage<AddArtifactsRequest_BeginChunkedArtifact> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 9, 3);

/**
 * Response to adding an artifact. Contains relevant metadata to verify successful transfer of
 * artifact(s).
 * Next ID: 4
 *
 * @generated from message spark.connect.AddArtifactsResponse
 */
export type AddArtifactsResponse = Message<"spark.connect.AddArtifactsResponse"> & {
  /**
   * Session id in which the AddArtifact was running.
   *
   * @generated from field: string session_id = 2;
   */
  sessionId: string;

  /**
   * Server-side generated idempotency key that the client can use to assert that the server side
   * session has not changed.
   *
   * @generated from field: string server_side_session_id = 3;
   */
  serverSideSessionId: string;

  /**
   * The list of artifact(s) seen by the server.
   *
   * @generated from field: repeated spark.connect.AddArtifactsResponse.ArtifactSummary artifacts = 1;
   */
  artifacts: AddArtifactsResponse_ArtifactSummary[];
};

/**
 * Describes the message spark.connect.AddArtifactsResponse.
 * Use `create(AddArtifactsResponseSchema)` to create a new message.
 */
export const AddArtifactsResponseSchema: GenMessage<AddArtifactsResponse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 10);

/**
 * Metadata of an artifact.
 *
 * @generated from message spark.connect.AddArtifactsResponse.ArtifactSummary
 */
export type AddArtifactsResponse_ArtifactSummary = Message<"spark.connect.AddArtifactsResponse.ArtifactSummary"> & {
  /**
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * Whether the CRC (Cyclic Redundancy Check) is successful on server verification.
   * The server discards any artifact that fails the CRC.
   * If false, the client may choose to resend the artifact specified by `name`.
   *
   * @generated from field: bool is_crc_successful = 2;
   */
  isCrcSuccessful: boolean;
};

/**
 * Describes the message spark.connect.AddArtifactsResponse.ArtifactSummary.
 * Use `create(AddArtifactsResponse_ArtifactSummarySchema)` to create a new message.
 */
export const AddArtifactsResponse_ArtifactSummarySchema: GenMessage<AddArtifactsResponse_ArtifactSummary> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 10, 0);

/**
 * Request to get current statuses of artifacts at the server side.
 *
 * @generated from message spark.connect.ArtifactStatusesRequest
 */
export type ArtifactStatusesRequest = Message<"spark.connect.ArtifactStatusesRequest"> & {
  /**
   * (Required)
   *
   * The session_id specifies a spark session for a user id (which is specified
   * by user_context.user_id). The session_id is set by the client to be able to
   * collate streaming responses from different queries within the dedicated session.
   * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * (Optional)
   *
   * Server-side generated idempotency key from the previous responses (if any). Server
   * can use this to validate that the server side session has not changed.
   *
   * @generated from field: optional string client_observed_server_side_session_id = 5;
   */
  clientObservedServerSideSessionId?: string;

  /**
   * User context
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 3;
   */
  clientType?: string;

  /**
   * The name of the artifact is expected in the form of a "Relative Path" that is made up of a
   * sequence of directories and the final file element.
   * Examples of "Relative Path"s: "jars/test.jar", "classes/xyz.class", "abc.xyz", "a/b/X.jar".
   * The server is expected to maintain the hierarchy of files as defined by their name. (i.e
   * The relative path of the file on the server's filesystem will be the same as the name of
   * the provided artifact)
   *
   * @generated from field: repeated string names = 4;
   */
  names: string[];
};

/**
 * Describes the message spark.connect.ArtifactStatusesRequest.
 * Use `create(ArtifactStatusesRequestSchema)` to create a new message.
 */
export const ArtifactStatusesRequestSchema: GenMessage<ArtifactStatusesRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 11);

/**
 * Response to checking artifact statuses.
 * Next ID: 4
 *
 * @generated from message spark.connect.ArtifactStatusesResponse
 */
export type ArtifactStatusesResponse = Message<"spark.connect.ArtifactStatusesResponse"> & {
  /**
   * Session id in which the ArtifactStatus was running.
   *
   * @generated from field: string session_id = 2;
   */
  sessionId: string;

  /**
   * Server-side generated idempotency key that the client can use to assert that the server side
   * session has not changed.
   *
   * @generated from field: string server_side_session_id = 3;
   */
  serverSideSessionId: string;

  /**
   * A map of artifact names to their statuses.
   *
   * @generated from field: map<string, spark.connect.ArtifactStatusesResponse.ArtifactStatus> statuses = 1;
   */
  statuses: { [key: string]: ArtifactStatusesResponse_ArtifactStatus };
};

/**
 * Describes the message spark.connect.ArtifactStatusesResponse.
 * Use `create(ArtifactStatusesResponseSchema)` to create a new message.
 */
export const ArtifactStatusesResponseSchema: GenMessage<ArtifactStatusesResponse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 12);

/**
 * @generated from message spark.connect.ArtifactStatusesResponse.ArtifactStatus
 */
export type ArtifactStatusesResponse_ArtifactStatus = Message<"spark.connect.ArtifactStatusesResponse.ArtifactStatus"> & {
  /**
   * Exists or not particular artifact at the server.
   *
   * @generated from field: bool exists = 1;
   */
  exists: boolean;
};

/**
 * Describes the message spark.connect.ArtifactStatusesResponse.ArtifactStatus.
 * Use `create(ArtifactStatusesResponse_ArtifactStatusSchema)` to create a new message.
 */
export const ArtifactStatusesResponse_ArtifactStatusSchema: GenMessage<ArtifactStatusesResponse_ArtifactStatus> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 12, 0);

/**
 * @generated from message spark.connect.InterruptRequest
 */
export type InterruptRequest = Message<"spark.connect.InterruptRequest"> & {
  /**
   * (Required)
   *
   * The session_id specifies a spark session for a user id (which is specified
   * by user_context.user_id). The session_id is set by the client to be able to
   * collate streaming responses from different queries within the dedicated session.
   * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * (Optional)
   *
   * Server-side generated idempotency key from the previous responses (if any). Server
   * can use this to validate that the server side session has not changed.
   *
   * @generated from field: optional string client_observed_server_side_session_id = 7;
   */
  clientObservedServerSideSessionId?: string;

  /**
   * (Required) User context
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 3;
   */
  clientType?: string;

  /**
   * (Required) The type of interrupt to execute.
   *
   * @generated from field: spark.connect.InterruptRequest.InterruptType interrupt_type = 4;
   */
  interruptType: InterruptRequest_InterruptType;

  /**
   * @generated from oneof spark.connect.InterruptRequest.interrupt
   */
  interrupt: {
    /**
     * if interrupt_tag == INTERRUPT_TYPE_TAG, interrupt operation with this tag.
     *
     * @generated from field: string operation_tag = 5;
     */
    value: string;
    case: "operationTag";
  } | {
    /**
     * if interrupt_tag == INTERRUPT_TYPE_OPERATION_ID, interrupt operation with this operation_id.
     *
     * @generated from field: string operation_id = 6;
     */
    value: string;
    case: "operationId";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.InterruptRequest.
 * Use `create(InterruptRequestSchema)` to create a new message.
 */
export const InterruptRequestSchema: GenMessage<InterruptRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 13);

/**
 * @generated from enum spark.connect.InterruptRequest.InterruptType
 */
export enum InterruptRequest_InterruptType {
  /**
   * @generated from enum value: INTERRUPT_TYPE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * Interrupt all running executions within the session with the provided session_id.
   *
   * @generated from enum value: INTERRUPT_TYPE_ALL = 1;
   */
  ALL = 1,

  /**
   * Interrupt all running executions within the session with the provided operation_tag.
   *
   * @generated from enum value: INTERRUPT_TYPE_TAG = 2;
   */
  TAG = 2,

  /**
   * Interrupt the running execution within the session with the provided operation_id.
   *
   * @generated from enum value: INTERRUPT_TYPE_OPERATION_ID = 3;
   */
  OPERATION_ID = 3,
}

/**
 * Describes the enum spark.connect.InterruptRequest.InterruptType.
 */
export const InterruptRequest_InterruptTypeSchema: GenEnum<InterruptRequest_InterruptType> = /*@__PURE__*/
  enumDesc(file_spark_connect_base, 13, 0);

/**
 * Next ID: 4
 *
 * @generated from message spark.connect.InterruptResponse
 */
export type InterruptResponse = Message<"spark.connect.InterruptResponse"> & {
  /**
   * Session id in which the interrupt was running.
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * Server-side generated idempotency key that the client can use to assert that the server side
   * session has not changed.
   *
   * @generated from field: string server_side_session_id = 3;
   */
  serverSideSessionId: string;

  /**
   * Operation ids of the executions which were interrupted.
   *
   * @generated from field: repeated string interrupted_ids = 2;
   */
  interruptedIds: string[];
};

/**
 * Describes the message spark.connect.InterruptResponse.
 * Use `create(InterruptResponseSchema)` to create a new message.
 */
export const InterruptResponseSchema: GenMessage<InterruptResponse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 14);

/**
 * @generated from message spark.connect.ReattachOptions
 */
export type ReattachOptions = Message<"spark.connect.ReattachOptions"> & {
  /**
   * If true, the request can be reattached to using ReattachExecute.
   * ReattachExecute can be used either if the stream broke with a GRPC network error,
   * or if the server closed the stream without sending a response with StreamStatus.complete=true.
   * The server will keep a buffer of responses in case a response is lost, and
   * ReattachExecute needs to back-track.
   *
   * If false, the execution response stream will will not be reattachable, and all responses are
   * immediately released by the server after being sent.
   *
   * @generated from field: bool reattachable = 1;
   */
  reattachable: boolean;
};

/**
 * Describes the message spark.connect.ReattachOptions.
 * Use `create(ReattachOptionsSchema)` to create a new message.
 */
export const ReattachOptionsSchema: GenMessage<ReattachOptions> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 15);

/**
 * @generated from message spark.connect.ReattachExecuteRequest
 */
export type ReattachExecuteRequest = Message<"spark.connect.ReattachExecuteRequest"> & {
  /**
   * (Required)
   *
   * The session_id of the request to reattach to.
   * This must be an id of existing session.
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * (Optional)
   *
   * Server-side generated idempotency key from the previous responses (if any). Server
   * can use this to validate that the server side session has not changed.
   *
   * @generated from field: optional string client_observed_server_side_session_id = 6;
   */
  clientObservedServerSideSessionId?: string;

  /**
   * (Required) User context
   *
   * user_context.user_id and session+id both identify a unique remote spark session on the
   * server side.
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * (Required)
   * Provide an id of the request to reattach to.
   * This must be an id of existing operation.
   *
   * @generated from field: string operation_id = 3;
   */
  operationId: string;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 4;
   */
  clientType?: string;

  /**
   * (Optional)
   * Last already processed response id from the response stream.
   * After reattach, server will resume the response stream after that response.
   * If not specified, server will restart the stream from the start.
   *
   * Note: server controls the amount of responses that it buffers and it may drop responses,
   * that are far behind the latest returned response, so this can't be used to arbitrarily
   * scroll back the cursor. If the response is no longer available, this will result in an error.
   *
   * @generated from field: optional string last_response_id = 5;
   */
  lastResponseId?: string;
};

/**
 * Describes the message spark.connect.ReattachExecuteRequest.
 * Use `create(ReattachExecuteRequestSchema)` to create a new message.
 */
export const ReattachExecuteRequestSchema: GenMessage<ReattachExecuteRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 16);

/**
 * @generated from message spark.connect.ReleaseExecuteRequest
 */
export type ReleaseExecuteRequest = Message<"spark.connect.ReleaseExecuteRequest"> & {
  /**
   * (Required)
   *
   * The session_id of the request to reattach to.
   * This must be an id of existing session.
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * (Optional)
   *
   * Server-side generated idempotency key from the previous responses (if any). Server
   * can use this to validate that the server side session has not changed.
   *
   * @generated from field: optional string client_observed_server_side_session_id = 7;
   */
  clientObservedServerSideSessionId?: string;

  /**
   * (Required) User context
   *
   * user_context.user_id and session+id both identify a unique remote spark session on the
   * server side.
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * (Required)
   * Provide an id of the request to reattach to.
   * This must be an id of existing operation.
   *
   * @generated from field: string operation_id = 3;
   */
  operationId: string;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 4;
   */
  clientType?: string;

  /**
   * @generated from oneof spark.connect.ReleaseExecuteRequest.release
   */
  release: {
    /**
     * @generated from field: spark.connect.ReleaseExecuteRequest.ReleaseAll release_all = 5;
     */
    value: ReleaseExecuteRequest_ReleaseAll;
    case: "releaseAll";
  } | {
    /**
     * @generated from field: spark.connect.ReleaseExecuteRequest.ReleaseUntil release_until = 6;
     */
    value: ReleaseExecuteRequest_ReleaseUntil;
    case: "releaseUntil";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.ReleaseExecuteRequest.
 * Use `create(ReleaseExecuteRequestSchema)` to create a new message.
 */
export const ReleaseExecuteRequestSchema: GenMessage<ReleaseExecuteRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 17);

/**
 * Release and close operation completely.
 * This will also interrupt the query if it is running execution, and wait for it to be torn down.
 *
 * @generated from message spark.connect.ReleaseExecuteRequest.ReleaseAll
 */
export type ReleaseExecuteRequest_ReleaseAll = Message<"spark.connect.ReleaseExecuteRequest.ReleaseAll"> & {
};

/**
 * Describes the message spark.connect.ReleaseExecuteRequest.ReleaseAll.
 * Use `create(ReleaseExecuteRequest_ReleaseAllSchema)` to create a new message.
 */
export const ReleaseExecuteRequest_ReleaseAllSchema: GenMessage<ReleaseExecuteRequest_ReleaseAll> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 17, 0);

/**
 * Release all responses from the operation response stream up to and including
 * the response with the given by response_id.
 * While server determines by itself how much of a buffer of responses to keep, client providing
 * explicit release calls will help reduce resource consumption.
 * Noop if response_id not found in cached responses.
 *
 * @generated from message spark.connect.ReleaseExecuteRequest.ReleaseUntil
 */
export type ReleaseExecuteRequest_ReleaseUntil = Message<"spark.connect.ReleaseExecuteRequest.ReleaseUntil"> & {
  /**
   * @generated from field: string response_id = 1;
   */
  responseId: string;
};

/**
 * Describes the message spark.connect.ReleaseExecuteRequest.ReleaseUntil.
 * Use `create(ReleaseExecuteRequest_ReleaseUntilSchema)` to create a new message.
 */
export const ReleaseExecuteRequest_ReleaseUntilSchema: GenMessage<ReleaseExecuteRequest_ReleaseUntil> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 17, 1);

/**
 * Next ID: 4
 *
 * @generated from message spark.connect.ReleaseExecuteResponse
 */
export type ReleaseExecuteResponse = Message<"spark.connect.ReleaseExecuteResponse"> & {
  /**
   * Session id in which the release was running.
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * Server-side generated idempotency key that the client can use to assert that the server side
   * session has not changed.
   *
   * @generated from field: string server_side_session_id = 3;
   */
  serverSideSessionId: string;

  /**
   * Operation id of the operation on which the release executed.
   * If the operation couldn't be found (because e.g. it was concurrently released), will be unset.
   * Otherwise, it will be equal to the operation_id from request.
   *
   * @generated from field: optional string operation_id = 2;
   */
  operationId?: string;
};

/**
 * Describes the message spark.connect.ReleaseExecuteResponse.
 * Use `create(ReleaseExecuteResponseSchema)` to create a new message.
 */
export const ReleaseExecuteResponseSchema: GenMessage<ReleaseExecuteResponse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 18);

/**
 * @generated from message spark.connect.ReleaseSessionRequest
 */
export type ReleaseSessionRequest = Message<"spark.connect.ReleaseSessionRequest"> & {
  /**
   * (Required)
   *
   * The session_id of the request to reattach to.
   * This must be an id of existing session.
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * (Required) User context
   *
   * user_context.user_id and session+id both identify a unique remote spark session on the
   * server side.
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 3;
   */
  clientType?: string;
};

/**
 * Describes the message spark.connect.ReleaseSessionRequest.
 * Use `create(ReleaseSessionRequestSchema)` to create a new message.
 */
export const ReleaseSessionRequestSchema: GenMessage<ReleaseSessionRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 19);

/**
 * Next ID: 3
 *
 * @generated from message spark.connect.ReleaseSessionResponse
 */
export type ReleaseSessionResponse = Message<"spark.connect.ReleaseSessionResponse"> & {
  /**
   * Session id of the session on which the release executed.
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * Server-side generated idempotency key that the client can use to assert that the server side
   * session has not changed.
   *
   * @generated from field: string server_side_session_id = 2;
   */
  serverSideSessionId: string;
};

/**
 * Describes the message spark.connect.ReleaseSessionResponse.
 * Use `create(ReleaseSessionResponseSchema)` to create a new message.
 */
export const ReleaseSessionResponseSchema: GenMessage<ReleaseSessionResponse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 20);

/**
 * @generated from message spark.connect.FetchErrorDetailsRequest
 */
export type FetchErrorDetailsRequest = Message<"spark.connect.FetchErrorDetailsRequest"> & {
  /**
   * (Required)
   * The session_id specifies a Spark session for a user identified by user_context.user_id.
   * The id should be a UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`.
   *
   * @generated from field: string session_id = 1;
   */
  sessionId: string;

  /**
   * (Optional)
   *
   * Server-side generated idempotency key from the previous responses (if any). Server
   * can use this to validate that the server side session has not changed.
   *
   * @generated from field: optional string client_observed_server_side_session_id = 5;
   */
  clientObservedServerSideSessionId?: string;

  /**
   * User context
   *
   * @generated from field: spark.connect.UserContext user_context = 2;
   */
  userContext?: UserContext;

  /**
   * (Required)
   * The id of the error.
   *
   * @generated from field: string error_id = 3;
   */
  errorId: string;

  /**
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   *
   * @generated from field: optional string client_type = 4;
   */
  clientType?: string;
};

/**
 * Describes the message spark.connect.FetchErrorDetailsRequest.
 * Use `create(FetchErrorDetailsRequestSchema)` to create a new message.
 */
export const FetchErrorDetailsRequestSchema: GenMessage<FetchErrorDetailsRequest> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 21);

/**
 * Next ID: 5
 *
 * @generated from message spark.connect.FetchErrorDetailsResponse
 */
export type FetchErrorDetailsResponse = Message<"spark.connect.FetchErrorDetailsResponse"> & {
  /**
   * Server-side generated idempotency key that the client can use to assert that the server side
   * session has not changed.
   *
   * @generated from field: string server_side_session_id = 3;
   */
  serverSideSessionId: string;

  /**
   * @generated from field: string session_id = 4;
   */
  sessionId: string;

  /**
   * The index of the root error in errors. The field will not be set if the error is not found.
   *
   * @generated from field: optional int32 root_error_idx = 1;
   */
  rootErrorIdx?: number;

  /**
   * A list of errors.
   *
   * @generated from field: repeated spark.connect.FetchErrorDetailsResponse.Error errors = 2;
   */
  errors: FetchErrorDetailsResponse_Error[];
};

/**
 * Describes the message spark.connect.FetchErrorDetailsResponse.
 * Use `create(FetchErrorDetailsResponseSchema)` to create a new message.
 */
export const FetchErrorDetailsResponseSchema: GenMessage<FetchErrorDetailsResponse> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 22);

/**
 * @generated from message spark.connect.FetchErrorDetailsResponse.StackTraceElement
 */
export type FetchErrorDetailsResponse_StackTraceElement = Message<"spark.connect.FetchErrorDetailsResponse.StackTraceElement"> & {
  /**
   * The fully qualified name of the class containing the execution point.
   *
   * @generated from field: string declaring_class = 1;
   */
  declaringClass: string;

  /**
   * The name of the method containing the execution point.
   *
   * @generated from field: string method_name = 2;
   */
  methodName: string;

  /**
   * The name of the file containing the execution point.
   *
   * @generated from field: optional string file_name = 3;
   */
  fileName?: string;

  /**
   * The line number of the source line containing the execution point.
   *
   * @generated from field: int32 line_number = 4;
   */
  lineNumber: number;
};

/**
 * Describes the message spark.connect.FetchErrorDetailsResponse.StackTraceElement.
 * Use `create(FetchErrorDetailsResponse_StackTraceElementSchema)` to create a new message.
 */
export const FetchErrorDetailsResponse_StackTraceElementSchema: GenMessage<FetchErrorDetailsResponse_StackTraceElement> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 22, 0);

/**
 * QueryContext defines the schema for the query context of a SparkThrowable.
 * It helps users understand where the error occurs while executing queries.
 *
 * @generated from message spark.connect.FetchErrorDetailsResponse.QueryContext
 */
export type FetchErrorDetailsResponse_QueryContext = Message<"spark.connect.FetchErrorDetailsResponse.QueryContext"> & {
  /**
   * @generated from field: spark.connect.FetchErrorDetailsResponse.QueryContext.ContextType context_type = 10;
   */
  contextType: FetchErrorDetailsResponse_QueryContext_ContextType;

  /**
   * The object type of the query which throws the exception.
   * If the exception is directly from the main query, it should be an empty string.
   * Otherwise, it should be the exact object type in upper case. For example, a "VIEW".
   *
   * @generated from field: string object_type = 1;
   */
  objectType: string;

  /**
   * The object name of the query which throws the exception.
   * If the exception is directly from the main query, it should be an empty string.
   * Otherwise, it should be the object name. For example, a view name "V1".
   *
   * @generated from field: string object_name = 2;
   */
  objectName: string;

  /**
   * The starting index in the query text which throws the exception. The index starts from 0.
   *
   * @generated from field: int32 start_index = 3;
   */
  startIndex: number;

  /**
   * The stopping index in the query which throws the exception. The index starts from 0.
   *
   * @generated from field: int32 stop_index = 4;
   */
  stopIndex: number;

  /**
   * The corresponding fragment of the query which throws the exception.
   *
   * @generated from field: string fragment = 5;
   */
  fragment: string;

  /**
   * The user code (call site of the API) that caused throwing the exception.
   *
   * @generated from field: string call_site = 6;
   */
  callSite: string;

  /**
   * Summary of the exception cause.
   *
   * @generated from field: string summary = 7;
   */
  summary: string;
};

/**
 * Describes the message spark.connect.FetchErrorDetailsResponse.QueryContext.
 * Use `create(FetchErrorDetailsResponse_QueryContextSchema)` to create a new message.
 */
export const FetchErrorDetailsResponse_QueryContextSchema: GenMessage<FetchErrorDetailsResponse_QueryContext> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 22, 1);

/**
 * The type of this query context.
 *
 * @generated from enum spark.connect.FetchErrorDetailsResponse.QueryContext.ContextType
 */
export enum FetchErrorDetailsResponse_QueryContext_ContextType {
  /**
   * @generated from enum value: SQL = 0;
   */
  SQL = 0,

  /**
   * @generated from enum value: DATAFRAME = 1;
   */
  DATAFRAME = 1,
}

/**
 * Describes the enum spark.connect.FetchErrorDetailsResponse.QueryContext.ContextType.
 */
export const FetchErrorDetailsResponse_QueryContext_ContextTypeSchema: GenEnum<FetchErrorDetailsResponse_QueryContext_ContextType> = /*@__PURE__*/
  enumDesc(file_spark_connect_base, 22, 1, 0);

/**
 * SparkThrowable defines the schema for SparkThrowable exceptions.
 *
 * @generated from message spark.connect.FetchErrorDetailsResponse.SparkThrowable
 */
export type FetchErrorDetailsResponse_SparkThrowable = Message<"spark.connect.FetchErrorDetailsResponse.SparkThrowable"> & {
  /**
   * Succinct, human-readable, unique, and consistent representation of the error category.
   *
   * @generated from field: optional string error_class = 1;
   */
  errorClass?: string;

  /**
   * The message parameters for the error framework.
   *
   * @generated from field: map<string, string> message_parameters = 2;
   */
  messageParameters: { [key: string]: string };

  /**
   * The query context of a SparkThrowable.
   *
   * @generated from field: repeated spark.connect.FetchErrorDetailsResponse.QueryContext query_contexts = 3;
   */
  queryContexts: FetchErrorDetailsResponse_QueryContext[];

  /**
   * Portable error identifier across SQL engines
   * If null, error class or SQLSTATE is not set.
   *
   * @generated from field: optional string sql_state = 4;
   */
  sqlState?: string;
};

/**
 * Describes the message spark.connect.FetchErrorDetailsResponse.SparkThrowable.
 * Use `create(FetchErrorDetailsResponse_SparkThrowableSchema)` to create a new message.
 */
export const FetchErrorDetailsResponse_SparkThrowableSchema: GenMessage<FetchErrorDetailsResponse_SparkThrowable> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 22, 2);

/**
 * Error defines the schema for the representing exception.
 *
 * @generated from message spark.connect.FetchErrorDetailsResponse.Error
 */
export type FetchErrorDetailsResponse_Error = Message<"spark.connect.FetchErrorDetailsResponse.Error"> & {
  /**
   * The fully qualified names of the exception class and its parent classes.
   *
   * @generated from field: repeated string error_type_hierarchy = 1;
   */
  errorTypeHierarchy: string[];

  /**
   * The detailed message of the exception.
   *
   * @generated from field: string message = 2;
   */
  message: string;

  /**
   * The stackTrace of the exception. It will be set
   * if the SQLConf spark.sql.connect.serverStacktrace.enabled is true.
   *
   * @generated from field: repeated spark.connect.FetchErrorDetailsResponse.StackTraceElement stack_trace = 3;
   */
  stackTrace: FetchErrorDetailsResponse_StackTraceElement[];

  /**
   * The index of the cause error in errors.
   *
   * @generated from field: optional int32 cause_idx = 4;
   */
  causeIdx?: number;

  /**
   * The structured data of a SparkThrowable exception.
   *
   * @generated from field: optional spark.connect.FetchErrorDetailsResponse.SparkThrowable spark_throwable = 5;
   */
  sparkThrowable?: FetchErrorDetailsResponse_SparkThrowable;
};

/**
 * Describes the message spark.connect.FetchErrorDetailsResponse.Error.
 * Use `create(FetchErrorDetailsResponse_ErrorSchema)` to create a new message.
 */
export const FetchErrorDetailsResponse_ErrorSchema: GenMessage<FetchErrorDetailsResponse_Error> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 22, 3);

/**
 * @generated from message spark.connect.CheckpointCommandResult
 */
export type CheckpointCommandResult = Message<"spark.connect.CheckpointCommandResult"> & {
  /**
   * (Required) The logical plan checkpointed.
   *
   * @generated from field: spark.connect.CachedRemoteRelation relation = 1;
   */
  relation?: CachedRemoteRelation;
};

/**
 * Describes the message spark.connect.CheckpointCommandResult.
 * Use `create(CheckpointCommandResultSchema)` to create a new message.
 */
export const CheckpointCommandResultSchema: GenMessage<CheckpointCommandResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_base, 23);

/**
 * Main interface for the SparkConnect service.
 *
 * @generated from service spark.connect.SparkConnectService
 */
export const SparkConnectService: GenService<{
  /**
   * Executes a request that contains the query and returns a stream of [[Response]].
   *
   * It is guaranteed that there is at least one ARROW batch returned even if the result set is empty.
   *
   * @generated from rpc spark.connect.SparkConnectService.ExecutePlan
   */
  executePlan: {
    methodKind: "server_streaming";
    input: typeof ExecutePlanRequestSchema;
    output: typeof ExecutePlanResponseSchema;
  },
  /**
   * Analyzes a query and returns a [[AnalyzeResponse]] containing metadata about the query.
   *
   * @generated from rpc spark.connect.SparkConnectService.AnalyzePlan
   */
  analyzePlan: {
    methodKind: "unary";
    input: typeof AnalyzePlanRequestSchema;
    output: typeof AnalyzePlanResponseSchema;
  },
  /**
   * Update or fetch the configurations and returns a [[ConfigResponse]] containing the result.
   *
   * @generated from rpc spark.connect.SparkConnectService.Config
   */
  config: {
    methodKind: "unary";
    input: typeof ConfigRequestSchema;
    output: typeof ConfigResponseSchema;
  },
  /**
   * Add artifacts to the session and returns a [[AddArtifactsResponse]] containing metadata about
   * the added artifacts.
   *
   * @generated from rpc spark.connect.SparkConnectService.AddArtifacts
   */
  addArtifacts: {
    methodKind: "client_streaming";
    input: typeof AddArtifactsRequestSchema;
    output: typeof AddArtifactsResponseSchema;
  },
  /**
   * Check statuses of artifacts in the session and returns them in a [[ArtifactStatusesResponse]]
   *
   * @generated from rpc spark.connect.SparkConnectService.ArtifactStatus
   */
  artifactStatus: {
    methodKind: "unary";
    input: typeof ArtifactStatusesRequestSchema;
    output: typeof ArtifactStatusesResponseSchema;
  },
  /**
   * Interrupts running executions
   *
   * @generated from rpc spark.connect.SparkConnectService.Interrupt
   */
  interrupt: {
    methodKind: "unary";
    input: typeof InterruptRequestSchema;
    output: typeof InterruptResponseSchema;
  },
  /**
   * Reattach to an existing reattachable execution.
   * The ExecutePlan must have been started with ReattachOptions.reattachable=true.
   * If the ExecutePlanResponse stream ends without a ResultComplete message, there is more to
   * continue. If there is a ResultComplete, the client should use ReleaseExecute with
   *
   * @generated from rpc spark.connect.SparkConnectService.ReattachExecute
   */
  reattachExecute: {
    methodKind: "server_streaming";
    input: typeof ReattachExecuteRequestSchema;
    output: typeof ExecutePlanResponseSchema;
  },
  /**
   * Release an reattachable execution, or parts thereof.
   * The ExecutePlan must have been started with ReattachOptions.reattachable=true.
   * Non reattachable executions are released automatically and immediately after the ExecutePlan
   * RPC and ReleaseExecute may not be used.
   *
   * @generated from rpc spark.connect.SparkConnectService.ReleaseExecute
   */
  releaseExecute: {
    methodKind: "unary";
    input: typeof ReleaseExecuteRequestSchema;
    output: typeof ReleaseExecuteResponseSchema;
  },
  /**
   * Release a session.
   * All the executions in the session will be released. Any further requests for the session with
   * that session_id for the given user_id will fail. If the session didn't exist or was already
   * released, this is a noop.
   *
   * @generated from rpc spark.connect.SparkConnectService.ReleaseSession
   */
  releaseSession: {
    methodKind: "unary";
    input: typeof ReleaseSessionRequestSchema;
    output: typeof ReleaseSessionResponseSchema;
  },
  /**
   * FetchErrorDetails retrieves the matched exception with details based on a provided error id.
   *
   * @generated from rpc spark.connect.SparkConnectService.FetchErrorDetails
   */
  fetchErrorDetails: {
    methodKind: "unary";
    input: typeof FetchErrorDetailsRequestSchema;
    output: typeof FetchErrorDetailsResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_spark_connect_base, 0);

