//
// Licensed to the Apache Software Foundation (ASF) under one or more
// contributor license agreements.  See the NOTICE file distributed with
// this work for additional information regarding copyright ownership.
// The ASF licenses this file to You under the Apache License, Version 2.0
// (the "License"); you may not use this file except in compliance with
// the License.  You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// @generated by protoc-gen-es v2.2.3 with parameter "target=ts,import_extension=none,js_import_style=module"
// @generated from file spark/connect/commands.proto (package spark.connect, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { enumDesc, fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import type { Any } from "@bufbuild/protobuf/wkt";
import { file_google_protobuf_any } from "@bufbuild/protobuf/wkt";
import type { ResourceInformation, ResourceProfile, StorageLevel } from "./common_pb";
import { file_spark_connect_common } from "./common_pb";
import type { CommonInlineUserDefinedFunction, Expression, Expression_Literal, PythonUDF, ScalarScalaUDF } from "./expressions_pb";
import { file_spark_connect_expressions } from "./expressions_pb";
import type { CachedRemoteRelation, CommonInlineUserDefinedDataSource, CommonInlineUserDefinedTableFunction, Relation } from "./relations_pb";
import { file_spark_connect_relations } from "./relations_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file spark/connect/commands.proto.
 */
export const file_spark_connect_commands: GenFile = /*@__PURE__*/
  fileDesc("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", [file_google_protobuf_any, file_spark_connect_common, file_spark_connect_expressions, file_spark_connect_relations]);

/**
 * A [[Command]] is an operation that is executed by the server that does not directly consume or
 * produce a relational result.
 *
 * @generated from message spark.connect.Command
 */
export type Command = Message<"spark.connect.Command"> & {
  /**
   * @generated from oneof spark.connect.Command.command_type
   */
  commandType: {
    /**
     * @generated from field: spark.connect.CommonInlineUserDefinedFunction register_function = 1;
     */
    value: CommonInlineUserDefinedFunction;
    case: "registerFunction";
  } | {
    /**
     * @generated from field: spark.connect.WriteOperation write_operation = 2;
     */
    value: WriteOperation;
    case: "writeOperation";
  } | {
    /**
     * @generated from field: spark.connect.CreateDataFrameViewCommand create_dataframe_view = 3;
     */
    value: CreateDataFrameViewCommand;
    case: "createDataframeView";
  } | {
    /**
     * @generated from field: spark.connect.WriteOperationV2 write_operation_v2 = 4;
     */
    value: WriteOperationV2;
    case: "writeOperationV2";
  } | {
    /**
     * @generated from field: spark.connect.SqlCommand sql_command = 5;
     */
    value: SqlCommand;
    case: "sqlCommand";
  } | {
    /**
     * @generated from field: spark.connect.WriteStreamOperationStart write_stream_operation_start = 6;
     */
    value: WriteStreamOperationStart;
    case: "writeStreamOperationStart";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryCommand streaming_query_command = 7;
     */
    value: StreamingQueryCommand;
    case: "streamingQueryCommand";
  } | {
    /**
     * @generated from field: spark.connect.GetResourcesCommand get_resources_command = 8;
     */
    value: GetResourcesCommand;
    case: "getResourcesCommand";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryManagerCommand streaming_query_manager_command = 9;
     */
    value: StreamingQueryManagerCommand;
    case: "streamingQueryManagerCommand";
  } | {
    /**
     * @generated from field: spark.connect.CommonInlineUserDefinedTableFunction register_table_function = 10;
     */
    value: CommonInlineUserDefinedTableFunction;
    case: "registerTableFunction";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryListenerBusCommand streaming_query_listener_bus_command = 11;
     */
    value: StreamingQueryListenerBusCommand;
    case: "streamingQueryListenerBusCommand";
  } | {
    /**
     * @generated from field: spark.connect.CommonInlineUserDefinedDataSource register_data_source = 12;
     */
    value: CommonInlineUserDefinedDataSource;
    case: "registerDataSource";
  } | {
    /**
     * @generated from field: spark.connect.CreateResourceProfileCommand create_resource_profile_command = 13;
     */
    value: CreateResourceProfileCommand;
    case: "createResourceProfileCommand";
  } | {
    /**
     * @generated from field: spark.connect.CheckpointCommand checkpoint_command = 14;
     */
    value: CheckpointCommand;
    case: "checkpointCommand";
  } | {
    /**
     * @generated from field: spark.connect.RemoveCachedRemoteRelationCommand remove_cached_remote_relation_command = 15;
     */
    value: RemoveCachedRemoteRelationCommand;
    case: "removeCachedRemoteRelationCommand";
  } | {
    /**
     * @generated from field: spark.connect.MergeIntoTableCommand merge_into_table_command = 16;
     */
    value: MergeIntoTableCommand;
    case: "mergeIntoTableCommand";
  } | {
    /**
     * This field is used to mark extensions to the protocol. When plugins generate arbitrary
     * Commands they can add them here. During the planning the correct resolution is done.
     *
     * @generated from field: google.protobuf.Any extension = 999;
     */
    value: Any;
    case: "extension";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.Command.
 * Use `create(CommandSchema)` to create a new message.
 */
export const CommandSchema: GenMessage<Command> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 0);

/**
 * A SQL Command is used to trigger the eager evaluation of SQL commands in Spark.
 *
 * When the SQL provide as part of the message is a command it will be immediately evaluated
 * and the result will be collected and returned as part of a LocalRelation. If the result is
 * not a command, the operation will simply return a SQL Relation. This allows the client to be
 * almost oblivious to the server-side behavior.
 *
 * @generated from message spark.connect.SqlCommand
 */
export type SqlCommand = Message<"spark.connect.SqlCommand"> & {
  /**
   * (Required) SQL Query.
   *
   * @generated from field: string sql = 1 [deprecated = true];
   * @deprecated
   */
  sql: string;

  /**
   * (Optional) A map of parameter names to literal expressions.
   *
   * @generated from field: map<string, spark.connect.Expression.Literal> args = 2 [deprecated = true];
   * @deprecated
   */
  args: { [key: string]: Expression_Literal };

  /**
   * (Optional) A sequence of literal expressions for positional parameters in the SQL query text.
   *
   * @generated from field: repeated spark.connect.Expression.Literal pos_args = 3 [deprecated = true];
   * @deprecated
   */
  posArgs: Expression_Literal[];

  /**
   * (Optional) A map of parameter names to expressions.
   * It cannot coexist with `pos_arguments`.
   *
   * @generated from field: map<string, spark.connect.Expression> named_arguments = 4 [deprecated = true];
   * @deprecated
   */
  namedArguments: { [key: string]: Expression };

  /**
   * (Optional) A sequence of expressions for positional parameters in the SQL query text.
   * It cannot coexist with `named_arguments`.
   *
   * @generated from field: repeated spark.connect.Expression pos_arguments = 5 [deprecated = true];
   * @deprecated
   */
  posArguments: Expression[];

  /**
   * (Optional) The relation that this SQL command will be built on.
   *
   * @generated from field: spark.connect.Relation input = 6;
   */
  input?: Relation;
};

/**
 * Describes the message spark.connect.SqlCommand.
 * Use `create(SqlCommandSchema)` to create a new message.
 */
export const SqlCommandSchema: GenMessage<SqlCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 1);

/**
 * A command that can create DataFrame global temp view or local temp view.
 *
 * @generated from message spark.connect.CreateDataFrameViewCommand
 */
export type CreateDataFrameViewCommand = Message<"spark.connect.CreateDataFrameViewCommand"> & {
  /**
   * (Required) The relation that this view will be built on.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) View name.
   *
   * @generated from field: string name = 2;
   */
  name: string;

  /**
   * (Required) Whether this is global temp view or local temp view.
   *
   * @generated from field: bool is_global = 3;
   */
  isGlobal: boolean;

  /**
   * (Required)
   *
   * If true, and if the view already exists, updates it; if false, and if the view
   * already exists, throws exception.
   *
   * @generated from field: bool replace = 4;
   */
  replace: boolean;
};

/**
 * Describes the message spark.connect.CreateDataFrameViewCommand.
 * Use `create(CreateDataFrameViewCommandSchema)` to create a new message.
 */
export const CreateDataFrameViewCommandSchema: GenMessage<CreateDataFrameViewCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 2);

/**
 * As writes are not directly handled during analysis and planning, they are modeled as commands.
 *
 * @generated from message spark.connect.WriteOperation
 */
export type WriteOperation = Message<"spark.connect.WriteOperation"> & {
  /**
   * (Required) The output of the `input` relation will be persisted according to the options.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Optional) Format value according to the Spark documentation. Examples are: text, parquet, delta.
   *
   * @generated from field: optional string source = 2;
   */
  source?: string;

  /**
   * (Optional)
   *
   * The destination of the write operation can be either a path or a table.
   * If the destination is neither a path nor a table, such as jdbc and noop,
   * the `save_type` should not be set.
   *
   * @generated from oneof spark.connect.WriteOperation.save_type
   */
  saveType: {
    /**
     * @generated from field: string path = 3;
     */
    value: string;
    case: "path";
  } | {
    /**
     * @generated from field: spark.connect.WriteOperation.SaveTable table = 4;
     */
    value: WriteOperation_SaveTable;
    case: "table";
  } | { case: undefined; value?: undefined };

  /**
   * (Required) the save mode.
   *
   * @generated from field: spark.connect.WriteOperation.SaveMode mode = 5;
   */
  mode: WriteOperation_SaveMode;

  /**
   * (Optional) List of columns to sort the output by.
   *
   * @generated from field: repeated string sort_column_names = 6;
   */
  sortColumnNames: string[];

  /**
   * (Optional) List of columns for partitioning.
   *
   * @generated from field: repeated string partitioning_columns = 7;
   */
  partitioningColumns: string[];

  /**
   * (Optional) Bucketing specification. Bucketing must set the number of buckets and the columns
   * to bucket by.
   *
   * @generated from field: spark.connect.WriteOperation.BucketBy bucket_by = 8;
   */
  bucketBy?: WriteOperation_BucketBy;

  /**
   * (Optional) A list of configuration options.
   *
   * @generated from field: map<string, string> options = 9;
   */
  options: { [key: string]: string };

  /**
   * (Optional) Columns used for clustering the table.
   *
   * @generated from field: repeated string clustering_columns = 10;
   */
  clusteringColumns: string[];
};

/**
 * Describes the message spark.connect.WriteOperation.
 * Use `create(WriteOperationSchema)` to create a new message.
 */
export const WriteOperationSchema: GenMessage<WriteOperation> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 3);

/**
 * @generated from message spark.connect.WriteOperation.SaveTable
 */
export type WriteOperation_SaveTable = Message<"spark.connect.WriteOperation.SaveTable"> & {
  /**
   * (Required) The table name.
   *
   * @generated from field: string table_name = 1;
   */
  tableName: string;

  /**
   * (Required) The method to be called to write to the table.
   *
   * @generated from field: spark.connect.WriteOperation.SaveTable.TableSaveMethod save_method = 2;
   */
  saveMethod: WriteOperation_SaveTable_TableSaveMethod;
};

/**
 * Describes the message spark.connect.WriteOperation.SaveTable.
 * Use `create(WriteOperation_SaveTableSchema)` to create a new message.
 */
export const WriteOperation_SaveTableSchema: GenMessage<WriteOperation_SaveTable> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 3, 0);

/**
 * @generated from enum spark.connect.WriteOperation.SaveTable.TableSaveMethod
 */
export enum WriteOperation_SaveTable_TableSaveMethod {
  /**
   * @generated from enum value: TABLE_SAVE_METHOD_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * @generated from enum value: TABLE_SAVE_METHOD_SAVE_AS_TABLE = 1;
   */
  SAVE_AS_TABLE = 1,

  /**
   * @generated from enum value: TABLE_SAVE_METHOD_INSERT_INTO = 2;
   */
  INSERT_INTO = 2,
}

/**
 * Describes the enum spark.connect.WriteOperation.SaveTable.TableSaveMethod.
 */
export const WriteOperation_SaveTable_TableSaveMethodSchema: GenEnum<WriteOperation_SaveTable_TableSaveMethod> = /*@__PURE__*/
  enumDesc(file_spark_connect_commands, 3, 0, 0);

/**
 * @generated from message spark.connect.WriteOperation.BucketBy
 */
export type WriteOperation_BucketBy = Message<"spark.connect.WriteOperation.BucketBy"> & {
  /**
   * @generated from field: repeated string bucket_column_names = 1;
   */
  bucketColumnNames: string[];

  /**
   * @generated from field: int32 num_buckets = 2;
   */
  numBuckets: number;
};

/**
 * Describes the message spark.connect.WriteOperation.BucketBy.
 * Use `create(WriteOperation_BucketBySchema)` to create a new message.
 */
export const WriteOperation_BucketBySchema: GenMessage<WriteOperation_BucketBy> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 3, 1);

/**
 * @generated from enum spark.connect.WriteOperation.SaveMode
 */
export enum WriteOperation_SaveMode {
  /**
   * @generated from enum value: SAVE_MODE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * @generated from enum value: SAVE_MODE_APPEND = 1;
   */
  APPEND = 1,

  /**
   * @generated from enum value: SAVE_MODE_OVERWRITE = 2;
   */
  OVERWRITE = 2,

  /**
   * @generated from enum value: SAVE_MODE_ERROR_IF_EXISTS = 3;
   */
  ERROR_IF_EXISTS = 3,

  /**
   * @generated from enum value: SAVE_MODE_IGNORE = 4;
   */
  IGNORE = 4,
}

/**
 * Describes the enum spark.connect.WriteOperation.SaveMode.
 */
export const WriteOperation_SaveModeSchema: GenEnum<WriteOperation_SaveMode> = /*@__PURE__*/
  enumDesc(file_spark_connect_commands, 3, 0);

/**
 * As writes are not directly handled during analysis and planning, they are modeled as commands.
 *
 * @generated from message spark.connect.WriteOperationV2
 */
export type WriteOperationV2 = Message<"spark.connect.WriteOperationV2"> & {
  /**
   * (Required) The output of the `input` relation will be persisted according to the options.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * (Required) The destination of the write operation must be either a path or a table.
   *
   * @generated from field: string table_name = 2;
   */
  tableName: string;

  /**
   * (Optional) A provider for the underlying output data source. Spark's default catalog supports
   * "parquet", "json", etc.
   *
   * @generated from field: optional string provider = 3;
   */
  provider?: string;

  /**
   * (Optional) List of columns for partitioning for output table created by `create`,
   * `createOrReplace`, or `replace`
   *
   * @generated from field: repeated spark.connect.Expression partitioning_columns = 4;
   */
  partitioningColumns: Expression[];

  /**
   * (Optional) A list of configuration options.
   *
   * @generated from field: map<string, string> options = 5;
   */
  options: { [key: string]: string };

  /**
   * (Optional) A list of table properties.
   *
   * @generated from field: map<string, string> table_properties = 6;
   */
  tableProperties: { [key: string]: string };

  /**
   * (Required) Write mode.
   *
   * @generated from field: spark.connect.WriteOperationV2.Mode mode = 7;
   */
  mode: WriteOperationV2_Mode;

  /**
   * (Optional) A condition for overwrite saving mode
   *
   * @generated from field: spark.connect.Expression overwrite_condition = 8;
   */
  overwriteCondition?: Expression;

  /**
   * (Optional) Columns used for clustering the table.
   *
   * @generated from field: repeated string clustering_columns = 9;
   */
  clusteringColumns: string[];
};

/**
 * Describes the message spark.connect.WriteOperationV2.
 * Use `create(WriteOperationV2Schema)` to create a new message.
 */
export const WriteOperationV2Schema: GenMessage<WriteOperationV2> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 4);

/**
 * @generated from enum spark.connect.WriteOperationV2.Mode
 */
export enum WriteOperationV2_Mode {
  /**
   * @generated from enum value: MODE_UNSPECIFIED = 0;
   */
  UNSPECIFIED = 0,

  /**
   * @generated from enum value: MODE_CREATE = 1;
   */
  CREATE = 1,

  /**
   * @generated from enum value: MODE_OVERWRITE = 2;
   */
  OVERWRITE = 2,

  /**
   * @generated from enum value: MODE_OVERWRITE_PARTITIONS = 3;
   */
  OVERWRITE_PARTITIONS = 3,

  /**
   * @generated from enum value: MODE_APPEND = 4;
   */
  APPEND = 4,

  /**
   * @generated from enum value: MODE_REPLACE = 5;
   */
  REPLACE = 5,

  /**
   * @generated from enum value: MODE_CREATE_OR_REPLACE = 6;
   */
  CREATE_OR_REPLACE = 6,
}

/**
 * Describes the enum spark.connect.WriteOperationV2.Mode.
 */
export const WriteOperationV2_ModeSchema: GenEnum<WriteOperationV2_Mode> = /*@__PURE__*/
  enumDesc(file_spark_connect_commands, 4, 0);

/**
 * Starts write stream operation as streaming query. Query ID and Run ID of the streaming
 * query are returned.
 *
 * @generated from message spark.connect.WriteStreamOperationStart
 */
export type WriteStreamOperationStart = Message<"spark.connect.WriteStreamOperationStart"> & {
  /**
   * (Required) The output of the `input` streaming relation will be written.
   *
   * @generated from field: spark.connect.Relation input = 1;
   */
  input?: Relation;

  /**
   * @generated from field: string format = 2;
   */
  format: string;

  /**
   * @generated from field: map<string, string> options = 3;
   */
  options: { [key: string]: string };

  /**
   * @generated from field: repeated string partitioning_column_names = 4;
   */
  partitioningColumnNames: string[];

  /**
   * @generated from oneof spark.connect.WriteStreamOperationStart.trigger
   */
  trigger: {
    /**
     * @generated from field: string processing_time_interval = 5;
     */
    value: string;
    case: "processingTimeInterval";
  } | {
    /**
     * @generated from field: bool available_now = 6;
     */
    value: boolean;
    case: "availableNow";
  } | {
    /**
     * @generated from field: bool once = 7;
     */
    value: boolean;
    case: "once";
  } | {
    /**
     * @generated from field: string continuous_checkpoint_interval = 8;
     */
    value: string;
    case: "continuousCheckpointInterval";
  } | { case: undefined; value?: undefined };

  /**
   * @generated from field: string output_mode = 9;
   */
  outputMode: string;

  /**
   * @generated from field: string query_name = 10;
   */
  queryName: string;

  /**
   * The destination is optional. When set, it can be a path or a table name.
   *
   * @generated from oneof spark.connect.WriteStreamOperationStart.sink_destination
   */
  sinkDestination: {
    /**
     * @generated from field: string path = 11;
     */
    value: string;
    case: "path";
  } | {
    /**
     * @generated from field: string table_name = 12;
     */
    value: string;
    case: "tableName";
  } | { case: undefined; value?: undefined };

  /**
   * @generated from field: spark.connect.StreamingForeachFunction foreach_writer = 13;
   */
  foreachWriter?: StreamingForeachFunction;

  /**
   * @generated from field: spark.connect.StreamingForeachFunction foreach_batch = 14;
   */
  foreachBatch?: StreamingForeachFunction;

  /**
   * (Optional) Columns used for clustering the table.
   *
   * @generated from field: repeated string clustering_column_names = 15;
   */
  clusteringColumnNames: string[];
};

/**
 * Describes the message spark.connect.WriteStreamOperationStart.
 * Use `create(WriteStreamOperationStartSchema)` to create a new message.
 */
export const WriteStreamOperationStartSchema: GenMessage<WriteStreamOperationStart> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 5);

/**
 * @generated from message spark.connect.StreamingForeachFunction
 */
export type StreamingForeachFunction = Message<"spark.connect.StreamingForeachFunction"> & {
  /**
   * @generated from oneof spark.connect.StreamingForeachFunction.function
   */
  function: {
    /**
     * @generated from field: spark.connect.PythonUDF python_function = 1;
     */
    value: PythonUDF;
    case: "pythonFunction";
  } | {
    /**
     * @generated from field: spark.connect.ScalarScalaUDF scala_function = 2;
     */
    value: ScalarScalaUDF;
    case: "scalaFunction";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.StreamingForeachFunction.
 * Use `create(StreamingForeachFunctionSchema)` to create a new message.
 */
export const StreamingForeachFunctionSchema: GenMessage<StreamingForeachFunction> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 6);

/**
 * @generated from message spark.connect.WriteStreamOperationStartResult
 */
export type WriteStreamOperationStartResult = Message<"spark.connect.WriteStreamOperationStartResult"> & {
  /**
   * (Required) Query instance. See `StreamingQueryInstanceId`.
   *
   * @generated from field: spark.connect.StreamingQueryInstanceId query_id = 1;
   */
  queryId?: StreamingQueryInstanceId;

  /**
   * An optional query name.
   *
   * @generated from field: string name = 2;
   */
  name: string;

  /**
   * Optional query started event if there is any listener registered on the client side.
   *
   * @generated from field: optional string query_started_event_json = 3;
   */
  queryStartedEventJson?: string;
};

/**
 * Describes the message spark.connect.WriteStreamOperationStartResult.
 * Use `create(WriteStreamOperationStartResultSchema)` to create a new message.
 */
export const WriteStreamOperationStartResultSchema: GenMessage<WriteStreamOperationStartResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 7);

/**
 * A tuple that uniquely identifies an instance of streaming query run. It consists of `id` that
 * persists across the streaming runs and `run_id` that changes between each run of the
 * streaming query that resumes from the checkpoint.
 *
 * @generated from message spark.connect.StreamingQueryInstanceId
 */
export type StreamingQueryInstanceId = Message<"spark.connect.StreamingQueryInstanceId"> & {
  /**
   * (Required) The unique id of this query that persists across restarts from checkpoint data.
   * That is, this id is generated when a query is started for the first time, and
   * will be the same every time it is restarted from checkpoint data.
   *
   * @generated from field: string id = 1;
   */
  id: string;

  /**
   * (Required) The unique id of this run of the query. That is, every start/restart of a query
   * will generate a unique run_id. Therefore, every time a query is restarted from
   * checkpoint, it will have the same `id` but different `run_id`s.
   *
   * @generated from field: string run_id = 2;
   */
  runId: string;
};

/**
 * Describes the message spark.connect.StreamingQueryInstanceId.
 * Use `create(StreamingQueryInstanceIdSchema)` to create a new message.
 */
export const StreamingQueryInstanceIdSchema: GenMessage<StreamingQueryInstanceId> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 8);

/**
 * Commands for a streaming query.
 *
 * @generated from message spark.connect.StreamingQueryCommand
 */
export type StreamingQueryCommand = Message<"spark.connect.StreamingQueryCommand"> & {
  /**
   * (Required) Query instance. See `StreamingQueryInstanceId`.
   *
   * @generated from field: spark.connect.StreamingQueryInstanceId query_id = 1;
   */
  queryId?: StreamingQueryInstanceId;

  /**
   * See documentation for the corresponding API method in StreamingQuery.
   *
   * @generated from oneof spark.connect.StreamingQueryCommand.command
   */
  command: {
    /**
     * status() API.
     *
     * @generated from field: bool status = 2;
     */
    value: boolean;
    case: "status";
  } | {
    /**
     * lastProgress() API.
     *
     * @generated from field: bool last_progress = 3;
     */
    value: boolean;
    case: "lastProgress";
  } | {
    /**
     * recentProgress() API.
     *
     * @generated from field: bool recent_progress = 4;
     */
    value: boolean;
    case: "recentProgress";
  } | {
    /**
     * stop() API. Stops the query.
     *
     * @generated from field: bool stop = 5;
     */
    value: boolean;
    case: "stop";
  } | {
    /**
     * processAllAvailable() API. Waits till all the available data is processed
     *
     * @generated from field: bool process_all_available = 6;
     */
    value: boolean;
    case: "processAllAvailable";
  } | {
    /**
     * explain() API. Returns logical and physical plans.
     *
     * @generated from field: spark.connect.StreamingQueryCommand.ExplainCommand explain = 7;
     */
    value: StreamingQueryCommand_ExplainCommand;
    case: "explain";
  } | {
    /**
     * exception() API. Returns the exception in the query if any.
     *
     * @generated from field: bool exception = 8;
     */
    value: boolean;
    case: "exception";
  } | {
    /**
     * awaitTermination() API. Waits for the termination of the query.
     *
     * @generated from field: spark.connect.StreamingQueryCommand.AwaitTerminationCommand await_termination = 9;
     */
    value: StreamingQueryCommand_AwaitTerminationCommand;
    case: "awaitTermination";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.StreamingQueryCommand.
 * Use `create(StreamingQueryCommandSchema)` to create a new message.
 */
export const StreamingQueryCommandSchema: GenMessage<StreamingQueryCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 9);

/**
 * @generated from message spark.connect.StreamingQueryCommand.ExplainCommand
 */
export type StreamingQueryCommand_ExplainCommand = Message<"spark.connect.StreamingQueryCommand.ExplainCommand"> & {
  /**
   * TODO: Consider reusing Explain from AnalyzePlanRequest message.
   *       We can not do this right now since it base.proto imports this file.
   *
   * @generated from field: bool extended = 1;
   */
  extended: boolean;
};

/**
 * Describes the message spark.connect.StreamingQueryCommand.ExplainCommand.
 * Use `create(StreamingQueryCommand_ExplainCommandSchema)` to create a new message.
 */
export const StreamingQueryCommand_ExplainCommandSchema: GenMessage<StreamingQueryCommand_ExplainCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 9, 0);

/**
 * @generated from message spark.connect.StreamingQueryCommand.AwaitTerminationCommand
 */
export type StreamingQueryCommand_AwaitTerminationCommand = Message<"spark.connect.StreamingQueryCommand.AwaitTerminationCommand"> & {
  /**
   * @generated from field: optional int64 timeout_ms = 2;
   */
  timeoutMs?: bigint;
};

/**
 * Describes the message spark.connect.StreamingQueryCommand.AwaitTerminationCommand.
 * Use `create(StreamingQueryCommand_AwaitTerminationCommandSchema)` to create a new message.
 */
export const StreamingQueryCommand_AwaitTerminationCommandSchema: GenMessage<StreamingQueryCommand_AwaitTerminationCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 9, 1);

/**
 * Response for commands on a streaming query.
 *
 * @generated from message spark.connect.StreamingQueryCommandResult
 */
export type StreamingQueryCommandResult = Message<"spark.connect.StreamingQueryCommandResult"> & {
  /**
   * (Required) Query instance id. See `StreamingQueryInstanceId`.
   *
   * @generated from field: spark.connect.StreamingQueryInstanceId query_id = 1;
   */
  queryId?: StreamingQueryInstanceId;

  /**
   * @generated from oneof spark.connect.StreamingQueryCommandResult.result_type
   */
  resultType: {
    /**
     * @generated from field: spark.connect.StreamingQueryCommandResult.StatusResult status = 2;
     */
    value: StreamingQueryCommandResult_StatusResult;
    case: "status";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryCommandResult.RecentProgressResult recent_progress = 3;
     */
    value: StreamingQueryCommandResult_RecentProgressResult;
    case: "recentProgress";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryCommandResult.ExplainResult explain = 4;
     */
    value: StreamingQueryCommandResult_ExplainResult;
    case: "explain";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryCommandResult.ExceptionResult exception = 5;
     */
    value: StreamingQueryCommandResult_ExceptionResult;
    case: "exception";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryCommandResult.AwaitTerminationResult await_termination = 6;
     */
    value: StreamingQueryCommandResult_AwaitTerminationResult;
    case: "awaitTermination";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.StreamingQueryCommandResult.
 * Use `create(StreamingQueryCommandResultSchema)` to create a new message.
 */
export const StreamingQueryCommandResultSchema: GenMessage<StreamingQueryCommandResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 10);

/**
 * @generated from message spark.connect.StreamingQueryCommandResult.StatusResult
 */
export type StreamingQueryCommandResult_StatusResult = Message<"spark.connect.StreamingQueryCommandResult.StatusResult"> & {
  /**
   * See documentation for these Scala 'StreamingQueryStatus' struct
   *
   * @generated from field: string status_message = 1;
   */
  statusMessage: string;

  /**
   * @generated from field: bool is_data_available = 2;
   */
  isDataAvailable: boolean;

  /**
   * @generated from field: bool is_trigger_active = 3;
   */
  isTriggerActive: boolean;

  /**
   * @generated from field: bool is_active = 4;
   */
  isActive: boolean;
};

/**
 * Describes the message spark.connect.StreamingQueryCommandResult.StatusResult.
 * Use `create(StreamingQueryCommandResult_StatusResultSchema)` to create a new message.
 */
export const StreamingQueryCommandResult_StatusResultSchema: GenMessage<StreamingQueryCommandResult_StatusResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 10, 0);

/**
 * @generated from message spark.connect.StreamingQueryCommandResult.RecentProgressResult
 */
export type StreamingQueryCommandResult_RecentProgressResult = Message<"spark.connect.StreamingQueryCommandResult.RecentProgressResult"> & {
  /**
   * Progress reports as an array of json strings.
   *
   * @generated from field: repeated string recent_progress_json = 5;
   */
  recentProgressJson: string[];
};

/**
 * Describes the message spark.connect.StreamingQueryCommandResult.RecentProgressResult.
 * Use `create(StreamingQueryCommandResult_RecentProgressResultSchema)` to create a new message.
 */
export const StreamingQueryCommandResult_RecentProgressResultSchema: GenMessage<StreamingQueryCommandResult_RecentProgressResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 10, 1);

/**
 * @generated from message spark.connect.StreamingQueryCommandResult.ExplainResult
 */
export type StreamingQueryCommandResult_ExplainResult = Message<"spark.connect.StreamingQueryCommandResult.ExplainResult"> & {
  /**
   * Logical and physical plans as string
   *
   * @generated from field: string result = 1;
   */
  result: string;
};

/**
 * Describes the message spark.connect.StreamingQueryCommandResult.ExplainResult.
 * Use `create(StreamingQueryCommandResult_ExplainResultSchema)` to create a new message.
 */
export const StreamingQueryCommandResult_ExplainResultSchema: GenMessage<StreamingQueryCommandResult_ExplainResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 10, 2);

/**
 * @generated from message spark.connect.StreamingQueryCommandResult.ExceptionResult
 */
export type StreamingQueryCommandResult_ExceptionResult = Message<"spark.connect.StreamingQueryCommandResult.ExceptionResult"> & {
  /**
   * (Optional) Exception message as string, maps to the return value of original
   * StreamingQueryException's toString method
   *
   * @generated from field: optional string exception_message = 1;
   */
  exceptionMessage?: string;

  /**
   * (Optional) Exception error class as string
   *
   * @generated from field: optional string error_class = 2;
   */
  errorClass?: string;

  /**
   * (Optional) Exception stack trace as string
   *
   * @generated from field: optional string stack_trace = 3;
   */
  stackTrace?: string;
};

/**
 * Describes the message spark.connect.StreamingQueryCommandResult.ExceptionResult.
 * Use `create(StreamingQueryCommandResult_ExceptionResultSchema)` to create a new message.
 */
export const StreamingQueryCommandResult_ExceptionResultSchema: GenMessage<StreamingQueryCommandResult_ExceptionResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 10, 3);

/**
 * @generated from message spark.connect.StreamingQueryCommandResult.AwaitTerminationResult
 */
export type StreamingQueryCommandResult_AwaitTerminationResult = Message<"spark.connect.StreamingQueryCommandResult.AwaitTerminationResult"> & {
  /**
   * @generated from field: bool terminated = 1;
   */
  terminated: boolean;
};

/**
 * Describes the message spark.connect.StreamingQueryCommandResult.AwaitTerminationResult.
 * Use `create(StreamingQueryCommandResult_AwaitTerminationResultSchema)` to create a new message.
 */
export const StreamingQueryCommandResult_AwaitTerminationResultSchema: GenMessage<StreamingQueryCommandResult_AwaitTerminationResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 10, 4);

/**
 * Commands for the streaming query manager.
 *
 * @generated from message spark.connect.StreamingQueryManagerCommand
 */
export type StreamingQueryManagerCommand = Message<"spark.connect.StreamingQueryManagerCommand"> & {
  /**
   * See documentation for the corresponding API method in StreamingQueryManager.
   *
   * @generated from oneof spark.connect.StreamingQueryManagerCommand.command
   */
  command: {
    /**
     * active() API, returns a list of active queries.
     *
     * @generated from field: bool active = 1;
     */
    value: boolean;
    case: "active";
  } | {
    /**
     * get() API, returns the StreamingQuery identified by id.
     *
     * @generated from field: string get_query = 2;
     */
    value: string;
    case: "getQuery";
  } | {
    /**
     * awaitAnyTermination() API, wait until any query terminates or timeout.
     *
     * @generated from field: spark.connect.StreamingQueryManagerCommand.AwaitAnyTerminationCommand await_any_termination = 3;
     */
    value: StreamingQueryManagerCommand_AwaitAnyTerminationCommand;
    case: "awaitAnyTermination";
  } | {
    /**
     * resetTerminated() API.
     *
     * @generated from field: bool reset_terminated = 4;
     */
    value: boolean;
    case: "resetTerminated";
  } | {
    /**
     * addListener API.
     *
     * @generated from field: spark.connect.StreamingQueryManagerCommand.StreamingQueryListenerCommand add_listener = 5;
     */
    value: StreamingQueryManagerCommand_StreamingQueryListenerCommand;
    case: "addListener";
  } | {
    /**
     * removeListener API.
     *
     * @generated from field: spark.connect.StreamingQueryManagerCommand.StreamingQueryListenerCommand remove_listener = 6;
     */
    value: StreamingQueryManagerCommand_StreamingQueryListenerCommand;
    case: "removeListener";
  } | {
    /**
     * listListeners() API, returns a list of streaming query listeners.
     *
     * @generated from field: bool list_listeners = 7;
     */
    value: boolean;
    case: "listListeners";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.StreamingQueryManagerCommand.
 * Use `create(StreamingQueryManagerCommandSchema)` to create a new message.
 */
export const StreamingQueryManagerCommandSchema: GenMessage<StreamingQueryManagerCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 11);

/**
 * @generated from message spark.connect.StreamingQueryManagerCommand.AwaitAnyTerminationCommand
 */
export type StreamingQueryManagerCommand_AwaitAnyTerminationCommand = Message<"spark.connect.StreamingQueryManagerCommand.AwaitAnyTerminationCommand"> & {
  /**
   * (Optional) The waiting time in milliseconds to wait for any query to terminate.
   *
   * @generated from field: optional int64 timeout_ms = 1;
   */
  timeoutMs?: bigint;
};

/**
 * Describes the message spark.connect.StreamingQueryManagerCommand.AwaitAnyTerminationCommand.
 * Use `create(StreamingQueryManagerCommand_AwaitAnyTerminationCommandSchema)` to create a new message.
 */
export const StreamingQueryManagerCommand_AwaitAnyTerminationCommandSchema: GenMessage<StreamingQueryManagerCommand_AwaitAnyTerminationCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 11, 0);

/**
 * @generated from message spark.connect.StreamingQueryManagerCommand.StreamingQueryListenerCommand
 */
export type StreamingQueryManagerCommand_StreamingQueryListenerCommand = Message<"spark.connect.StreamingQueryManagerCommand.StreamingQueryListenerCommand"> & {
  /**
   * @generated from field: bytes listener_payload = 1;
   */
  listenerPayload: Uint8Array;

  /**
   * @generated from field: optional spark.connect.PythonUDF python_listener_payload = 2;
   */
  pythonListenerPayload?: PythonUDF;

  /**
   * @generated from field: string id = 3;
   */
  id: string;
};

/**
 * Describes the message spark.connect.StreamingQueryManagerCommand.StreamingQueryListenerCommand.
 * Use `create(StreamingQueryManagerCommand_StreamingQueryListenerCommandSchema)` to create a new message.
 */
export const StreamingQueryManagerCommand_StreamingQueryListenerCommandSchema: GenMessage<StreamingQueryManagerCommand_StreamingQueryListenerCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 11, 1);

/**
 * Response for commands on the streaming query manager.
 *
 * @generated from message spark.connect.StreamingQueryManagerCommandResult
 */
export type StreamingQueryManagerCommandResult = Message<"spark.connect.StreamingQueryManagerCommandResult"> & {
  /**
   * @generated from oneof spark.connect.StreamingQueryManagerCommandResult.result_type
   */
  resultType: {
    /**
     * @generated from field: spark.connect.StreamingQueryManagerCommandResult.ActiveResult active = 1;
     */
    value: StreamingQueryManagerCommandResult_ActiveResult;
    case: "active";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryManagerCommandResult.StreamingQueryInstance query = 2;
     */
    value: StreamingQueryManagerCommandResult_StreamingQueryInstance;
    case: "query";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryManagerCommandResult.AwaitAnyTerminationResult await_any_termination = 3;
     */
    value: StreamingQueryManagerCommandResult_AwaitAnyTerminationResult;
    case: "awaitAnyTermination";
  } | {
    /**
     * @generated from field: bool reset_terminated = 4;
     */
    value: boolean;
    case: "resetTerminated";
  } | {
    /**
     * @generated from field: bool add_listener = 5;
     */
    value: boolean;
    case: "addListener";
  } | {
    /**
     * @generated from field: bool remove_listener = 6;
     */
    value: boolean;
    case: "removeListener";
  } | {
    /**
     * @generated from field: spark.connect.StreamingQueryManagerCommandResult.ListStreamingQueryListenerResult list_listeners = 7;
     */
    value: StreamingQueryManagerCommandResult_ListStreamingQueryListenerResult;
    case: "listListeners";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.StreamingQueryManagerCommandResult.
 * Use `create(StreamingQueryManagerCommandResultSchema)` to create a new message.
 */
export const StreamingQueryManagerCommandResultSchema: GenMessage<StreamingQueryManagerCommandResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 12);

/**
 * @generated from message spark.connect.StreamingQueryManagerCommandResult.ActiveResult
 */
export type StreamingQueryManagerCommandResult_ActiveResult = Message<"spark.connect.StreamingQueryManagerCommandResult.ActiveResult"> & {
  /**
   * @generated from field: repeated spark.connect.StreamingQueryManagerCommandResult.StreamingQueryInstance active_queries = 1;
   */
  activeQueries: StreamingQueryManagerCommandResult_StreamingQueryInstance[];
};

/**
 * Describes the message spark.connect.StreamingQueryManagerCommandResult.ActiveResult.
 * Use `create(StreamingQueryManagerCommandResult_ActiveResultSchema)` to create a new message.
 */
export const StreamingQueryManagerCommandResult_ActiveResultSchema: GenMessage<StreamingQueryManagerCommandResult_ActiveResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 12, 0);

/**
 * @generated from message spark.connect.StreamingQueryManagerCommandResult.StreamingQueryInstance
 */
export type StreamingQueryManagerCommandResult_StreamingQueryInstance = Message<"spark.connect.StreamingQueryManagerCommandResult.StreamingQueryInstance"> & {
  /**
   * (Required) The id and runId of this query.
   *
   * @generated from field: spark.connect.StreamingQueryInstanceId id = 1;
   */
  id?: StreamingQueryInstanceId;

  /**
   * (Optional) The name of this query.
   *
   * @generated from field: optional string name = 2;
   */
  name?: string;
};

/**
 * Describes the message spark.connect.StreamingQueryManagerCommandResult.StreamingQueryInstance.
 * Use `create(StreamingQueryManagerCommandResult_StreamingQueryInstanceSchema)` to create a new message.
 */
export const StreamingQueryManagerCommandResult_StreamingQueryInstanceSchema: GenMessage<StreamingQueryManagerCommandResult_StreamingQueryInstance> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 12, 1);

/**
 * @generated from message spark.connect.StreamingQueryManagerCommandResult.AwaitAnyTerminationResult
 */
export type StreamingQueryManagerCommandResult_AwaitAnyTerminationResult = Message<"spark.connect.StreamingQueryManagerCommandResult.AwaitAnyTerminationResult"> & {
  /**
   * @generated from field: bool terminated = 1;
   */
  terminated: boolean;
};

/**
 * Describes the message spark.connect.StreamingQueryManagerCommandResult.AwaitAnyTerminationResult.
 * Use `create(StreamingQueryManagerCommandResult_AwaitAnyTerminationResultSchema)` to create a new message.
 */
export const StreamingQueryManagerCommandResult_AwaitAnyTerminationResultSchema: GenMessage<StreamingQueryManagerCommandResult_AwaitAnyTerminationResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 12, 2);

/**
 * @generated from message spark.connect.StreamingQueryManagerCommandResult.StreamingQueryListenerInstance
 */
export type StreamingQueryManagerCommandResult_StreamingQueryListenerInstance = Message<"spark.connect.StreamingQueryManagerCommandResult.StreamingQueryListenerInstance"> & {
  /**
   * @generated from field: bytes listener_payload = 1;
   */
  listenerPayload: Uint8Array;
};

/**
 * Describes the message spark.connect.StreamingQueryManagerCommandResult.StreamingQueryListenerInstance.
 * Use `create(StreamingQueryManagerCommandResult_StreamingQueryListenerInstanceSchema)` to create a new message.
 */
export const StreamingQueryManagerCommandResult_StreamingQueryListenerInstanceSchema: GenMessage<StreamingQueryManagerCommandResult_StreamingQueryListenerInstance> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 12, 3);

/**
 * @generated from message spark.connect.StreamingQueryManagerCommandResult.ListStreamingQueryListenerResult
 */
export type StreamingQueryManagerCommandResult_ListStreamingQueryListenerResult = Message<"spark.connect.StreamingQueryManagerCommandResult.ListStreamingQueryListenerResult"> & {
  /**
   * (Required) Reference IDs of listener instances.
   *
   * @generated from field: repeated string listener_ids = 1;
   */
  listenerIds: string[];
};

/**
 * Describes the message spark.connect.StreamingQueryManagerCommandResult.ListStreamingQueryListenerResult.
 * Use `create(StreamingQueryManagerCommandResult_ListStreamingQueryListenerResultSchema)` to create a new message.
 */
export const StreamingQueryManagerCommandResult_ListStreamingQueryListenerResultSchema: GenMessage<StreamingQueryManagerCommandResult_ListStreamingQueryListenerResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 12, 4);

/**
 * The protocol for client-side StreamingQueryListener.
 * This command will only be set when either the first listener is added to the client, or the last
 * listener is removed from the client.
 * The add_listener_bus_listener command will only be set true in the first case.
 * The remove_listener_bus_listener command will only be set true in the second case.
 *
 * @generated from message spark.connect.StreamingQueryListenerBusCommand
 */
export type StreamingQueryListenerBusCommand = Message<"spark.connect.StreamingQueryListenerBusCommand"> & {
  /**
   * @generated from oneof spark.connect.StreamingQueryListenerBusCommand.command
   */
  command: {
    /**
     * @generated from field: bool add_listener_bus_listener = 1;
     */
    value: boolean;
    case: "addListenerBusListener";
  } | {
    /**
     * @generated from field: bool remove_listener_bus_listener = 2;
     */
    value: boolean;
    case: "removeListenerBusListener";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message spark.connect.StreamingQueryListenerBusCommand.
 * Use `create(StreamingQueryListenerBusCommandSchema)` to create a new message.
 */
export const StreamingQueryListenerBusCommandSchema: GenMessage<StreamingQueryListenerBusCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 13);

/**
 * The protocol for the returned events in the long-running response channel.
 *
 * @generated from message spark.connect.StreamingQueryListenerEvent
 */
export type StreamingQueryListenerEvent = Message<"spark.connect.StreamingQueryListenerEvent"> & {
  /**
   * (Required) The json serialized event, all StreamingQueryListener events have a json method
   *
   * @generated from field: string event_json = 1;
   */
  eventJson: string;

  /**
   * (Required) Query event type used by client to decide how to deserialize the event_json
   *
   * @generated from field: spark.connect.StreamingQueryEventType event_type = 2;
   */
  eventType: StreamingQueryEventType;
};

/**
 * Describes the message spark.connect.StreamingQueryListenerEvent.
 * Use `create(StreamingQueryListenerEventSchema)` to create a new message.
 */
export const StreamingQueryListenerEventSchema: GenMessage<StreamingQueryListenerEvent> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 14);

/**
 * @generated from message spark.connect.StreamingQueryListenerEventsResult
 */
export type StreamingQueryListenerEventsResult = Message<"spark.connect.StreamingQueryListenerEventsResult"> & {
  /**
   * @generated from field: repeated spark.connect.StreamingQueryListenerEvent events = 1;
   */
  events: StreamingQueryListenerEvent[];

  /**
   * @generated from field: optional bool listener_bus_listener_added = 2;
   */
  listenerBusListenerAdded?: boolean;
};

/**
 * Describes the message spark.connect.StreamingQueryListenerEventsResult.
 * Use `create(StreamingQueryListenerEventsResultSchema)` to create a new message.
 */
export const StreamingQueryListenerEventsResultSchema: GenMessage<StreamingQueryListenerEventsResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 15);

/**
 * Command to get the output of 'SparkContext.resources'
 *
 * @generated from message spark.connect.GetResourcesCommand
 */
export type GetResourcesCommand = Message<"spark.connect.GetResourcesCommand"> & {
};

/**
 * Describes the message spark.connect.GetResourcesCommand.
 * Use `create(GetResourcesCommandSchema)` to create a new message.
 */
export const GetResourcesCommandSchema: GenMessage<GetResourcesCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 16);

/**
 * Response for command 'GetResourcesCommand'.
 *
 * @generated from message spark.connect.GetResourcesCommandResult
 */
export type GetResourcesCommandResult = Message<"spark.connect.GetResourcesCommandResult"> & {
  /**
   * @generated from field: map<string, spark.connect.ResourceInformation> resources = 1;
   */
  resources: { [key: string]: ResourceInformation };
};

/**
 * Describes the message spark.connect.GetResourcesCommandResult.
 * Use `create(GetResourcesCommandResultSchema)` to create a new message.
 */
export const GetResourcesCommandResultSchema: GenMessage<GetResourcesCommandResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 17);

/**
 * Command to create ResourceProfile
 *
 * @generated from message spark.connect.CreateResourceProfileCommand
 */
export type CreateResourceProfileCommand = Message<"spark.connect.CreateResourceProfileCommand"> & {
  /**
   * (Required) The ResourceProfile to be built on the server-side.
   *
   * @generated from field: spark.connect.ResourceProfile profile = 1;
   */
  profile?: ResourceProfile;
};

/**
 * Describes the message spark.connect.CreateResourceProfileCommand.
 * Use `create(CreateResourceProfileCommandSchema)` to create a new message.
 */
export const CreateResourceProfileCommandSchema: GenMessage<CreateResourceProfileCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 18);

/**
 * Response for command 'CreateResourceProfileCommand'.
 *
 * @generated from message spark.connect.CreateResourceProfileCommandResult
 */
export type CreateResourceProfileCommandResult = Message<"spark.connect.CreateResourceProfileCommandResult"> & {
  /**
   * (Required) Server-side generated resource profile id.
   *
   * @generated from field: int32 profile_id = 1;
   */
  profileId: number;
};

/**
 * Describes the message spark.connect.CreateResourceProfileCommandResult.
 * Use `create(CreateResourceProfileCommandResultSchema)` to create a new message.
 */
export const CreateResourceProfileCommandResultSchema: GenMessage<CreateResourceProfileCommandResult> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 19);

/**
 * Command to remove `CashedRemoteRelation`
 *
 * @generated from message spark.connect.RemoveCachedRemoteRelationCommand
 */
export type RemoveCachedRemoteRelationCommand = Message<"spark.connect.RemoveCachedRemoteRelationCommand"> & {
  /**
   * (Required) The remote to be related
   *
   * @generated from field: spark.connect.CachedRemoteRelation relation = 1;
   */
  relation?: CachedRemoteRelation;
};

/**
 * Describes the message spark.connect.RemoveCachedRemoteRelationCommand.
 * Use `create(RemoveCachedRemoteRelationCommandSchema)` to create a new message.
 */
export const RemoveCachedRemoteRelationCommandSchema: GenMessage<RemoveCachedRemoteRelationCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 20);

/**
 * @generated from message spark.connect.CheckpointCommand
 */
export type CheckpointCommand = Message<"spark.connect.CheckpointCommand"> & {
  /**
   * (Required) The logical plan to checkpoint.
   *
   * @generated from field: spark.connect.Relation relation = 1;
   */
  relation?: Relation;

  /**
   * (Required) Locally checkpoint using a local temporary
   * directory in Spark Connect server (Spark Driver)
   *
   * @generated from field: bool local = 2;
   */
  local: boolean;

  /**
   * (Required) Whether to checkpoint this dataframe immediately.
   *
   * @generated from field: bool eager = 3;
   */
  eager: boolean;

  /**
   * (Optional) For local checkpoint, the storage level to use.
   *
   * @generated from field: optional spark.connect.StorageLevel storage_level = 4;
   */
  storageLevel?: StorageLevel;
};

/**
 * Describes the message spark.connect.CheckpointCommand.
 * Use `create(CheckpointCommandSchema)` to create a new message.
 */
export const CheckpointCommandSchema: GenMessage<CheckpointCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 21);

/**
 * @generated from message spark.connect.MergeIntoTableCommand
 */
export type MergeIntoTableCommand = Message<"spark.connect.MergeIntoTableCommand"> & {
  /**
   * (Required) The name of the target table.
   *
   * @generated from field: string target_table_name = 1;
   */
  targetTableName: string;

  /**
   * (Required) The relation of the source table.
   *
   * @generated from field: spark.connect.Relation source_table_plan = 2;
   */
  sourceTablePlan?: Relation;

  /**
   * (Required) The condition to match the source and target.
   *
   * @generated from field: spark.connect.Expression merge_condition = 3;
   */
  mergeCondition?: Expression;

  /**
   * (Optional) The actions to be taken when the condition is matched.
   *
   * @generated from field: repeated spark.connect.Expression match_actions = 4;
   */
  matchActions: Expression[];

  /**
   * (Optional) The actions to be taken when the condition is not matched.
   *
   * @generated from field: repeated spark.connect.Expression not_matched_actions = 5;
   */
  notMatchedActions: Expression[];

  /**
   * (Optional) The actions to be taken when the condition is not matched by source.
   *
   * @generated from field: repeated spark.connect.Expression not_matched_by_source_actions = 6;
   */
  notMatchedBySourceActions: Expression[];

  /**
   * (Required) Whether to enable schema evolution.
   *
   * @generated from field: bool with_schema_evolution = 7;
   */
  withSchemaEvolution: boolean;
};

/**
 * Describes the message spark.connect.MergeIntoTableCommand.
 * Use `create(MergeIntoTableCommandSchema)` to create a new message.
 */
export const MergeIntoTableCommandSchema: GenMessage<MergeIntoTableCommand> = /*@__PURE__*/
  messageDesc(file_spark_connect_commands, 22);

/**
 * The enum used for client side streaming query listener event
 * There is no QueryStartedEvent defined here,
 * it is added as a field in WriteStreamOperationStartResult
 *
 * @generated from enum spark.connect.StreamingQueryEventType
 */
export enum StreamingQueryEventType {
  /**
   * @generated from enum value: QUERY_PROGRESS_UNSPECIFIED = 0;
   */
  QUERY_PROGRESS_UNSPECIFIED = 0,

  /**
   * @generated from enum value: QUERY_PROGRESS_EVENT = 1;
   */
  QUERY_PROGRESS_EVENT = 1,

  /**
   * @generated from enum value: QUERY_TERMINATED_EVENT = 2;
   */
  QUERY_TERMINATED_EVENT = 2,

  /**
   * @generated from enum value: QUERY_IDLE_EVENT = 3;
   */
  QUERY_IDLE_EVENT = 3,
}

/**
 * Describes the enum spark.connect.StreamingQueryEventType.
 */
export const StreamingQueryEventTypeSchema: GenEnum<StreamingQueryEventType> = /*@__PURE__*/
  enumDesc(file_spark_connect_commands, 0);

